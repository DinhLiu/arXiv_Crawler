\section{Related Work} \label{sec:related_work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% State-of-Health Prediction of Li-ion Batteries
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{State-of-Health Prediction of Li-ion Batteries} \label{subsec:related_work_soh}
\cite{Ren2023} categorizes battery SOH prediction methods into two classes: model-driven and data-driven methods. In this work we focus on data-driven methods.

Many works combine recurrent networks and convolution networks to predict a battery's SOH. \cite{mazzi_lithium-ion_2024} use a 1D-CNN followed by BiGRU layers, utilizing measured voltage, current, and temperature signals from the NASA PCoE dataset \citep{saha2007nasa}. Utilizing the same dataset, \cite{Yao2024} develop a CNN-WNN-WLSTM network with wavelet activation functions. \cite{Shen2023} use an extreme learning machine (ELM) algorithm on voltage signals measured during charging mode. \cite{Wu2022} combine convolutional and recurrent autoencoders with GRU networks. \cite{Zhu2022} use a CNN-BiLSTM with attention for SOH and remaining useful life (RUL) estimation. \cite{Ren2021} employ an autoencoder feeding parallel CNN and LSTM blocks. \cite{Tong2021} develop an ADLSTM network with Bayesian optimization. \cite{Tan2020} propose a feature score rule for LSTM-FC networks. \cite{Crocioni2020} compare CNN-LSTM and CNN-GRU networks. \cite{Li2020} introduce an AST-LSTM network. \cite{Yang2020} merge CNN with random forest in a CNN-RF network. \cite{Garse2024} use a random forest regression and FC network in the RFR-ANN model. \cite{Chen2024} tackle SOH with a self-attention knowledge domain adaptation network.

Other works focus on transformer-based models. \cite{Feng2024} introduce GPT4Battery, a large language model (LLM) finetuned to estimate SOH on the GOTION dataset \citep{lu_deep_2023}. It employs a pre-trained GPT-2 backbone, followed by a feature extractor and two heads for charging curve reconstruction and SOH estimation. \cite{Gomez2024} use a temporal fusion transformer (TFT) on a Toyota dataset \citep{severson_data-driven_2019}, integrating Bi-LSTM layers for time series forecasting. \cite{Zhu2024} develop a Transformer with sparse attention and dilated convolution layers on the CALCE \citep{he_prognostics_2011} and NASA PCoE datasets. \cite{Huang2024} use singular value decomposition before inputting data into a Transformer model. \cite{Nakano2024} combine a CNN with a Transformer model in an experimental EV. They feed voltage, current, and speed signals along with the SOC.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Structured State Space Models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Structured State Space Models} \label{subsec:related_work_ssm}

Recently, state space models (SSMs) made their debut in the field of deep learning challenging the dominance of transformers \citep{vaswani_attention_2017} in sequential data tasks. While the transformer is successfully used in most fields of deep learning, its quadratic scaling law makes it challenging and expensive to be used for certain tasks with long sequences.

\cite{gu_lssl_2021}'s LSSL model incorporated \cite{gu_hippo_2020}'s HiPPO Framework into SSMs and showed that SSMs can be trained. They further highlighted the duality of its recurrent and convolution representation, which meant, that it can be inferred with $O(N)$ complexity in its recurrent view and trained in parallel leveraging modern hardware accelerators using the convolution representation. The S4 model by \cite{gu_s4_2022} further employed a certain structure upon its state matrix A, which allowed for a more efficient construction of the convolution kernel required for training. Many subsequent work \citep{smith_s5_2023,gupta_dss_2022,gu_s4d_2022,fu_h3_2023,gu_how_2022} further improved upon existing SSMs which ultimately led to the development of the Mamba model by \cite{gu_mamba_2024}. Mamba added selectivity into the SSM increasing its performance while still featuring sub-quadratic complexity during inference. It is this Transformer-like performance while scaling sub-quadratically with the sequence length which makes it especially suited for sequential data tasks with long sequences such as audio \citep{lin_audio_2024,erol_audio_2024}, images \citep{nguyen_s4nd_2022,liu_vmamba_2024,zhu_vision_2024}, video \citep{chen_video_2024,li_videomamba_2024}, NLP \citep{lieber_jamba_2024}, segmentation \citep{wan_sigma_2024}, motion generation \citep{zhang_motion_2024} and stock prediction \citep{shi_mambastock_2024}.
Recent work focuses on the connection between attention and SSMs \cite{ali_hidden_2024,dao_mamba-2_2024} to simplify its formulation and to be able to leverage the vast amount of research done on attention mechanisms of transformers and its hardware aware and efficient implementations. \cite{behrouz_mambamixer_2024} extends Mamba-like models to apply its selectivity not only along tokens but also along channels, making it especially well suited for multi-variate time signals such those found in the state of health prediction of Li-ion batteries.
