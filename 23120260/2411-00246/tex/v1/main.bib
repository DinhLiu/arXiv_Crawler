@inproceedings{gandelsman2024interpreting,
  title={Interpreting CLIP's Image Representation via Text-Based Decomposition},
  author={Gandelsman, Yossi and Efros, Alexei A and Steinhardt, Jacob},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{maiorca2024latent,
  title={Latent Space Translation via Semantic Alignment},
  author={Maiorca, Valentino and Moschella, Luca and Norelli, Antonio and Fumero, Marco and Locatello, Francesco and Rodol{\`a}, Emanuele},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
@article{gribonval2005exponential,
  title={On the exponential convergence of matching pursuits in quasi-incoherent dictionaries},
  author={Gribonval, R{\'e}mi and Vandergheynst, Pierre},
  journal={IEEE Transactions on Information Theory},
  volume={52},
  number={1},
  pages={255--261},
  year={2005},
  publisher={IEEE}
}
@article{merchant2023scaling,
  title={Scaling deep learning for materials discovery},
  author={Merchant, Amil and Batzner, Simon and Schoenholz, Samuel S and Aykol, Muratahan and Cheon, Gowoon and Cubuk, Ekin Dogus},
  journal={Nature},
  volume={624},
  number={7990},
  pages={80--85},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{locatello2019challenging,
  title={Challenging common assumptions in the unsupervised learning of disentangled representations},
  author={Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  booktitle={international conference on machine learning},
  pages={4114--4124},
  year={2019},
  organization={PMLR}
}
@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}
@article{espeholt2022deep,
  title={Deep learning for twelve hour precipitation forecasts},
  author={Espeholt, Lasse and Agrawal, Shreya and S{\o}nderby, Casper and Kumar, Manoj and Heek, Jonathan and Bromberg, Carla and Gazen, Cenk and Carver, Rob and Andrychowicz, Marcin and Hickey, Jason and others},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={1--10},
  year={2022},
  publisher={Nature Publishing Group}
}
@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@inproceedings{cherti2023reproducible,
  title={Reproducible scaling laws for contrastive language-image learning},
  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2818--2829},
  year={2023}
}

@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9650--9660},
  year={2021}
}

@article{tropp2006algorithms,
  title={Algorithms for simultaneous sparse approximation. Part I: Greedy pursuit},
  author={Tropp, Joel A and Gilbert, Anna C and Strauss, Martin J},
  journal={Signal processing},
  volume={86},
  number={3},
  pages={572--588},
  year={2006},
  publisher={Elsevier}
}

@inproceedings{pati1993orthogonal,
  title={Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition},
  author={Pati, Yagyensh Chandra and Rezaiifar, Ramin and Krishnaprasad, Perinkulam Sambamurthy},
  booktitle={Proceedings of 27th Asilomar conference on signals, systems and computers},
  pages={40--44},
  year={1993},
  organization={IEEE}
}

@article{mallat1993matching,
  title={Matching pursuits with time-frequency dictionaries},
  author={Mallat, St{\'e}phane G and Zhang, Zhifeng},
  journal={IEEE Transactions on signal processing},
  volume={41},
  number={12},
  pages={3397--3415},
  year={1993},
  publisher={IEEE}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{chattopadhyay2024information,
  title={Information maximization perspective of orthogonal matching pursuit with applications to explainable ai},
  author={Chattopadhyay, Aditya and Pilgrim, Ryan and Vidal, Rene},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{bhalla2024interpreting,
  title={Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)},
  author={Bhalla, Usha and Oesterling, Alex and Srinivas, Suraj and Calmon, Flavio P and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2402.10376},
  year={2024}
}

@article{wang2024concept,
  title={Concept Algebra for (Score-Based) Text-Controlled Generative Models},
  author={Wang, Zihao and Gui, Lin and Negrea, Jeffrey and Veitch, Victor},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{park2024the,
  title={The Linear Representation Hypothesis and the Geometry of Large Language Models},
  author={Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  booktitle={Forty-first International Conference on Machine Learning}
}

@inproceedings{cheng2023bridging,
  title={Bridging Information-Theoretic and Geometric Compression in Language Models},
  author={Cheng, Emily and Kervadec, Corentin and Baroni, Marco},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={12397--12420},
  year={2023}
}

@inproceedings{lewis2024does,
  title={Does CLIP Bind Concepts? Probing Compositionality in Large Image Models},
  author={Lewis, Martha and Nayak, Nihal and Yu, Peilin and Merullo, Jack and Yu, Qinan and Bach, Stephen and Pavlick, Ellie},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2024},
  pages={1487--1500},
  year={2024}
}

@article{lv2024interpreting,
  title={Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models},
  author={Lv, Ang and Zhang, Kaiyi and Chen, Yuhan and Wang, Yulong and Liu, Lifeng and Wen, Ji-Rong and Xie, Jian and Yan, Rui},
  journal={arXiv preprint arXiv:2403.19521},
  year={2024}
}

@article{chughtai2024summing,
  title={Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs},
  author={Chughtai, Bilal and Cooney, Alan and Nanda, Neel},
  journal={arXiv preprint arXiv:2402.07321},
  year={2024}
}

@inproceedings{wang2019learning,
  title={Learning Deep Transformer Models for Machine Translation},
  author={Wang, Qiang and Li, Bei and Xiao, Tong and Zhu, Jingbo and Li, Changliang and Wong, Derek F and Chao, Lidia S},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1810--1822},
  year={2019}
}

@inproceedings{zhang2024connect,
  title={Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data},
  author={Zhang, Yuhui and Sui, Elaine and Yeung, Serena},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{li2023interpreting,
  title={Interpreting and Exploiting Functional Specialization in Multi-Head Attention under Multi-task Learning},
  author={Li, Chong and Wang, Shaonan and Zhang, Yunhao and Zhang, Jiajun and Zong, Chengqing},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={16460--16476},
  year={2023}
}
@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@article{
oquab2024dinov,
title={{DINO}v2: Learning Robust Visual Features without Supervision},
author={Maxime Oquab and Timoth{\'e}e Darcet and Th{\'e}o Moutakanni and Huy V. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel HAZIZA and Francisco Massa and Alaaeldin El-Nouby and Mido Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herve Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=a68SUt6zFt},
note={}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{yosinski2014transferable,
  title={How transferable are features in deep neural networks?},
  author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{gavrikov2022cnn,
  title={Cnn filter db: An empirical investigation of trained convolutional filters},
  author={Gavrikov, Paul and Keuper, Janis},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={19066--19076},
  year={2022}
}

@article{wold1987principal,
  title={Principal component analysis},
  author={Wold, Svante and Esbensen, Kim and Geladi, Paul},
  journal={Chemometrics and intelligent laboratory systems},
  volume={2},
  number={1-3},
  pages={37--52},
  year={1987},
  publisher={Elsevier}
}


@article{templeton2024scaling,
    title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
    author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
    year={2024},
    journal={Transformer Circuits Thread},
    url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@inproceedings{kaufman2023data,
  title={Data representations' study of latent image manifolds},
  author={Kaufman, Ilya and Azencot, Omri},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  pages={15928--15945},
  year={2023}
}

@inproceedings{
darcet2024vision,
title={Vision Transformers Need Registers},
author={Timoth{\'e}e Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=2dnO3LLiJ1}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@misc{defazio2024road,
      title={The Road Less Scheduled}, 
      author={Aaron Defazio and Xingyu Yang and Harsh Mehta and Konstantin Mishchenko and Ahmed Khaled and Ashok Cutkosky},
      year={2024},
      eprint={2405.15682},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{principal_angles,
 ISSN = {00255718, 10886842},
 URL = {http://www.jstor.org/stable/2005662},
 abstract = {Assume that two subspaces $F$ and $G$ of a unitary space are defined as the ranges (or null spaces) of given rectangular matrices $A$ and $B$. Accurate numerical methods are developed for computing the principal angles $\theta_k(F, G)$ and orthogonal sets of principal vectors $u_k \in F$ and $\upsilon_k \in G, k = 1, 2, \cdots, q = \dim(G) \leqq \dim(F)$. An important application in statistics is computing the canonical correlations $\sigma_k = \cos \theta_k$ between two sets of variates. A perturbation analysis shows that the condition number for $\theta_k$ essentially is $\max(\kappa(A), _\kappa(B))$, where $\kappa$ denotes the condition number of a matrix. The algorithms are based on a preliminary $QR$-factorization of $A$ and $B$ (or $A^H$ and $B^H$), for which either the method of Householder transformations (HT) or the modified Gram-Schmidt method (MGS) is used. Then $\cos \theta_k$ and $\sin \theta_k$ are computed as the singular values of certain related matrices. Experimental results are given, which indicates that MGS gives $\theta_k$ with equal precision and fewer arithmetic operations than HT. However, HT gives principal vectors, which are orthogonal to working accuracy, which is not generally true for MGS. Finally, the case when $A$ and/or $B$ are rank deficient is discussed.},
 author = {Björck, Åke and Golub, Gene H.},
 journal = {Mathematics of Computation},
 number = {123},
 pages = {579--594},
 publisher = {American Mathematical Society},
 title = {Numerical Methods for Computing Angles Between Linear Subspaces},
 urldate = {2024-10-01},
 volume = {27},
 year = {1973}
}

@inproceedings{
huben2024sparse,
title={Sparse Autoencoders Find Highly Interpretable Features in Language Models},
author={Robert Huben and Hoagy Cunningham and Logan Riggs Smith and Aidan Ewart and Lee Sharkey},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=F76bwRSLeK}
}

@article{marks2024sparse,
  title={Sparse feature circuits: Discovering and editing interpretable causal graphs in language models},
  author={Marks, Samuel and Rager, Can and Michaud, Eric J and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  journal={arXiv preprint arXiv:2403.19647},
  year={2024}
}

@article{krizhevsky2009learning,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Krizhevsky, Alex},
  year={2009}
}

@inproceedings{nanda2023emergent,
  title={Emergent Linear Representations in World Models of Self-Supervised Sequence Models},
  author={Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  booktitle={Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP},
  pages={16--30},
  year={2023}
}

@inproceedings{
wolff2023the,
title={The Independent Compositional Subspace Hypothesis for the Structure of {CLIP}'s Last Layer},
author={Max Wolff and Wieland Brendel and Stuart Wolff},
booktitle={ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models},
year={2023},
url={https://openreview.net/forum?id=MmhGK8YkUKO}
}

@inproceedings{
balasubramanian2024decomposing,
title={Decomposing and Interpreting Image Representations via Text in ViTs Beyond {CLIP}},
author={Sriram Balasubramanian and Samyadeep Basu and Soheil Feizi},
booktitle={ICML 2024 Workshop on Mechanistic Interpretability},
year={2024},
url={https://openreview.net/forum?id=DwhvppIZsD}
}

@inproceedings{
chen2024sudden,
title={Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in {MLM}s},
author={Angelica Chen and Ravid Shwartz-Ziv and Kyunghyun Cho and Matthew L Leavitt and Naomi Saphra},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=MO5PiKHELW}
}

@article{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{voita2019analyzing,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5797--5808},
  year={2019}
}

@article{nostalgebraist2020interpreting,
    title={interpreting GPT: the logit lens},
    author={nostalgebraist},
    year={2020},
    journal={LessWrong},
    url={https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}
}

@article{merullo2024talking,
  title={Talking Heads: Understanding Inter-layer Communication in Transformer Language Models},
  author={Merullo, Jack and Eickhoff, Carsten and Pavlick, Ellie},
  journal={arXiv preprint arXiv:2406.09519},
  year={2024}
}

@article{zhang2024spectral,
  title={Spectral Adapter: Fine-Tuning in Spectral Space},
  author={Zhang, Fangzhao and Pilanci, Mert},
  journal={arXiv preprint arXiv:2405.13952},
  year={2024}
}

@inproceedings{
shah2024decomposing,
title={Decomposing and Editing Predictions by Modeling Model Computation},
author={Harshay Shah and Andrew Ilyas and Aleksander Madry},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=rTBR0eqE4G}
}

@inproceedings{mayilvahanan2024does,
      title={Does CLIP's Generalization Performance Mainly Stem from High Train-Test Similarity?},
      author={Prasanna Mayilvahanan and Thadd{\"a}us Wiedemer and Evgenia Rusak and Matthias Bethge and Wieland Brendel},
      booktitle={The Twelfth International Conference on Learning Representations},
      year={2024},
      url={https://openreview.net/forum?id=tnBaiidobu}
}

@article{facco2017estimating,
  title={Estimating the intrinsic dimension of datasets by a minimal neighborhood information},
  author={Facco, Elena and d’Errico, Maria and Rodriguez, Alex and Laio, Alessandro},
  journal={Scientific reports},
  volume={7},
  number={1},
  pages={12140},
  year={2017},
  publisher={Nature Publishing Group UK London}
}

@article{levina2004maximum,
  title={Maximum likelihood estimation of intrinsic dimension},
  author={Levina, Elizaveta and Bickel, Peter},
  journal={Advances in neural information processing systems},
  volume={17},
  year={2004}
}

@article{valeriani2024geometry,
  title={The geometry of hidden representations of large transformer models},
  author={Valeriani, Lucrezia and Doimo, Diego and Cuturello, Francesca and Laio, Alessandro and Ansuini, Alessio and Cazzaniga, Alberto},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ansuini2019intrinsic,
  title={Intrinsic dimension of data representations in deep neural networks},
  author={Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H and Zoccolan, Davide},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{
ilharco2023editing,
title={Editing models with task arithmetic},
author={Gabriel Ilharco and Marco Tulio Ribeiro and Mitchell Wortsman and Ludwig Schmidt and Hannaneh Hajishirzi and Ali Farhadi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=6t0Kwf8-jrj}
}

@article{ortiz2024task,
  title={Task arithmetic in the tangent space: Improved editing of pre-trained models},
  author={Ortiz-Jimenez, Guillermo and Favero, Alessandro and Frossard, Pascal},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{rahaman2019spectral,
  title={On the spectral bias of neural networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle={International conference on machine learning},
  pages={5301--5310},
  year={2019},
  organization={PMLR}
}

@inproceedings{10.5555/3295222.3295349,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is all you need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{engels2024not,
  title={Not All Language Model Features Are Linear},
  author={Engels, Joshua and Liao, Isaac and Michaud, Eric J and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2405.14860},
  year={2024}
}

@inproceedings{
jiang2024on,
title={On the Origins of Linear Representations in Large Language Models},
author={Yibo Jiang and Goutham Rajendran and Pradeep Kumar Ravikumar and Bryon Aragam and Victor Veitch},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=otuTw4Mghk}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}
@inproceedings{krause20133d,
  title={3d object representations for fine-grained categorization},
  author={Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE international conference on computer vision workshops},
  pages={554--561},
  year={2013}
}
@article{helber2019eurosat,
  title={Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification},
  author={Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume={12},
  number={7},
  pages={2217--2226},
  year={2019},
  publisher={IEEE}
}
@inproceedings{stallkamp2011german,
  title={The German traffic sign recognition benchmark: a multi-class classification competition},
  author={Stallkamp, Johannes and Schlipsing, Marc and Salmen, Jan and Igel, Christian},
  booktitle={The 2011 international joint conference on neural networks},
  pages={1453--1460},
  year={2011},
  organization={IEEE}
}
@article{cheng2017remote,
  title={Remote sensing image scene classification: Benchmark and state of the art},
  author={Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},
  journal={Proceedings of the IEEE},
  volume={105},
  number={10},
  pages={1865--1883},
  year={2017},
  publisher={IEEE}
}
@article{xiao2016sun,
  title={Sun database: Exploring a large collection of scene categories},
  author={Xiao, Jianxiong and Ehinger, Krista A and Hays, James and Torralba, Antonio and Oliva, Aude},
  journal={International Journal of Computer Vision},
  volume={119},
  pages={3--22},
  year={2016},
  publisher={Springer}
}
@inproceedings{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Baolin and Ng, Andrew Y and others},
  booktitle={NIPS workshop on deep learning and unsupervised feature learning},
  volume={2011},
  number={2},
  pages={4},
  year={2011},
  organization={Granada}
}

@article{lei2016layer,
  title={Layer normalization},
  author={Lei Ba, Jimmy and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={ArXiv e-prints},
  pages={arXiv--1607},
  year={2016}
}

@inproceedings{cimpoi2014describing,
  title={Describing textures in the wild},
  author={Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3606--3613},
  year={2014}
}
@inproceedings{li2017deeper,
  title={Deeper, broader and artier domain generalization},
  author={Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5542--5550},
  year={2017}
}

@misc{cancedda2024spectralfiltersdarksignals,
      title={Spectral Filters, Dark Signals, and Attention Sinks}, 
      author={Nicola Cancedda},
      year={2024},
      eprint={2402.09221},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.09221}, 
}

@inproceedings{asif,
 author = {Norelli, Antonio and Fumero, Marco and Maiorca, Valentino and Moschella, Luca and Rodol\`{a}, Emanuele and Locatello, Francesco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {15303--15319},
 publisher = {Curran Associates, Inc.},
 title = {ASIF: Coupled Data Turns Unimodal Models to Multimodal without Training},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/3186591903d9db31770ad131adb5ceb4-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@software{Falcon_PyTorch_Lightning_2019,
author = {Falcon, William and {The PyTorch Lightning team}},
doi = {10.5281/zenodo.3828935},
license = {Apache-2.0},
month = mar,
title = {{PyTorch Lightning}},
url = {https://github.com/Lightning-AI/lightning},
version = {1.4},
year = {2019}
}

@inproceedings{
cannistraci2024from,
title={From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication},
author={Irene Cannistraci and Luca Moschella and Marco Fumero and Valentino Maiorca and Emanuele Rodol{\`a}},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=vngVydDWft}
}


@InProceedings{pmlr-v243-lahner24a,
  title = 	 {On the Direct Alignment of Latent Spaces},
  author =       {L\"ahner, Zorah and Moeller, Michael},
  booktitle = 	 {Proceedings of UniReps: the First Workshop on Unifying Representations in Neural Models},
  pages = 	 {158--169},
  year = 	 {2024},
  editor = 	 {Fumero, Marco and Rodolá, Emanuele and Domine, Clementine and Locatello, Francesco and Dziugaite, Karolina and Mathilde, Caron},
  volume = 	 {243},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {15 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v243/lahner24a/lahner24a.pdf},
  url = 	 {https://proceedings.mlr.press/v243/lahner24a.html},
  abstract = 	 {With the wide adaption of deep learning and pre-trained models rises the question of how to effectively reuse existing latent spaces for new applications.One important question is how the geometry of the latent space changes in-between different training runs of the same architecture and different architectures trained for the same task. Previous works proposed that the latent spaces for similar tasks are approximately isometric. However, in this work we show that method restricted to this assumption perform worse than when just using a linear transformation to align the latent spaces. We propose directly computing a transformation between the latent codes of different architectures which is more efficient than previous approaches and flexible wrt. to the type of transformation used. Our experiments show that aligning the latent space with a linear transformation performs best while not needing more prior knowledge.}
}

@inproceedings{
moschella2023relative,
title={Relative representations enable zero-shot latent space communication},
author={Luca Moschella and Valentino Maiorca and Marco Fumero and Antonio Norelli and Francesco Locatello and Emanuele Rodol{\`a}},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=SrC-nwieGJ}
}

@misc{smith2017cyclicallearningratestraining,
      title={Cyclical Learning Rates for Training Neural Networks}, 
      author={Leslie N. Smith},
      year={2017},
      eprint={1506.01186},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1506.01186}, 
}

@inproceedings{
  liang2022mind,
  title={Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning},
  author    = {Weixin Liang and
               Yuhui Zhang and
               Yongchan Kwon and
               Serena Yeung and
               James Zou},
  booktitle={NeurIPS},
  year={2022},
  url={https://openreview.net/forum?id=S7Evzt9uit3}
}

@article{HYVARINEN1999429,
title = {Nonlinear independent component analysis: Existence and uniqueness results},
journal = {Neural Networks},
volume = {12},
number = {3},
pages = {429-439},
year = {1999},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(98)00140-3},
url = {https://www.sciencedirect.com/science/article/pii/S0893608098001403},
author = {Aapo Hyvärinen and Petteri Pajunen},
keywords = {Independent component analysis, Blind source separation, Redundancy reduction, Feature extraction},
abstract = {The question of existence and uniqueness of solutions for nonlinear independent component analysis is addressed. It is shown that if the space of mixing functions is not limited there exists always an infinity of solutions. In particular, it is shown how to construct parameterized families of solutions. The indeterminacies involved are not trivial, as in the linear case. Next, it is shown how to utilize some results of complex analysis to obtain uniqueness of solutions. We show that for two dimensions, the solution is unique up to a rotation, if the mixing function is constrained to be a conformal mapping together with some other assumptions. We also conjecture that the solution is strictly unique except in some degenerate cases, as the indeterminacy implied by the rotation is essentially similar to estimating the model of linear ICA.}
}