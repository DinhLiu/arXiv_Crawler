\documentclass[10pt]{article} % For LaTeX2e
\usepackage[preprint]{tmlr}
\input{math_commands.tex}

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\usepackage{algorithm2e}
\usepackage{xcolor}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
% \usepackage{colortbl}
\usepackage{booktabs}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{cleveref}
\usepackage{subcaption}
\usepackage{wrapfig}
% \newcommand\equaltext[1]{\stackrel{\mathclap{\normalfont\mbox{#1}}}{=}}
\newcommand\equaltext[1]{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily #1}}}{=}}}

\definecolor{brown}{HTML}{846358}
\definecolor{purple}{HTML}{6741d9}
\definecolor{orange}{HTML}{f08c00}
\definecolor{green}{HTML}{2f9e44}

% Define a heatmap color function

% \newcommand{\vale}[1]{{\color{blue}{(\textbf{V}: #1)}}}  
% \newcommand{\lore}[1]{{\color{red}{(\textbf{L}: #1)}}}  
% \newcommand{\FL}[1]{\textbf{\color{violet}FL:}{~\color{cyan}#1}}
% \title{ResiDual: Spectral Analysis and Adaptation of Transformer Units}
\title{ResiDual Transformer Alignment \\with Spectral Decomposition}
% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{
\centering
Lorenzo Basile$^{1,*,\dagger}$, 
Valentino Maiorca$^{2,*,\dagger}$, \\
\vspace{0.5em}
Luca Bortolussi$^{1}$,
Emanuele Rodolà$^2$, 
Francesco Locatello$^3$\\
\normalfont
\vspace{1em}
\addr{$^1$University of Trieste} \\
\addr{$^2$Sapienza University of Rome} \\
\addr{$^3$Institute of Science and Technology Austria (ISTA)} \\
\vspace{0.5em}
\small{$^*$Equal contribution. \\ $^\dagger$Work done while visiting ISTA}
}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\blfootnote{
Correspondence to: lorenzo.basile@phd.units.it and maiorca@di.uniroma1.it
}
\begin{abstract}
When examined through the lens of their residual streams, a puzzling property emerges in transformer networks: 
residual contributions (e.g., attention heads) sometimes specialize in specific tasks or input attributes. 
% 
In this paper, we analyze this phenomenon in vision transformers, focusing on the spectral geometry of residuals, and explore its implications for modality alignment in vision-language models.
% 
First, we link it to the intrinsically low-dimensional structure of visual head representations, zooming into their principal components and showing that they encode specialized roles across a wide variety of input data distributions. 
% 
Then, we analyze the effect of head specialization in multimodal models, focusing on how improved alignment between text and specialized heads impacts zero-shot classification performance. This specialization-performance link consistently holds across diverse pre-training data, network sizes, and objectives, demonstrating a powerful new mechanism for boosting zero-shot classification through targeted alignment.
% 
Ultimately, we translate these insights into actionable terms by introducing ResiDual, a technique for spectral alignment of the residual stream.
Much like panning for gold, it lets the noise from irrelevant unit principal components (i.e., attributes) wash away to amplify task-relevant ones.
% 
Remarkably, this dual perspective on modality alignment yields fine-tuning level performances on different data distributions while modeling an extremely interpretable and parameter-efficient transformation, as we extensively show on more than 50 (pre-trained network, dataset) pairs.
\end{abstract}

% brown/output 846358
% purple/patttern 6741d9
% orange/shape f08c00
% green/color 2f9e44

\section{Introduction}
\begin{figure}
\centering
    \includegraphics[width=\textwidth]{figures/residual_teaser.pdf}
\caption{From the transformer's residual stream, we can obtain direct head contributions from across the network. In a multimodal, zero-shot classification setting (e.g., in CLIP) task boundaries originate from text that can be at different level of concept granularity. When some heads are specialized in some properties (e.g., \textcolor{orange}{shape}, \textcolor{purple}{pattern}, \textcolor{green}{color}), they can be better aligned to apply some boundaries than the original output. In this example, only the fine-grained task (\textcolor{brown}{brown}) can correctly separate the samples at the output level.}
%be applied to the output to correctly separate the samples.}
\label{fig:teaser}
\end{figure}


In recent times, transformers have become the backbone of most state-of-the-art machine learning systems, thanks to their adaptability to various domains, including language modeling \citep{brown2020language, touvron2023llama}, vision \citep{dosovitskiy2021an, radford2021learning} and many different scientific domains~\citep{espeholt2022deep,jumper2021highly,merchant2023scaling}.
Traditionally, these models are treated as producing a unique, monolithic output.
However, a key component in the success of transformers is the versatile inductive bias introduced by multi-head attention (MHA) layers, which alternate with multi-layer perceptrons (MLP) to form any transformer-based architecture. MHA layers are made of several independent computational units, called heads, that process input data in parallel and update the residual stream that carries it to the output via skip connections.

Similarly to what had been observed for filters of convolutional neural networks \citep{yosinski2014transferable, gavrikov2022cnn}, recent works point to the emergence of a \textit{specialization} property in attention heads, both in large language models \citep{voita2019analyzing, li2023interpreting, chughtai2024summing} and in the visual branch of CLIP models \citep{gandelsman2024interpreting}.
Specialization seems to be a by-product related to large-scale training, but it is not clear exactly why it emerges and whether this is a systematic property. Interestingly, this property implies that different units might learn to attend to specific attributes or to solve specific tasks, thus processing the input in a disentangled manner, overcoming known theoretical challenges \citep{HYVARINEN1999429,locatello2019challenging}.

In modern transformer networks, the model’s final output is produced by applying a simple linear transformation to the residual stream (up to LayerNorm). This residual stream accumulates information additively, drawing from each attention head and all MLP layers \citep{elhage2021mathematical}, producing a general-purpose representation used as a feature set for many tasks.
This decomposition raises an intriguing question: are all these units essential for solving specific tasks, or do some introduce noise that obscures task-relevant information? Typically, there’s a trade-off between a model’s generalization and its performance on specific tasks. However, it might be that to specialize a model for a specific task, we don’t need to retrain the whole model. Instead, by manipulating the residual stream, we can boost the units already aligned with the task, amplifying relevant signals while reducing noise.

In this paper, we tackle this question from the perspective of the latent geometry of residual units. First, on a variety of transformer-based vision models, including multiple versions of CLIP \citep{radford2021learning}, BLIP \citep{li2022blip}, ViT \citep{dosovitskiy2021an} and DINOv2 \citep{oquab2024dinov}, we show that such units are embedded in low-dimensional manifolds and that, when there is specialization, it can be traced back to the role of few principal components. Then, by introducing a spectral analysis method based on a discrete version of principal angles~\citep{principal_angles}, we quantitatively measure the similarity of residual units across different datasets revealing that the roles of specialized units remain surprisingly stable.

Building on this insight, we hypothesize that, in many cases, the information necessary for solving a task is already embedded within a subset of highly specialized residual units. We show that this picture emerges clearly in vision-language models like CLIP, where we find units or sets of units that align with textual attributes more precisely than the full model output, on a given task. In fact, as the output combines all residual units, this relevant information may be obfuscated by other units that introduce irrelevant signals. Instead of fine-tuning the entire model, we propose to isolate and enhance the task-relevant units by filtering out the noise - akin to panning for gold. By doing so, we can significantly boost model performance with up to 4 orders or magnitude less parameters than full fine-tuning and 2 less than those needed for training a simple linear transformation at the output level.
To implement this, we introduce \textit{ResiDual}, a novel approach that focuses on the principal components (PCs) of the residual units to identify and retain the needed information. This framework selectively reweights the most relevant PCs, amplifying the signals that align with the task objective while remaining computationally efficient.  
This spectral reweighting of individual units addresses nonlinear interactions between them, and provides a geometrically principled and interpretable method for optimizing transformer models by capitalizing on the knowledge they already possess. 

In summary, our contributions are as follows:
\begin{itemize}
\item We inspect the geometric structure of attention head representations in vision transformers, showcasing their low dimensionality and their increasing nonlinearity along model depth;
\item We characterize the emergent specialization of attention heads through their principal components and show that it stays consistent across data distributions;
\item We identify task-specific units in vision-language models, showcasing that focusing on these units in zero-shot settings can outperform using the full residual output when there is latent alignment between units and tasks.
\item We present \textit{ResiDual}, a geometrically grounded method for manipulating transformers by reweighting the most relevant principal components of the residual units. This approach can sidestep the need for full-model finetuning, as it reaches competitive performance with minimal parameter overhead.
\end{itemize}

\section{Related Work}

\paragraph{Transformer Residual Decomposition}
Transformer networks \citep{vaswani2017attention} rely on residual connections around each multi-head attention (MHA) and MLP layer, resulting in a final representation that combines contributions from all units across layers by simple summation. Techniques like logit lens \citep{nostalgebraist2020interpreting}, Direct Logit Attribution (DLA) \citep{elhage2021mathematical} -- and \cite{cancedda2024spectralfiltersdarksignals}, at the spectral level -- focus on how individual layers or residual units (such as MLPs or attention heads) affect the final output in logit space, given that their contributions are projected upstream via linear transformations (up to LayerNorm \citep{lei2016layer}, an affine one).
% 
Here, we apply this residual decomposition to provide a more comprehensive understanding of how these units interact and align across different tasks, revealing the deeper structure within the residual space.

\paragraph{Residual Properties}
To understand the geometric nature of latent manifolds, previous works analyze the intrinsic dimensionality ($I_d$) \citep{ansuini2019intrinsic, cheng2023bridging, valeriani2024geometry}
of the representations within the network, which is typically much lower than the embedding dimension.
% 
We posit that the Linear Representation Hypothesis \citep{park2024the, jiang2024on}, 
which suggests that transformer representations encode high-level concepts in linear directions, complements this view hinting at transformer models sometimes modulating input attributes in a linearly structured and low-dimensional (i.e. specialized) way.
%  
Previous works in language modeling have highlighted this specialization \citep{voita2019analyzing, michel2019sixteen, li2023interpreting, lv2024interpreting, chughtai2024summing}, revealing that only a few attention heads are responsible for specific tasks and that they assume specialized and interpretable roles. 
% 2
Our analysis bridges geometry and specialization, revealing that vision transformer heads are low-dimensional (though often nonlinear, especially in deeper layers) and highly specialized for downstream tasks.

\paragraph{Multimodal Alignment}
In multimodal models such as CLIP \citep{radford2021learning}, it is well known that the vision and text branches operate in neatly separated latent spaces \citep{liang2022mind}.
% 
Despite this modality gap, \citet{chattopadhyay2024information} and     \citet{bhalla2024interpreting} leverage the multimodal latent space of CLIP to find sparse decompositions of its representations using text. Similarly, \citet{gandelsman2024interpreting} show that text encodings can align with specific head-level representations of CLIP’s visual branch, providing insights into the specialized roles of individual heads through manually crafted textual inputs.
% 
\citet{balasubramanian2024decomposing} generalize this approach to unimodal vision transformers and arbitrary residual units through a scoring function and the estimation of an aligning transformation between the spaces.
The idea that CLIP is able to disentangle concepts and encode them in separate subspaces also appears in \citet{wolff2023the} and \citet{lewis2024does}. Here, we study the modality alignment at the spectral level, reaching the granularity of head principal components, and show how they can be used to improve the alignment between text and visual branches in CLIP-like models. 

\section{The Geometry of Residual Units}
In this section, we examine the relationship between head specialization and the low-dimensional nature of head manifolds. Initially, head representations exist in a relatively low-dimensional ambient space. Through a linear transformation, however, they are subsequently embedded into a higher-dimensional space within the residual stream \citep{elhage2021mathematical}, which shares the same dimensionality as the model’s output. At this point, head representations are transcribed to the residual stream, and they contribute additively to the final output of the model. In fact, throughout the paper, we will assume that the model output is the summation of the encodings of all residual units (attention heads $\mH$, MLPs $\mM$ and input embeddings $\mX_0$):
\begin{equation}\label{eq:decomposition}
    \mY = \sum_{i=1}^{|\sU|}\mU_i = \mX_0 + \sum_{i=1}^{|\sH|}\mH_i + \sum_{j=1}^{|\sM|}\mM_j
\end{equation}
With $\mY$ as the final output of the model, summing up all the residual units in $\mU \in \sU$.
Please refer to \Cref{ssec:decomposition} for a more rigorous description of the residual decomposition.
\subsection{Residual Dimensionality}\label{ssec:residual_dimensionality}

Despite being embedded into a higher-dimensional space, head representations exhibit an even lower \textit{intrinsic dimensionality} ($I_d$) than that of the original ambient space. This indicates that, irrespective of their high-dimensional embedding, the essential structure of head representations is highly compressed and governed by a compact, low-dimensional geometry.
% 
In short, the intrinsic dimensionality of a dataset is the least number of variables required to satisfactorily describe the data points. Ideally, if data lie on a linear manifold (a hyperplane), the intrinsic dimensionality coincides with the number of principal components required to completely explain their variance. In a more realistic setting, data lie on curved, nonlinear manifolds, and linear estimators like PCA fail to capture their real intrinsic dimensionality. In such cases, one can resort to nonlinear $I_d$ estimators. Among them, we choose to employ the TwoNN \citep{facco2017estimating} because of its efficiency and stability on complex and non-uniform manifolds.

\paragraph{Experimental setting}
We start by evaluating the intrinsic dimensionality of head representations across multiple transformer-based vision architectures pre-trained with different objectives (supervised, unsupervised, self-supervised). Namely, we employ OpenAI's CLIP \citep{radford2021learning}, OpenCLIP \citep{cherti2023reproducible}, BLIP \citep{li2022blip}, ViT \citep{dosovitskiy2021an} and DINOv2 \citep{oquab2024dinov}, all in their version based on ViT-Large (results on ViT-Base models are in the Appendix in \Cref{fig:id_heads_base}). We feed them a subset of the training set of ImageNet \citep{russakovsky2015imagenet} containing 80000 images stratified on the class labels and we extract the representations for all attention heads. Then, we compute the intrinsic dimensionality of such representations using a linear estimator (PCA) and a nonlinear one (TwoNN). Linear $I_d$ is computed as the number of components needed by PCA to explain 99\% of head variance.
\begin{figure}[h]
\centering
    \includegraphics[width=\textwidth]{figures/id_columns_99.pdf}
\caption{Heads in early layers show low-dimensional, linear structures, as suggested by similar intrinsic dimension estimates from PCA (\textbf{L}) and TwoNN (\textbf{N}). Moving toward the output layer, the nonlinear dimensionality peaks and then decreases, while PCA’s linear estimate continues to rise, indicating increasing nonlinearity in head manifolds (\textbf{Ratio} = $\frac{L}{N}$). The first principal component (\textbf{EVR$_1$}) explains around 50\% of the variance in early layers, dropping to around 10\% in later layers.
}
\label{fig:id_heads}
\end{figure}
\paragraph{Result analysis} We report our results in \Cref{fig:id_heads}.
We observe that the \textit{true} head dimensionality (the one computed with a nonlinear estimator, TwoNN) tends to increase in the first half of the model and to decrease towards the last few layers, following a characteristic hunchback shape, similar to previous findings in other vision architectures \citep{ansuini2019intrinsic}. However, the number of dimensions returned by the linear estimator grows constantly through the model. This disparity, witnessed by the increasing ratio between the two estimates, indicates that the units of the first layers are close to linear, while in late layers they lie on more curved manifolds. The last column shows the average explained variance ratio (EVR) of the first PCA component and highlights that heads in the first layers are largely explained by this direction, while it still accounts for a nontrivial 10\% of head variance in late layers.

These findings highlight that low head dimensionality and monotonically increasing nonlinearity arise along the residual streams of vision transformers, regardless of pre-training objective and data.

\subsection{Principal Components Encode Unit Semantics}\label{ssec:omp}
The low dimensionality of head encodings results in relevant consequences for their interpretability. Head representations can be easily approximated with sparse recovery algorithms, in a way that is akin of performing PCA, but over a discrete set of vectors. For CLIP models, this approach has been recently explored by \citet{gandelsman2024interpreting}. There, the authors introduce a sparse approximation algorithm, TextSpan (TS), and decompose head encodings using a set of textual descriptions, coming from the text branch of CLIP, as a dictionary. They observe strong specialization properties, witnessed by high coherence in the textual explanations of each head.
We link TextSpan to the more established family of Matching Pursuit (MP) \citep{mallat1993matching} algorithms, widely employed in signal processing. More specifically, as we show in the \Cref{ssec:proof}, TextSpan is analogous to Simultaneous Orthogonal Matching Pursuit (SOMP) \citep{tropp2006algorithms}, with light modifications.
TextSpan, like any MP algorithm, approximates the signal through linear combinations of basis functions. 
Considering the high nonlinearity of later layers (\Cref{ssec:residual_dimensionality}), and TS being a linear sparse approximation method, we now want to investigate whether TS is, in reality, focusing on the first principal components of the signal (head-level representations).

\paragraph{Experimental setting}
For this experiment, we position ourselves in the same setup of the original TS paper \citep{gandelsman2024interpreting}. Hence, we consider the attention heads belonging to the last 4 layers of OpenCLIP-L. We use two sparse approximation algorithms: the original TextSpan, which operates on the whole head representation, and Orthogonal Matching Pursuit (OMP) \citep{pati1993orthogonal}. Different from TextSpan, OMP computes sparse approximations of vectors, not matrices (like head representations). Therefore, in this experiment, we apply OMP to the first principal component of each head. We denote this method as OMP$_1$.
The dictionary we use contains the encodings produced by OpenCLIP-L for the set of image descriptions provided by \citet{gandelsman2024interpreting}. We apply both algorithms to select 5 descriptions for each head and compute an agreement score between the two sets. The agreement is computed as the absolute Z-score of the cosine similarity (\texttt{sim}) between them, compared with the average cosine similarity $\mu$ between the descriptions selected by TextSpan and the entire dictionary $Z=\frac{|\overline{\texttt{sim}(\text{TS}, \text{OMP}_1)}-\mu|}{\sigma}$.

\paragraph{Result analysis} We report in the left panel of \Cref{fig:ts_omp} the agreement scores. The right panel reports a few examples (one per layer) of descriptions obtained using the two algorithms. 
We observe that a high Z-score (e.g., head 8 of layer 22, which is almost $5\sigma$ away from $\mu$), is reflected in extremely similar descriptions from the two methods. 
When the agreement is lower, as in the case of head 20 from layer 8, even though they share some high-level semantics, the two sets of descriptions substantially differ. 

Overall, this analysis indicates that, in some cases, the first principal component captures nearly all the essential information about the head’s specialized semantics. In other cases, the head’s role appears to be distributed across multiple components.

\begin{figure}[h]
    \centering
    % Left subfigure (image)
    \begin{minipage}{0.23\textwidth}
        %\centering
        \includegraphics[width=\textwidth]{figures/omp_textspan_1.pdf} % Replace with your image path
        \label{fig:subfig-image}
    \end{minipage}%
    % Right subfigure (table)
    \begin{minipage}{0.7\textwidth}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{ll}
\toprule
\textbf{TextSpan} & \textbf{OMP$_1$} \\
\midrule
\textbf{L20.H8} (``Scenery'') & \textbf{L20.H8} ($Z=0.02$) \\
\midrule
Photo taken in Galápagos Islands & Unspoiled beauty \\
Image taken in Norway & Picture taken in Portugal \\
Evocative beauty & Crisp autumn leaves \\
Vibrant urban energy & Evocative candid gaze \\
A skirt & Picture taken in Cyprus \\
\midrule
\textbf{L21.H11} (``Location'') & \textbf{L21.H11} ($Z=2.73$) \\
\midrule
Picture taken in Cyprus & Picture taken in Cyprus \\
Picture taken in Ontario, Canada & Picture taken in the Canadian lakes \\
Photo taken in Rio de Janeiro, Brazil & Image taken in the Florida Everglades \\
Photo captured in the Arizona desert & Image taken in New England \\
Picture captured in the Scottish highlands & Warm and cozy indoor scene \\
\midrule
\textbf{L22.H8} (``Letters'') & \textbf{L22.H8} ($Z=4.93$) \\
\midrule
A photo with the letter F & A photo with the letter F \\
A photo with the letter V & A photo with the letter P \\
A photo with the letter D & A photo with the letter T \\
A photo with the letter T & A photo with the letter X \\
A photo with the letter X & A photo with the letter B \\
\midrule
\textbf{L23.H2} (``Animals'') & \textbf{L23.H2} ($Z=1.91$) \\
\midrule
Image showing prairie grouse & Picture of a feline \\
Image with a penguin & An image with dogs \\
A magnolia & Photo of a furry animal \\
An image with dogs & Photo taken in Grand Canyon \\
An image with cats & An image with cats \\
\bottomrule
\end{tabular}
}
\vspace{0.9cm}
    \end{minipage}
    \caption{Comparison between TextSpan and Ortogonal Matching Pursuit on the first principal component (OMP$_1$), applied to the heads of OpenCLIP-L. \textit{Left}: agreement score between the descriptions returned by the two methods. \textit{Right}: qualitative comparison of selected descriptions for 4 heads, one per layer, at different agreement levels. A similar analysis for the second principal component is presented in the Appendix in \Cref{sup:fig:ts_omp}.}
    \label{fig:ts_omp}
\end{figure}

\subsection{Spectral Dataset Comparison}\label{ssec:cross_dataset}
Our aim is now to understand to what extent specialization generalizes across different input data distributions.
To do so, we introduce a spectral metric to compare the representations of residual units. Since units are low-dimensional (\Cref{ssec:residual_dimensionality}) and their specialization is deeply impacted by a few principal components (\Cref{ssec:omp}), we define a metric to quantify the similarity between their PCA bases, inspired by principal angles \citep{principal_angles}.

Let \(\mathcal{S}_1\) and \(\mathcal{S}_2\) be two subspaces of dimensions \(k_1\) and \(k_2\) in an \(d\)-dimensional space. The principal angles \(\theta_n\) for \(n = 1, \dots, \min(k_1, k_2)\) are given by:

\begin{equation}
\cos \theta_n = \max_{\vu \in \mathcal{S}_1^{\perp \mathcal{U}_{n-1}} , \vv \in \mathcal{S}_2^{\perp \mathcal{V}_{n-1}}} \frac{\vu^\top \vv}{\|\vu\| \|\vv\|}
\end{equation}

where \(\mathcal{U}_{n-1}\) and \(\mathcal{V}_{n-1}\) are the span of \(\{\vu_1, \dots, \vu_{n-1}\}\) and \(\{\vv_1, \dots, \vv_{n-1}\}\), respectively.

Now, let \(\sS_1 = \{\vu_1, \dots, \vu_{k_1}\}\) and \(\sS_2 = \{\vv_1, \dots, \vv_{k_2}\}\) represent sets of $\ell_2$-normalized discrete vectors (e.g., principal components) with (optional) associated weights \(w_1^i\) and \(w_2^j\) (e.g., singular values).

We define the \textbf{spectral cosine similarity} $s_n$ for \(n = 1, \dots, \min(k_1, k_2)\) as:

\begin{equation}
s_n = [\max_{\substack{i \not\in \{i_1, \dots, i_{n-1}\} \\ j \not\in \{j_1, \dots, j_{n-1}\}}} (\vu_i^\top \vv_j)] w_1^i w_2^j
\end{equation}

where \(i_1, \dots, i_{n-1}\) and \(j_1, \dots, j_{n-1}\) are previously selected indices.


The original principal angles measure the alignment between \textbf{subspaces} by maximizing cosine similarity of vectors in their \textbf{span}. In our discrete case, vectors are selected from sets \(\sS_1\) and \(\sS_2\) directly, with optional weighting.
The final measure is the aggregation of the spectral cosine similarities along the $\min(k_1, k_2)$ entries, with normalization to bound our measure between 0 and 1.
We define the \textbf{normalized spectral cosine similarity} between the two sets of principal components as:

\begin{equation}
\text{sim}(\sS_1, \sS_2) = 
\sqrt{\frac{\sum_{n=1}^{\min(k_1, k_2)} s_n^2}{\sum_{n=1}^{\min(k_1, k_2)} (w_1^n w_2^n)^2}}
\end{equation}

In the following, we apply this measure to compare residual units across different input datasets. It is worth noting here that the main advantage of this formulation, compared to standard approaches to representation similarity, is that our metric does not rely on the alignment between samples, as it operates in the dual spectral space. This edge is crucial in our application to different datasets, which even vary in size.

\paragraph{Experimental setting}
 We consider the same ViT-based encoders of \Cref{ssec:residual_dimensionality}, and 14 different datasets: 
 ImageNet (the same split used in \Cref{ssec:residual_dimensionality}), CIFAR(-100/-10) \citep{krizhevsky2009learning}, ImageNet-Sketch \citep{wang2019learning}, Cars \citep{krause20133d}, MNIST \citep{lecun1998gradient}, SVHN \citep{netzer2011reading}, EuroSAT \citep{helber2019eurosat}, RESISC45 \citep{cheng2017remote}, DTD \citep{cimpoi2014describing}, SUN397 \citep{xiao2016sun}, GTSRB \citep{stallkamp2011german}, PACS \citep{li2017deeper}, and random images (10000 samples with RGB values in $[-1,1]$). We use the original train/validation/test splits if available, otherwise we produce the splits through a stratified random sampling over the classes. For each encoder, we use our similarity measure to compare its unit representations produced on each training dataset with the ones obtained on the training split of ImageNet. ImageNet is taken as a reference under the assumption that, being a general enough dataset, its head PCA bases are sufficiently comprehensive to approximate the primary features across other datasets. Additionally, we perform a qualitative inspection of a few heads that stand out by finding their textual decomposition. For this step, we use Simultaneous Orthogonal Matching Pursuit (SOMP), having established its strong relationship with TextSpan (\Cref{ssec:proof}).


\paragraph{Result analysis}

The results of the spectral head-to-head comparison between ImageNet and all other datasets on OpenCLIP-L are reported in \Cref{fig:head_comparison} (results on other models can be found in the Appendix in \cref{sup:sec:head_comparison}). Rows are ordered according to the mean overall similarity between the corresponding dataset and ImageNet. Interestingly, dataset ordering is consistent across different encoders. The mean correlation coefficient between dataset similarities (averaged over all heads) across different models is 0.97. The full comparison between encoders is reported in the Appendix (\Cref{sup:fig:pearson_models}). On OpenCLIP-L, we observe that datasets that maximally align with ImageNet share classes with it (e.g., SUN397 and Sketch) and/or contain generic images (e.g., DTD and Cars). Moreover, datasets that share the same input image structure and concepts (CIFAR-10 and CIFAR-100) have an almost identical similarity distribution across heads. 
Overall, we observe a decreasing trend in head similarity scores as depth in the model increases. The simple, linear heads of the first layers are responsible for the extraction of low-level patterns \citep{dosovitskiy2021an} and emerge as almost always identical across different data distributions. On the last layers, just a few heads per dataset stand out: this is where we are looking for specialization. Zooming in on a few of these heads, in \Cref{tab:descriptions_exp2} we report their textual descriptions obtained with SOMP. Head 7 of layer 22 (specialized on seasons) stands out because it is highly activated in many datasets that contain pictures of scenery (such as SUN397, GTSRB, and more prominently EuroSAT). Head 11 of layer 22 (specialized on shades of gray) emerges as extremely different between ImageNet and Sketch, that contains grayscale drawings of ImageNet classes. Head 10 of layer 23 (specialized on numbers) is highly activated on both MNIST and SVHN. We note that this is not the only 'shared' head between the two, but others, like head 1 of the same layer, are also activated by the random dataset, signaling that they are not as specific.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.87\linewidth}
        % \centering
        \includegraphics[width=\linewidth]{figures/openclip_l_allheads.pdf}
        \caption{}
        \label{fig:head_comparison_a}
    \end{subfigure}
    
    \begin{subfigure}{0.9\linewidth}
        \centering
        \begin{tabular}{lll}
            \toprule
            \textbf{L22.H7} (``Seasons'') & \textbf{L22.H11} (``Grayscale'') & \textbf{L23.H10} (``Numbers'') \\
            \midrule
            Serene winter wonderland & A charcoal gray color & Image with six subjects \\
            A photo taken in the summer & Minimalist white backdrop & An image of three subjects \\
            A photo taken in the fall & Sepia-toned photograph & An image of the number 9 \\
            A photo taken in the spring & An amber color & The number fifteen \\
            Serene garden oasis & High-contrast black and white & Image with four people \\
            \bottomrule
        \end{tabular}
        \caption{}
        \label{tab:descriptions_exp2}
    \end{subfigure}
    \caption{\textit{(a)} Attention head similarity across layers of OpenCLIP-L, computed between ImageNet head representations and those obtained on other datasets. \textit{(b)} Descriptions picked by SOMP for three specialized heads that emerge from the analysis of panel \textit{(a)}.}
    \label{fig:head_comparison}
\end{figure}

From these findings, we observe that the similarity measure yields intuitive scores, with ImageNet’s foundational attributes demonstrating generalizability across various data distributions.

\section{ResiDual Alignment}
With a refined understanding of head specialization, our objective is now to leverage this property to enhance alignment between visual unit representations (both heads and MLPs) and text encodings in CLIP-like models. From this point onwards, our experiments will have a shared objective: given a zero-shot text classifier, i.e., text encodings for classes, we manipulate the residual to improve its alignment with the text subspace. Improved alignment directly benefits multimodal tasks such as zero-shot classification, as it strengthens the model capacity to interpret visual data through text-based descriptors.

\subsection{Coarse Unit Alignment}\label{ssec:coarse_selection}

We start by exploring whether certain heads are already aligned with the text subspace of interest for our task. 
Specifically, we aim to optimize vision-text alignment by combining visual heads selected using various scoring functions.

\paragraph{Experimental setting}


We have 3 different selection methods: i) \textbf{Unsupervised (U):} We use the head-to-output correlation as a measure. Intuitively, the more a head correlates to the full output, the more information it carries. In practice, we compute the Pearson correlation between each sample at the head level and its corresponding output encoding, averaging across samples to obtain a scalar; ii) \textbf{Task-conditioned Unsupervised (U|T)}: Conditioning on the output alone does not necessarily imply that the selected heads will be suitable for a given task. Since the task is modeled by a text subspace, having one encoding for each class, we can condition the previous unsupervised measure to be applied only on the head and output subspaces spanned by the task encodings. This has the effect of ignoring features that might influence the correlation but are not related to the task at hand. This is a direct application of the CompAttribute metric introduced in \cite{balasubramanian2024decomposing}; iii) \textbf{Supervised (S):} When we assume the availability not only of the task encodings, but also of labeled samples, we can directly estimate the head score by looking at its performance on the downstream task (in the style of logit lens \citep{nostalgebraist2020interpreting}). 

For each scoring function, we evaluate each head individually, rank them according to their scores, and apply a greedy top-k selection. We then sum these selected heads $\sH'\subseteq \sH$ to create a partially recovered residual, which is subsequently evaluated on downstream task performance.
\begin{equation}
    \mY' = \sum_{i=1}^{|\sH'|}\mH_i 
\end{equation}

We have 3 control measures in place: i) \textbf{Heads (H):} The performance of the model when \textit{all} the attention heads, and only them, are used. This gives information about the heads' contribution to the residual and, symmetrically, how much the final performance depend on the MLP units; ii) \textbf{Random (R):} The average performance over 10 independent random samplings of $k$ \textit{head units}. This can be seen as a lower bound on the expected performance; iii) \textbf{Base (B):} The original performance of the model without any modification to its residual. Intuitively, this could represent a theoretical upper bound on the performance if there are no task-aligned units.

The greedy selection strategy scores heads independently, disregarding inter-head relationships. To make the selection aware of them, we optimize a scalar weight for each head simultaneously using gradient descent, providing an empirical upper bound for the selection performance. We refer to this procedure as \textbf{Optimized~(O)}.

We evaluate these unit selection strategies on two CLIP-like models (BLIP-L and OpenCLIP-L) and 10 of the datasets of \Cref{ssec:cross_dataset}. We choose $k$ for the greedy and random selection so that 5\% of the total heads are considered. Additional results are presented in the Appendix in \Cref{sup:tab:ablation_clipl}, \Cref{sup:tab:ablation_clipb} and \Cref{sup:tab:ablation_openclipb}. 

\input{tables/exp3}

\paragraph{Result analysis}
As reported in \Cref{tab:ablation}, the unsupervised scoring (\textbf{U}) performs unexpectedly well, despite not explicitly considering the task. This effectiveness likely stems from the fact that core task information is often embedded in the first few principal components (PCs) of the output. Underlying this method is the assumption that first principal components already align with the main task-relevant information. By aligning with this information using only a few heads, we achieve a dual benefit: preserving essential task-relevant information while effectively filtering out noise. In fact, adding the conditioning on the task (\textbf{U|T}) is just slightly beneficial in terms of performance, with the exception of SVHN. The vast majority (on average around 90\%) of alignment between task and residual comes from the head contributions, as witnessed by the similarity between the columns \textbf{H} and \textbf{B}. The optimized selection strategy (\textbf{O}) is extremely powerful, having the best score among them, and sometimes almost doubling the original model performances (\textbf{B}) showing how task-relevant information is already present in the residual, just hidden.

These results illustrate that retaining only task-aligned units (effectively ``panning for the gold'' contained in the residual) is highly effective across the board.

\subsection{Spectral ResiDual Alignment}
Given that: i) retaining only task-aligned units is beneficial for image-text alignment (\Cref{ssec:coarse_selection}); ii) unit specialization is essentially encoded in their principal components (\Cref{ssec:omp}); iii) components of general enough data distributions (e.g., ImageNet) capture specialized behaviour even on other datasets (\Cref{ssec:cross_dataset}), we propose a method to directly filter information along the residual at the spectral level: \textbf{ResiDual}.

In short, we start from the decomposition formula for the residual stream (\Cref{eq:decomposition}), and allow anisotropic scaling of each unit representation. Specifically, given a unit representation $\mX$, its corresponding principal component basis $\bm{\Phi}$, and associated mean $\bm{\mu}$, we define the ResiDual transformation of $\mX$ as:

\begin{equation}\label{eq:residual}
    \text{RD}_{\bm{\Phi}, \bm{\mu}}(\mX, \bm{\lambda}) = \mathbf{\Phi}^T\text{diag}(\bm{\lambda})\mathbf{\Phi}(\mX-\bm{\mu})^T
\end{equation}

where the learnable vector $\bm{\lambda}$ contains the weights associated with each principal component. Then, the transformation is applied to every residual unit independently, resulting in a transformed version of the output $\mY$:

\begin{equation}
    \mY' = \sum_{i=1}^{|\sU|} \text{RD}_{\bm{\Phi_i}, \bm{\mu_i}}(\mU_i, \bm{\lambda}_i)
\end{equation}
In summary, ResiDual models a simple spectral anisotropic scaling of residual units, that results in more complex dynamics in the output space.
In this section, we extensively evaluate the effectiveness of this method across a variety of configurations, models and datasets.
\paragraph{Experimental setting} We evaluate ResiDual in 3 different configurations: i) \textbf{RD}, as presented in the original formulation; ii) \textbf{RD}$^*$, a version of ResiDual constrained on the number and type of units (we restrict it to heads) and number of considered components (we truncate PCA bases to explain 90\% of the variance). This is done because head units, and specifically their principal components, are strongly connected to specialization; iii) \textbf{RD$^\mY$}, where the ResiDual transformation is applied directly on the output encoding $\mY$, to assess whether output components can already be aligned with the task.
For all configurations, we select ImageNet as a reference for the PCA bases $\bm{\Phi}$ and unit means $\bm{\mu}$ that appear in the ResiDual \Cref{eq:residual}.

We compare ResiDual with 3 reference alignment methods: i) \textbf{Base}: the base zero-shot performance of the model, relying on the alignment coming from pre-training; ii) \textbf{Full Finetuning}: we consider its score the empirical upper bound for alignment. It is obtained by finetuning the whole vision transformer with frozen text encodings; iii) \textbf{Linear Aligner (Lin)}: the performance of a trained linear transformation of the output shows to what extent output alignment could be linearly recovered. This approach is well-supported by recent studies, which indicate that even independently trained models with different architectures can often be aligned by a simple linear transformation \citep{moschella2023relative, maiorca2024latent, asif, pmlr-v243-lahner24a, balasubramanian2024decomposing}.

For this experiment, we work with 3 CLIP-like models, BLIP-L, CLIP-L and OpenCLIP-L (CLIP/OpenCLIP-B results can be found in the Appendix in \Cref{sup:tab:residual} and \Cref{sup:fig:residual}), and tune them on the 10 datasets employed in \Cref{ssec:coarse_selection}. All training runs use the Schedule-Free Adam optimizer \citep{defazio2024road} with the automatic learning rate finder by \citet{smith2017cyclicallearningratestraining}, implemented in PyTorch Lightning \citep{Falcon_PyTorch_Lightning_2019}. The maximum number of epochs is 30, with an early-stopping policy on the validation set accuracy with a patience of 5 epochs.

\begin{figure}
\centering
    \includegraphics[width=\textwidth]{figures/all_diamond.pdf}
\caption{Performance comparison between text-image alignment methods on zero-shot classification tasks.
}
\label{fig:residual}
\end{figure}

\begin{table}
\centering
\begin{tabular}{lcccccccccccc}
\toprule
 &  \multicolumn{4}{c}{\textbf{BLIP-L}}  &  \multicolumn{4}{c}{\textbf{CLIP-L}}  & \multicolumn{4}{c}{\textbf{OpenCLIP-L}} \\
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
\textbf{Dataset} & \textbf{Lin} & \textbf{RD} & \textbf{RD$^*$} & \textbf{RD$^\mY$}& \textbf{Lin} & \textbf{RD} & \textbf{RD$^*$} & \textbf{RD$^\mY$} & \textbf{Lin} & \textbf{RD} & \textbf{RD$^*$} & \textbf{RD$^\mY$} \\
\midrule
 CIFAR10 &   0.97 &   0.97 &   0.97 &   0.96 &   0.97 &   0.98 &   0.98 &   0.97 &       0.98 &       0.98 &       0.98 &       0.98 \\
CIFAR100 &   0.82 &   0.83 &   0.81 &   0.74 &   0.85 &   0.86 &   0.85 &   0.80 &       0.88 &       0.88 &       0.87 &       0.85 \\
     DTD &   0.79 &   0.77 &   0.76 &   0.58 &   0.81 &   0.80 &   0.79 &   0.63 &       0.84 &       0.84 &       0.83 &       0.69 \\
 EuroSAT &   0.95 &   0.98 &   0.98 &   0.83 &   0.97 &   0.99 &   0.98 &   0.95 &       0.97 &       0.98 &       0.98 &       0.94 \\
   GTSRB &   0.87 &   0.87 &   0.84 &   0.59 &   0.92 &   0.92 &   0.92 &   0.77 &       0.94 &       0.92 &       0.92 &       0.78 \\
   MNIST &   0.98 &   0.99 &   0.99 &   0.93 &   0.99 &   0.99 &   0.99 &   0.97 &       0.99 &       0.99 &       0.99 &       0.97 \\
RESISC45 &   0.92 &   0.93 &   0.92 &   0.77 &   0.95 &   0.96 &   0.95 &   0.87 &       0.95 &       0.95 &       0.95 &       0.89 \\
    Cars &   0.86 &   0.84 &   0.82 &   0.75 &   0.89 &   0.86 &   0.85 &   0.81 &       0.94 &       0.94 &       0.94 &       0.93 \\
  SUN397 &   0.80 &   0.80 &   0.77 &   0.72 &   0.82 &   0.80 &   0.77 &   0.70 &       0.83 &       0.82 &       0.80 &       0.75 \\
    SVHN &   0.65 &   0.81 &   0.77 &   0.53 &   0.77 &   0.86 &   0.86 &   0.70 &       0.75 &       0.86 &       0.85 &       0.64 \\
\midrule
\textbf{Average} & 0.86 & \textbf{0.88} & 0.86 & 0.74 & 0.89 & \textbf{0.90} & \textbf{0.90} & 0.82 & 0.91 & \textbf{0.92} & 0.91 & 0.84 \\
\midrule 
\textbf{\#params} & 65.8k &  30.7k & 8.3k & 256 & 590k &  43k & 14k &768 & 590k & 43k & 13.2k & 768 \\
\bottomrule
\end{tabular}
\caption{Accuracy produced by different configurations of ResiDual, compared with a linear aligner. (Lin): linear aligner at the output level; (RD): ResiDual in the original formulation (all principal components of all residual units); (RD$^*$): ResiDual limited to head units alone (no MLPs), and PCs truncated to 90\% of explained variance; (RD$^\mY$): ResiDual applied to the output encoding.}
\label{tab:residual}
\end{table}

\paragraph{Result analysis} A comparative analysis of ResiDual against reference alignment methods is reported in \Cref{fig:residual}. We observe that a linear transformation of the output is sufficient to approximate full finetuning performance. The spectral residual transformation modeled by ResiDual attains comparable (if not better) results than the linear aligner on all datasets. These statements hold true across all models. Specifically, the case of SVHN stands out: on this input dataset, ResiDual has an advantage of approximately 10\% on all models. We hypothesize that this gap is due to the absence of task-relevant features at the output level. This is confirmed by the results in \Cref{tab:residual}: on SVHN, \textbf{RD$^\mY$} has the largest gap from \textbf{RD}, meaning that output components are not well aligned with the task. While having approximately 30\% of the learnable parameters, \textbf{RD$^*$} achieves comparable results to \textbf{RD}, indicating that applying the ResiDual procedure to heads (and not MLPs) and their first principal components alone is enough. Moreover, we note that in the cases where the performance of the original model is already satisfactory (e.g., CIFAR-10), applying ResiDual does not compromise alignment.

Overall, these results show that, by leveraging spectral-level operations along the residual, ResiDual builds a concise yet expressive transformation that bridges the modality gap, closely approximating full finetuning-level performance, even in its more parameter-efficient configuration.



\section{Conclusions}

\looseness=-1 In this work, we analyzed the emergent specialization property of attention heads in vision transformers and unveiled its connection with the spectral geometry of residual representations. Specifically, we focused on the relationship between head specialization and downstream task performance. Then, we leveraged this to introduce ResiDual, a method that we employed to improve alignment in multimodal transformers by applying spectral anisotropic scaling along the residual stream. ResiDual proved effective in emphasizing task-relevant principal components and dampening down the others, akin to panning for gold in the residual stream.
\paragraph{Limitations}
ResiDual fundamentally works by extracting information from residual units already containing task-relevant principal components. We expect that when this assumption does not hold, ResiDual cannot recover the alignment, resulting in a significant drop in downstream performance.
Moreover, our downstream tasks focused on zero-shot classification in CLIP-like models. This implies considering only the alignment between \texttt{[CLS]} tokens, ignoring sequence-level information.  

\paragraph{Future work}
The ResiDual formulation is based solely on the residual decomposition technique, which opens to its application across virtually any transformer architecture. Our findings show that ResiDual can be limited to operating on a subset of residual units (i.e., attention heads). An additional constraint on selecting only contributions from the first few model layers creates opportunities to enhance model inference.
% 

\paragraph{Acknowledgements}
The authors gratefully acknowledge Volkan Cevher for an insightful discussion about sparse recovery algorithms, Alex Smola for valuable feedback on the experiments, and Marco Baroni for an engaging conversation on the phenomenon of head specialization in NLP.

% \newpage
\bibliography{main}
\bibliographystyle{tmlr}

\newpage
\appendix
\section{Appendix}
\subsection{Residual decomposition}\label{ssec:decomposition}

\begin{figure}[h]
\centering
    \includegraphics[width=\textwidth]{figures/transformer.pdf}
\caption{Overview of a Transformer block. Multi-Head Attention and MLP are both surrounded by residual connections and LayerNorm is applied before each sub-layer.
}
\label{fig:transformer}
\end{figure}
Transformers are residual networks, made of stacked blocks that contain a Multi-Head Attention (MHA) layer and a MLP, both surrounded by a skip (residual connection). All vision transformers we employ in this work abide by the 'pre-norm' \cite{wang2019learning} architecture, which slightly modified the original \citep{vaswani2017attention} by moving LayerNorm before MHA and MLP sub-layers (\autoref{fig:transformer}). This modification implies that, at each layer, MHA and MLP sub-layers directly write their output representation to the residual stream. Hence, the final latent representation of the model is given by:
$$
\mY = \mX_0 + \sum_{i=1}^L \mA_i + \sum_{i=1}^L \mM_i
$$
where $\mX_0$ is the initial data embedding, $\mA_i$ is the attention output of layer $i$ and $\mM_i$ is the MLP output at layer $i$. All these encodings share the same dimensionality $d$ with the model output.

Attention representations can be further decomposed into head contributions, as they are the result of linear operations applied to the head-level representations. Specifically, each attention head $h$ of $N_h$ at layer $i$ produces a representation $\mH_{i,h}=\text{Softmax} \left(\frac{\mQ_{i,h}\mK_{i,h}^T}{\sqrt{d_k}} \right) \mV_{i,h}$, whose dimension is $\frac{d}{N_h}$. Contributions for all heads are then concatenated and projected linearly to obtain the MHA output:
$$
\mA_i = (\text{cat}(\mH_{i,1},...,\mH_{i,N_h}))\mW_i + \vb_i
$$
Assuming that the bias term $\vb$ can be split equally between heads, this operation can be equivalently expressed in a distributed form:
$$
\mA_i = \sum_{h=1}^{N_h}\hat{\mH}_{i,h}=\sum_{h=1}^{N_h}(\mH^0_{i,h}\mW_i + \frac{\vb_i}{N_h})
$$
These terms $\hat{\mH}_{i,j}$ are the \textit{head representations} we consider in this paper. They have the same dimensionality $d$ as the residual stream (and model output), and they are simply obtained by linearly projecting the 'raw' head contributions, properly padded with 0s to match the dimensionality of the residual stream.
Hence, we arrive at this final decomposition:
$$
\mY = \mX_0 + \sum_{i=1}^L\sum_{h=1}^{N_h}\hat{\mH}_{i,h} + \sum_{i=1}^L \mM_i
$$
In many transformer models, such as the ones employed in this work, the final residual encoding $\mY$ is not the final output of the model. For instance, in CLIP, $\mY$ is passed through a LayerNorm and then through a biasless linear projection $\mP$, that maps ViT encodings to the shared vision-language space:
$$
\hat{\mY} = \mP(\text{LayerNorm}(\mY))
$$

However, this operation can be again distributed over the summands that produce $\mY$ because of the linearity of the final projection and because LayerNorm can be rewritten as an affine transformation, as in \citet{gandelsman2024interpreting}. The representations we employ in this paper are mapped to the output space by applying (if present) the final projection and LayerNorm to each unit entry ($\hat{\mH}_{i,h}$ or $\mM_i$).

\subsection{Connecting TextSpan and Matching Pursuit}\label{ssec:proof}
\RestyleAlgo{ruled}
\begin{algorithm}
%\SetKwInOut{Input}{Input}
\SetKwInput{Input}{Input~}
\SetKwInOut{Output}{Output}
\caption{TextSpan \citep{gandelsman2024interpreting}}
\Input{Signal Matrix $\mX \in \mathbb{R}^{n, d}$, dictionary $\mD \in \mathbb{R}^{k, d}$, number of iterations $N$.}
\Output{Reconstruction $\mX_r^{N}$, support set $\sC^{N}$}
\textbf{Initialization:} Residual $\mR^0 = \mX$, reconstruction $\mX_r^0=\mathbf{0}$, dictionary $\mD^0=\mD$, support set $\sC^0 = \emptyset$ \;
\For{$t \in \{0,...,N-1\}$}{
$\mP \gets \mD^t {\mR^t}^T$\;
$p^t \gets \arg\max_{j=1}^k \text{Var}(\mP[j])$\;
$\sC^{t+1}\gets \sC^{t}\cup \{p^t\}$\;
$\mR^{t+1} \gets \mR^{t} - \text{proj}(\mR^{t}, \mD^t[p^t])$\;
$\mX_r^{t+1} \gets \mX_r^{t} + \text{proj}(\mR^{t}, \mD^t[p^t])$\;
$\mD^{t+1} \gets \mD^{t} - \text{proj}(\mD^{t}, \mD^t[p^t])$\;
}

\end{algorithm}

\RestyleAlgo{ruled}
\begin{algorithm}
%\SetKwInOut{Input}{Input}
\SetKwInput{Input}{Input~}
\SetKwInOut{Output}{Output}
\caption{Simultaneous Orthogonal Matching Pursuit (SOMP) \citep{tropp2006algorithms}}
\Input{Signal Matrix $\mX \in \mathbb{R}^{n, d}$, dictionary $\mD \in \mathbb{R}^{k, d}$, number of iterations $N$.}
\Output{Reconstruction $\mX_r^{N}$, support set $\sC^{N}$}
\textbf{Initialization:} Residual $\mR^0 = \mX$, reconstruction $\mX_r^0=\mathbf{0}$, support set $\sC^0 = \emptyset$\;
\For{$t \in \{0,...,N-1\}$}{
$\mP \gets \mD {\mR^t}^T$\;
$p^t \gets \arg\max_{j=1}^k (||\mP[j]||_1)$\;
$\sC^{t+1}\gets \sC^{t}\cup \{p^t\}$\;
$\mW^t \gets \arg\min_\mW ||\mX-\mW{\mD[\sC^t]}||_F$\;
$\mX_r^{t+1} \gets \mW^t{\mD[\sC^t]}$\;
$\mR^{t+1} \gets \mX - \mX_r^{t+1}$\;
}
\end{algorithm}

The TextSpan algorithm was introduced in \citet{gandelsman2024interpreting} to find a decomposition of CLIP heads on a set of textual descriptions. Here, we show that TextSpan is equivalent to Simultaneous Orthogonal Matching Pursuit (SOMP) \citep{tropp2006algorithms}, with a few light modifications.

The first modification is that, in TextSpan, before computing the decomposition, the dictionary is filtered through a projection on the first principal components of the signal. Output-level text encodings have high semantic granularity: this operation results in a dictionary restricted to the head span. In the following, we will consider this a dictionary preprocessing step and assume that SOMP and TextSpan are provided with the same dictionary, filtered or not.

The second modification is that in TextSpan, the row variance of $\mD \mR^T$ is used instead of the $\ell_1$ norm as a criterion for atom selection. Here, we show that if this criterion is applied in SOMP (or, vice versa, the $\ell_1$ in TextSpan), the two algorithms are equivalent.

We are given a signal matrix $\mX\in \R^{n,d}$ and two (initially identical) dictionaries $\mD_{MP}=\mD^0_{TS} \in \R^{k,d}$.

We will proceed by induction. At $t=0$, the two methods pick the same atom $p_0$ (which enters the support set $\sC^1$), and identically update the residual:

$$
\sC^1_{MP}=\{p_0\}, \quad \mR^1_{MP}=\mX-\text{proj}(\mX, \mD_{MP}[p_0])
$$

$$
\sC^1_{TS}=\{p_0\}, \quad \mR^1_{TS}=\mX-\text{proj}(\mX, \mD^0_{TS}[p_0])=\mR^1_{MP}, \quad \mD^1_{TS}\perp\mD^0_{TS}[p_0]
$$

Now, suppose we are at step $t=n$ with $\sC^n_{MP}=\sC^n_{TS}$ and $\mR^n_{TS}=\mR^n_{MP}$.

The two dictionaries will be different: $\mD_{MP}$ never gets updated, while $\mD^n_{TS}\perp\mD^0_{TS}[\sC^n_{TS}]$ because TextSpan applies a Gram–Schmidt process that finds an orthogonal basis for the subspace of selected atoms. 
Since in TextSpan the dictionary is orthogonalized at each step, at time $n$ it is orthogonal to \textit{all} previously chosen atoms (which are also orthogonal to each other).
The dictionary of SOMP can be decomposed in two terms, one contained in the span of atoms chosen until this point by TextSpan and one orthogonal, which corresponds to the current dictionary of TextSpan:
$$
\mD_{MP}=\mD_{MP, \parallel} + \mD_{MP, \perp}=\mD_{MP, \parallel} + \mD^n_{TS}$$
The residual of TextSpan (which, by inductive hypothesis is identical to the residual of SOMP) is by definition orthogonal to the atoms already chosen by TextSpan. Hence the selection step of SOMP will compute:
$$
\mD_{MP} {\mR^n_{MP}}^T = (\mD_{MP, \parallel} + \mD_{MP, \perp}){\mR^n_{MP}}^T = \mD_{MP, \perp} {\mR^n_{TS}}^T = \mD^n_{TS} {\mR^n_{TS}}^T
$$
Then, at step $n+1$ the two algorithms will pick the same atom index again, and update identically the residual, removing its projection on the chosen atom, i.e. $\sC^{n+1}_{MP}=\sC^{n+1}_{TS}$. The last thing to prove is that the residual is also the same. For SOMP, the least squares solution of the optimization problem results in:
\begin{align*}
\mR_{MP}^{n+1} = \mX- \mX \mD^T_{MP}[\sC^{n+1}_{MP}](\mD_{MP}[\sC^{n+1}_{MP}]\mD^T_{MP}[\sC^{n+1}_{MP}])^{-1}\mD_{MP}[\sC^{n+1}_{MP}] = \mX - \text{proj}(\mX, \mD_{MP}[\sC^{n+1}_{MP}])
\end{align*}

%Note that the residual at time $n$ is orthogonal to the subspace of all previously chosen atoms by inductive hypothesis (since it is the same as orthogonal matching pursuit and they chose the same atom index -- although the actual atom vectors are different due to the projections in TextSpan) and the newly chosen atom in TextSpan is orthogonal to the past ones by construction. Therefore, the orthogonal projection on this atom is equivalent to the orthogonal projection on the subspace spanned by \textit{all} previous atoms (because they orthogonal to the current residual). 
While for TextSpan we get:
\begin{align*}    
\mR_{TS}^{n+1} = \mR_{TS}^{n} - \text{proj}(\mR_{TS}^{n}, \mD_{TS}^{n}[p_{n+1}]) = \mR_{TS}^{n} - \text{proj}(\mR_{TS}^{n}, \mD_{TS}^{n}[\sC_{TS}^{n+1}]) = \mR_{TS}^{n} - \text{proj}(\mR_{TS}^{n}, \mD_{TS}^{0}[\sC_{TS}^{n+1}])    
\end{align*}
Where the first equality is the definition of the residual; the second is because $\mD_{TS}^{n}[p_{n+1}]\perp \mD_{TS}^{n}[\sC_{TS}^{n}]$; the last is because the residual and the last chosen atom $\mD^n_{TS}[p_{n+1}]$ are orthogonal to $\mD^0_{TS}[\sC_{TS}^{n}]$.
% span of the dictionary subspace from time $n$ and $0$ is the same. 
Now, we can use the inductive hypothesis $\mR_{TS}^{n} = \mR_{MP}^{n}$, that $\sC_{TS}^{n+1} =  \sC_{MP}^{n+1}$ and $\mD_{TS}^{0} = \mD_{MP}$, so:
% $$R_{TS}^{n+1} =  R_{MP}^{n} - \text{proj}(R_{MP}^{n}, D_{TS}^{0}[\sC_{TS}^{n+1}]).$$
% The remaining step is now to invoke that the $\text{span}(\sC_{TS}^{n+1})=\text{span}(\sC_{MP}^{n+1})$, which is true since $\sC_{TS}^{n+1} = \sC_{MP}^{n+1}$. Therefore:
\begin{align*}
\mR_{TS}^{n+1} &= \mR_{MP}^{n} - \text{proj}(\mR_{MP}^{n}, \mD_{MP}[\sC_{MP}^{n+1}]) \\
&\equaltext{MP residual} \quad  \mX - \text{proj}(\mX, \mD_{MP}[\sC_{MP}^{n}]) - \text{proj}(\mX - \text{proj}(\mX, \mD_{MP}[\sC_{MP}^{n}]), \mD_{MP}[\sC_{MP}^{n+1}]) \\
&\equaltext{proj is linear} \quad  \mX - \text{proj}(\mX, \mD_{MP}[\sC_{MP}^{n}]) - \text{proj}(\mX, \mD_{MP}[\sC_{MP}^{n+1}]) +  \text{proj}(\text{proj}(\mX, \mD_{MP}[\sC_{MP}^{n}]), \mD_{MP}[\sC_{MP}^{n+1}]) \qquad \\
 &\equaltext{MP residual} \quad  \mR_{MP}^{n+1} - \text{proj}(\mX, \mD_{MP}[\sC_{MP}^{n}]) + \text{proj}(\text{proj}(\mX, \mD_{MP}[\sC_{MP}^{n}]), \mD_{MP}[\sC_{MP}^{n+1}]) \\
    &= \mR_{MP}^{n+1} - \text{proj}(\mX, \mD_{MP}[\sC_{MP}^{n}]) + \text{proj}(\mX, \mD_{MP}[\sC_{MP}^{n}])]) \\ &=  \mR_{MP}^{n+1}
\end{align*}
where the second to last equality comes from the fact that $\text{proj}(\mX, \mD_{MP}[\sC_{MP}^{n}])$ is already in a subspace of $\mD_{MP}[\sC_{MP}^{n+1}]$, so the outer projection does not change the result of the inner one.



\newpage
\subsection{Additional results}

\begin{figure}[h]
\centering
    \includegraphics[width=0.7\textwidth]{figures/id_columns_99_base.pdf}
\caption{Heads in early layers show low-dimensional, linear structures, as suggested by similar intrinsic dimension estimates from PCA (\textbf{L}) and TwoNN (\textbf{N}). Moving toward the output layer, the true dimensionality peaks and then decreases, while PCA’s linear estimate continues to rise, indicating increasing nonlinearity in head manifolds (\textbf{Ratio} = $\frac{L}{N}$). The first principal component (\textbf{EVR$_1$}) explains around 50\% of the variance in early layers, dropping to around 10\% in later layers.
}
\label{fig:id_heads_base}
\end{figure}

\begin{figure}[h]
    \centering
    % Left subfigure (image)
    \begin{minipage}{0.18\textwidth}
        %\centering
        \includegraphics[width=\textwidth]{figures/omp_textspan_2.pdf} % Replace with your image path
    \end{minipage}%
    % Right subfigure (table)
    \begin{minipage}{0.7\textwidth}
    \resizebox{0.9\linewidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{TextSpan} & \textbf{OMP$_2$} \\
\midrule
\textbf{L20.H8} (``Scenery'') & \textbf{L20.H8} ($Z=0.35$) \\
\midrule
Photo taken in Galápagos Islands & Picture taken in the Swiss chocolate factories \\
Image taken in Norway & Image with Mayan-inspired designs \\
Evocative beauty & Stark and minimalist urban scene \\
Vibrant urban energy & serene oceanside scene \\
A skirt & Vivid cultural ceremony \\
\midrule
\textbf{L21.H11} (``Location'') & \textbf{L21.H11} ($Z=1.47$) \\
\midrule
Picture taken in Cyprus & Picture taken in Hungary \\
Picture taken in Ontario, Canada & Photo taken in the Californian vineyards \\
Photo taken in Rio de Janeiro, Brazil & serene woodland refuge \\
Photo captured in the Arizona desert & Photo taken in the Australian rainforest \\
Picture captured in the Scottish highlands & Photo taken in Canadian Rockies \\
\midrule
\textbf{L22.H8} (``Letters'') & \textbf{L22.H8} ($Z=1.84$) \\
\midrule
A photo with the letter F & A photo with the letter G \\
A photo with the letter V & A photo with the letter J \\
A photo with the letter D & Photo taken in Monument Valley \\
A photo with the letter T & Enchanting fantasy world \\
A photo with the letter X & A labyrinth \\
\midrule
\textbf{L23.H2} (``Animals'') & \textbf{L23.H2} ($Z=1.11$) \\
\midrule
Image showing prairie grouse & A capacitor \\
Image with a penguin & A spiky texture \\
A magnolia & A wolf \\
An image with dogs & Image with an ant \\
An image with cats & A spirograph-like shape \\
\bottomrule
\end{tabular}
}
\vspace{0.9cm}
    \end{minipage}
    \caption{Comparison between TextSpan and Orthogonal Matching Pursuit on the second principal component (OMP$_2$), applied to the heads of OpenCLIP-L. \textit{Left}: agreement score between the descriptions returned by the two methods. \textit{Right}: qualitative comparison of selected descriptions for 4 heads, one per layer, at different agreement levels.}
    \label{sup:fig:ts_omp}
\end{figure}

\newpage
\subsubsection{Dataset Comparison}

\begin{figure}[h]
\centering
    \includegraphics[width=0.5\textwidth]{figures/pearson_models.pdf}
\caption{Pearson correlation between cross-dataset similarities on different models. Comparison is done between ImageNet and each of the other datasets and averaged over heads.
}
\label{sup:fig:pearson_models}
\end{figure}

\label{sup:sec:head_comparison}
\begin{figure}[h]
\centering
    \includegraphics[width=0.9\textwidth]{figures/blip_l_flickr_allheads.pdf}
    \caption{Attention head similarity across layers of BLIP-L, computed between ImageNet head representations and those obtained on other datasets.}\label{sup:fig:head_comparison_blipl}
\end{figure}
\newpage
\begin{figure}[h]
\centering
    \includegraphics[width=0.9\textwidth]{figures/openai_l_allheads.pdf}
    \caption{Attention head similarity across layers of CLIP-L, computed between ImageNet head representations and those obtained on other datasets.}\label{sup:fig:head_comparison_clipl}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=0.6\textwidth]{figures/openai_b_allheads.pdf}
    \caption{Attention head similarity across layers of CLIP-B, computed between ImageNet head representations and those obtained on other datasets.}\label{sup:fig:head_comparison_clipb}
\end{figure}
\newpage
\begin{figure}[h]
\centering
    \includegraphics[width=0.6\textwidth]{figures/openclip_b_allheads.pdf}
    \caption{Attention head similarity across layers of OpenCLIP-B, computed between ImageNet head representations and those obtained on other datasets.}\label{sup:fig:head_comparison_openclipb}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=\textwidth]{figures/dinov2_l_allheads.pdf}
    \caption{Attention head similarity across layers of DINOv2-L, computed between ImageNet head representations and those obtained on other datasets.}\label{sup:fig:head_comparison_dino}
\end{figure}

\begin{figure}[h]
\centering
    \includegraphics[width=\textwidth]{figures/vit_l_allheads.pdf}
    \caption{Attention head similarity across layers of ViT-L, computed between ImageNet head representations and those obtained on other datasets.}\label{sup:fig:head_comparison_vitl}
\end{figure}

\newpage
\subsubsection{Coarse Unit Alignment}

\begin{table}[h]
\small
\centering
\begin{tabular}{lccccccc}
\toprule
 & \multicolumn{7}{c}{\textbf{CLIP-L}} \\
\textbf{Dataset} & U & U|T & S & R & H & B & O \\
\cmidrule(lr){2-5}\cmidrule(lr){6-8}
CIFAR10 & 0.88 & 0.95 & 0.95 & 0.51 & 0.92 & 0.96 & 0.97 \\
CIFAR100 & 0.72 & 0.73 & 0.72 & 0.32 & 0.68 & 0.75 & 0.80 \\
Cars & 0.74 & 0.74 & 0.73 & 0.29 & 0.70 & 0.78 & 0.79 \\
DTD & 0.50 & 0.50 & 0.51 & 0.26 & 0.51 & 0.55 & 0.61 \\
EuroSAT & 0.71 & 0.71 & 0.73 & 0.36 & 0.53 & 0.62 & 0.95 \\
GTSRB & 0.49 & 0.48 & 0.50 & 0.18 & 0.31 & 0.50 & 0.71 \\
MNIST & 0.78 & 0.77 & 0.85 & 0.36 & 0.75 & 0.76 & 0.96 \\
RESISC45 & 0.63 & 0.63 & 0.64 & 0.30 & 0.59 & 0.71 & 0.84 \\
SUN397 & 0.56 & 0.56 & 0.58 & 0.23 & 0.49 & 0.67 & 0.71 \\
SVHN & 0.65 & 0.65 & 0.65 & 0.30 & 0.60 & 0.58 & 0.71 \\
\midrule
Average & 0.66 & 0.67 & \textbf{0.69} & 0.31 & 0.61 & 0.69 & 0.80 \\
\bottomrule
\end{tabular}

\caption{Accuracy when doing \textbf{zero ablation of all units except top 5\%} of attention heads. Heads are assigned a binary weight using an Unsupervised (U), Task-conditioned Unsupervised (U|T), Supervised (S), and Random (R) strategy (a mean over 10 different seeds). H corresponds to using all the attention heads available, B is the original model performance, and O is the optimized continuous weighting case.}\label{sup:tab:ablation_clipl}
\end{table}
\newpage
\begin{table}[h]
\small
\centering
\begin{tabular}{lccccccc}
\toprule
 & \multicolumn{7}{c}{\textbf{CLIP-B}} \\
\textbf{Dataset} & U & U|T & S & R & H & B & O \\
\cmidrule(lr){2-5}\cmidrule(lr){6-8}
CIFAR10 & 0.87 & 0.90 & 0.89 & 0.41 & 0.81 & 0.91 & 0.93 \\
CIFAR100 & 0.60 & 0.60 & 0.60 & 0.20 & 0.57 & 0.66 & 0.69 \\
Cars & 0.56 & 0.57 & 0.58 & 0.15 & 0.51 & 0.65 & 0.63 \\
DTD & 0.42 & 0.41 & 0.41 & 0.16 & 0.41 & 0.45 & 0.47 \\
EuroSAT & 0.54 & 0.54 & 0.58 & 0.23 & 0.53 & 0.55 & 0.88 \\
GTSRB & 0.33 & 0.34 & 0.40 & 0.14 & 0.25 & 0.42 & 0.61 \\
MNIST & 0.55 & 0.53 & 0.50 & 0.22 & 0.29 & 0.52 & 0.88 \\
RESISC45 & 0.61 & 0.61 & 0.61 & 0.20 & 0.58 & 0.66 & 0.73 \\
SUN397 & 0.42 & 0.42 & 0.47 & 0.12 & 0.42 & 0.64 & 0.64 \\
SVHN & 0.48 & 0.44 & 0.52 & 0.22 & 0.42 & 0.52 & 0.58 \\
\midrule
Average & 0.54 & 0.54 & \textbf{0.56} & 0.21 & 0.48 & 0.60 & 0.70 \\
\bottomrule
\end{tabular}

\caption{Accuracy when doing \textbf{zero ablation of all units except top 5\%} of attention heads. Heads are assigned a binary weight using an Unsupervised (U), Task-conditioned Unsupervised (U|T), Supervised (S), and Random (R) strategy (a mean over 10 different seeds). H corresponds to using all the attention heads available, B is the original model performance, and O is the optimized continuous weighting case.}\label{sup:tab:ablation_clipb}
\end{table}

\begin{table}[h]
\small
\centering
\begin{tabular}{lccccccc}
\toprule
 & \multicolumn{7}{c}{\textbf{OpenCLIP-B}} \\
\textbf{Dataset} & U & U|T & S & R & H & B & O \\
\cmidrule(lr){2-5}\cmidrule(lr){6-8}
CIFAR10 & 0.94 & 0.94 & 0.94 & 0.49 & 0.92 & 0.95 & 0.96 \\
CIFAR100 & 0.73 & 0.71 & 0.71 & 0.29 & 0.69 & 0.76 & 0.77 \\
Cars & 0.79 & 0.84 & 0.82 & 0.24 & 0.73 & 0.88 & 0.86 \\
DTD & 0.52 & 0.51 & 0.51 & 0.25 & 0.51 & 0.57 & 0.58 \\
EuroSAT & 0.51 & 0.49 & 0.49 & 0.26 & 0.44 & 0.52 & 0.87 \\
GTSRB & 0.49 & 0.47 & 0.48 & 0.12 & 0.42 & 0.50 & 0.66 \\
MNIST & 0.75 & 0.71 & 0.74 & 0.19 & 0.45 & 0.66 & 0.91 \\
RESISC45 & 0.64 & 0.63 & 0.63 & 0.25 & 0.56 & 0.68 & 0.77 \\
SUN397 & 0.55 & 0.55 & 0.56 & 0.20 & 0.46 & 0.70 & 0.69 \\
SVHN & 0.62 & 0.62 & 0.61 & 0.21 & 0.44 & 0.50 & 0.66 \\
\midrule
Average & \textbf{0.65} & \textbf{0.65} & \textbf{0.65} & 0.25 & 0.56 & 0.67 & 0.77 \\
\bottomrule
\end{tabular}

\caption{Accuracy when doing \textbf{zero ablation of all units except top 5\%} of attention heads. Heads are assigned a binary weight using an Unsupervised (U), Task-conditioned Unsupervised (U|T), Supervised (S), and Random (R) strategy (a mean over 10 different seeds). H corresponds to using all the attention heads available, B is the original model performance, and O is the optimized continuous weighting case.}\label{sup:tab:ablation_openclipb}
\end{table}

\newpage

\subsubsection{Spectral ResiDual Alignment}

\begin{figure}[h]
\centering
    \includegraphics[width=\textwidth]{figures/all_diamond_base.pdf}
\caption{Performance comparison between text-image alignment methods on zero-shot classification tasks.
}
\label{sup:fig:residual}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{lcccccccc}
\toprule
 &  \multicolumn{4}{c}{\textbf{CLIP-B}}  &  \multicolumn{4}{c}{\textbf{OpenCLIP-B}} \\
\cmidrule(lr){2-5}\cmidrule(lr){6-9}
\textbf{Dataset} & \textbf{Lin} & \textbf{RD} & \textbf{RD$^*$} & \textbf{RD$^\mY$}& \textbf{Lin} & \textbf{RD} & \textbf{RD$^*$} & \textbf{RD$^\mY$} \\
\midrule
CIFAR10 & 0.95 & 0.96 & 0.96 & 0.94 & 0.97 & 0.97 & 0.97 & 0.96 \\
CIFAR100 & 0.81 & 0.79 & 0.76 & 0.71 & 0.85 & 0.83 & 0.82 & 0.78 \\
DTD & 0.77 & 0.74 & 0.69 & 0.52 & 0.82 & 0.78 & 0.74 & 0.64 \\
EuroSAT & 0.96 & 0.98 & 0.97 & 0.90 & 0.97 & 0.98 & 0.98 & 0.93 \\
GTSRB & 0.90 & 0.86 & 0.83 & 0.67 & 0.91 & 0.88 & 0.85 & 0.73 \\
MNIST & 0.99 & 0.99 & 0.99 & 0.95 & 0.99 & 0.99 & 0.99 & 0.97 \\
RESISC45 & 0.93 & 0.92 & 0.90 & 0.81 & 0.93 & 0.93 & 0.91 & 0.83 \\
Cars & 0.83 & 0.75 & 0.70 & 0.67 & 0.92 & 0.91 & 0.89 & 0.89 \\
SUN397 & 0.79 & 0.76 & 0.71 & 0.67 & 0.81 & 0.77 & 0.74 & 0.70 \\
SVHN & 0.71 & 0.78 & 0.74 & 0.60 & 0.76 & 0.81 & 0.79 & 0.68 \\
\midrule
\textbf{Average} & 0.86 & 0.85 & 0.83 & 0.75 & 0.89 & 0.88 & 0.87 & 0.81 \\
\midrule 
\textbf{\#params} & 262k & 15.4k & 4.9k & 512 & 262k &  15.4k & 4.7k & 512 \\
\bottomrule
\end{tabular}
\caption{Accuracy produced by different configurations of ResiDual, compared with a linear aligner. (Lin): linear aligner at the output level; (RD): ResiDual in the original formulation (all principal components of all residual units); (RD$^*$): ResiDual limited to head units alone (no MLPs), and PCs truncated to 90\% of explained variance; (RD$^\mY$): ResiDual applied to the output encoding.}
\label{sup:tab:residual}
\end{table}

\end{document}


