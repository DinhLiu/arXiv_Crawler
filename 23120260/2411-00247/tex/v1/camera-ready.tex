\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[nonatbib,final]{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
        % colors
\usepackage{amsmath}
\usepackage[capitalise]{cleveref}
\usepackage{mathtools}
\usepackage{color,soul}
\usepackage{thm-restate}
\usepackage{amsthm}
\usepackage{xfrac}
\usepackage{bm}
\usepackage{relsize}

\usepackage{wrapfig}
\usepackage{subfigure}



\usepackage[dvipsnames, x11names]{xcolor}

\definecolor{sharedblue}{rgb}{0.05021145713187236, 0.341760861207228, 0.6306036139946175}
\definecolor{sharedgreen}{rgb}{0.0, 0.3625067281814687, 0.14562091503267974}
\definecolor{sharedred}{rgb}{0.6756939638600539, 0.06560553633217993, 0.08838139177239522}
\definecolor{sharedgray}{rgb}{0.25098039215686274, 0.25098039215686274, 0.25098039215686274}
\definecolor{sharedorange}{rgb}{0.8871510957324106, 0.3320876585928489, 0.03104959630911188}

% colors from https://latex-tutorial.com/color-latex/
% \def\kernelcol{Red4}
% \def\wacol{RoyalBlue4}
\def\kernelcol{black}
\def\wacol{black}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows.meta}


\crefname{section}{Sec.}{Sections}


\newcommand{\squish}[1]{{#1\parfillskip=0pt\par}}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\mydots{\hbox to 1em{.\hss.\hss.}}

\newtheorem{conj}{Conjecture}
\newtheorem{defi}{Definition}
\newtheorem{prop}{Proposition}

\title{
% A Telescoping Model of Deep Learning: Understanding by Unraveling Gradient Descent
%: Understanding Deep Learning (Just) Requires Unraveling Gradient Descent
%Understanding Deep Learning Through A Telescope: From Gradient Descent to Gradient Boosting

% Deep Learning Through A Telescoping Lens: \\A Simple Model Helps To Understand Phenomena Relating To Grokking, Gradient Boosting and Beyond
% }
%Deep Learning Phenomena Through A Telescope:\\A Simplified Model To Better Understand\\Grokking, Gradient Boosting, and Beyond
% {Deep Learning Through A Telescoping Lens:} \\ A Simple Model Facilitates Empirical Insights Into Grokking, Gradient Boosting \& Beyond
% }
{Deep Learning Through A Telescoping Lens:} \\ A Simple Model Provides Empirical Insights On Grokking, Gradient Boosting \& Beyond
}
% https://ell.stackexchange.com/questions/117563/insights-on-or-insights-into




% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Alan Jeffares\thanks{\vspace*{-.25cm}Equal contribution} \\
  University of Cambridge\\
    \texttt{aj659@cam.ac.uk} \\
  % examples of more authors
   \And
     Alicia Curth$^*$ \\
  University of Cambridge\\
  \texttt{amc253@cam.ac.uk} \\
  \And
    Mihaela van der Schaar \\
  University of Cambridge\\
  \texttt{mv472@cam.ac.uk} \\
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
Deep learning sometimes \textit{appears to} work in unexpected ways. In pursuit of a deeper understanding of its surprising behaviors, we investigate the utility of a simple yet accurate model of a trained neural network consisting of a sequence of first-order approximations \textit{telescoping} out into a single empirically operational tool for practical analysis. Across three case studies, we illustrate how it can be applied to derive new empirical insights on a diverse range of prominent phenomena in the literature -- including double descent, grokking, linear mode connectivity, and the challenges of applying deep learning on tabular data -- highlighting that this model allows us to construct and extract metrics that help predict and understand the a priori unexpected performance of neural networks. We also demonstrate that this model presents a pedagogical formalism allowing us to isolate components of the training process even in complex contemporary settings, providing a lens to reason about the effects of design choices such as architecture \& optimization strategy, and reveals surprising parallels between neural network learning and gradient boosting.

\end{abstract}


\section{Introduction}
{Deep learning \textit{works}, but it sometimes works in mysterious ways.  Despite the remarkable recent success of deep learning in applications ranging from image recognition \cite{krizhevsky2012imagenet} to text generation \cite{brownlanguage}, there remain many contexts in which it performs in apparently unpredictable ways: neural networks sometimes exhibit surprisingly non-monotonic generalization performance \cite{belkin2019reconciling,power2022grokking}, continue to be outperformed by gradient boosted trees on tabular tasks despite successes elsewhere \cite{grinsztajn2022tree}, and sometimes behave surprisingly similarly to linear models \cite{frankle2020linear}. The pursuit of a deeper understanding of deep learning and its phenomena has since motivated many subfields, and progress on fundamental questions has been distributed across many distinct yet complementary perspectives that range from purely theoretical to predominantly empirical research. }

{\textbf{Outlook.} {In this work, we take a hybrid approach and investigate how we can \textit{apply} ideas primarily used in theoretical research to investigate the behavior of a simple yet accurate model of a neural network \textit{empirically}. Building upon previous work that studies linear approximations to learning in neural networks through tangent kernels (e.g. \cite{jacot2018neural, chizat2019lazy}, see \cref{sec:background}), we consider a model that uses first-order approximations for the functional updates made during training. However, unlike most previous work, we define this model incrementally by simply \textit{telescoping out} approximations to individual updates made during training (\cref{sec:setup-model}) such that it more closely approximates the true behavior of a fully trained neural network in practical settings. This provides us with a pedagogical lens through which we can view modern optimization strategies and other design choices (\cref{sec:otheroptimizers}), and a mechanism with which we can conduct empirical investigations into several prominent deep learning phenomena that showcase how neural networks sometimes generalize \textit{seemingly} unpredictably.}

 Across three case studies in \cref{sec:application}, we then show that this model allows us to \textit{construct and extract metrics that help predict and understand} the a priori unexpected performance of neural networks. First, in \cref{sec:complexity}, we demonstrate that it allows us to extend \cite{curth2023u}'s recent model complexity metric to neural networks, and use this to investigate surprising generalization curves -- discovering that the non-monotonic behaviors observed in both deep double descent \cite{belkin2019reconciling} and grokking \cite{power2022grokking} are associated with \textit{quantifiable} divergence of train- and test-time model complexity. Second, in \cref{sec:gradient-boosting}, we show that it reveals perhaps surprising parallels between gradient boosting \cite{friedman2001greedy} and neural network learning, which we then use to investigate the known performance differences between neural networks and gradient boosted trees on tabular data in the presence of dataset irregularities \cite{mcelfresh2024neural}. Third, in \cref{sec:lmc}, we use it to investigate the connections between gradient stabilization and the success of weight averaging (i.e. linear mode connectivity \cite{frankle2020linear}).}

\vspace{-.1cm}
\section{Background}\label{sec:background}
\textbf{Notation and preliminaries.} Let $f_{\bm{\theta}}: \mathcal{X} \subseteq \mathbb{R}^d \rightarrow \mathcal{Y} \subseteq \mathbb{R}^k$ denote a neural network parameterized by (stacked) model weights $\bm{\bm{\theta}} \in \mathbb{R}^p$.  Assume we observe a training sample of $n$ input-output pairs $\{\mathbf{x}_i, y_i\}^n_{i=1}$, i.i.d. realizations of the tuple $(X, Y)$ sampled from some distribution $P$, and wish to learn good model parameters $\bm{\theta}$ for predicting outputs from this data by minimizing an empirical prediction loss $\frac{1}{n}\sum^n_{i=1}\ell(f_{\bm{\theta}}(\mathbf{x}_i), y_i)$, where $\ell: \mathbb{R}^k \times \mathbb{R}^k \to \mathbb{R}$ denotes some differentiable loss function. Throughout, we let $k=1$ for ease of exposition, but unless otherwise indicated our discussion generally extends to $k>1$. We focus on the case where $\bm{\theta}$ is optimized by initializing the model with some $\bm{\theta}_0$ and then iteratively updating the parameters through stochastic gradient descent (SGD) with learning rates $\gamma_t$ for $T$ steps, where at each $t\in[T]=\{1,\ldots, T\}$ we subsample batches $B_t \subseteq [n] =\{1, \ldots, n\}$ of the training indices, leading to parameter updates $\Delta \bm{\theta}_{t} \coloneq \bm{\theta}_{t} - \bm{\theta}_{t-1}$ as:\vspace{-.05cm}
\begin{equation}\label{eq:sgd_update}
\textstyle  \bm{\theta}_t=\bm{\theta}_{t-1} + \Delta \bm{\theta}_{t} = \bm{\theta}_{t-1} 
  -  \frac{\gamma_t}{|B_t|}\sum_{i \in B_t} \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x}_i) g^{\ell}_{it} = \bm{\theta}_{t-1} - \gamma_t  \mathbf{T}_t  \mathbf{g}^{\ell}_t \vspace{-.05cm}
\end{equation}
where $g^{\ell}_{it}=\frac{\partial \ell(f_{\bm{\theta}_{t-1}}(\mathbf{x}_i), y_i)}{\partial f_{\bm{\theta}_{t-1}}(\mathbf{x}_i)}$ is the gradient of the loss w.r.t. the model prediction for the $i^{th}$ training example, which we will sometimes collect in the vector $\mathbf{g}^{\ell}_t=[g^{\ell}_{1t}, \ldots, g^{\ell}_{nt}]^\top$, and the $p\times n$ matrix $\mathbf{T}_t=[\frac{\mathbf{1}\{1 \in B_t\}}{|B_t|} \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x}_1), \ldots, \frac{\mathbf{1}\{n \in B_t\}}{|B_t|}\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x}_n)]$ has as columns the gradients of the model prediction with respect to its parameters for examples in the training batch (and $\mathbf{0}$ otherwise). Beyond vanilla SGD, modern deep learning practice usually relies on a number of modifications to the update described above, such as momentum and weight decay; we discuss these in \cref{sec:otheroptimizers}.

\textbf{Related work: Linearized neural networks and tangent kernels.} A growing body of recent work has explored the use of \textit{linearized} neural networks (linear in their parameters) as a tool for theoretical \cite{jacot2018neural, chizat2019lazy, lee2019wide} and empirical \cite{fort2020deep, liu2020linearity, ortiz2021can} study. In this paper, we similarly make extensive use of the following observation (as in e.g. \cite{fort2020deep}): we can linearize the difference $\Delta f_t(\mathbf{x}) \coloneq f_{\bm{\theta}_t}(\mathbf{x}) - f_{\bm{\theta}_{t-1}}(\mathbf{x})$ between two parameter updates as \vspace{-.07cm}
\begin{equation}\label{eq:delta-approx}
    \Delta f_t(\mathbf{x}) = \nabla_{\bm{\theta}}  f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top \Delta \bm{\theta}_t + \mathcal{O}(||\Delta \bm{\theta}_t||^2) \approx \textcolor{\wacol}{\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top \Delta \bm{\theta}_t \coloneq  \Delta \tilde{f}_t(\mathbf{x})} \vspace{-.07cm}
\end{equation}
where the quality of the approximation $\Delta \tilde{f}_t(\mathbf{x})$ is good whenever the parameter updates $\Delta \bm{\theta}_t$ from a single batch are sufficiently small (or when the Hessian product $||\Delta \bm{\theta}_t^\top\nabla^2_{\bm{\theta}}f_{\bm{\theta}_{t-1}}(\mathbf{x})\Delta \bm{\theta}_t||$ vanishes). If \cref{eq:delta-approx} holds exactly (e.g. for infinitesimal $\gamma_t$), then running SGD in the network's parameter space to obtain $\Delta \bm{\theta}_t$ corresponds to executing steepest descent on the function output $f_{\bm{\theta}}(\mathbf{x})$ itself using the \textit{neural tangent kernel} $K^{\bm{\theta}}_t(\mathbf{x}, \mathbf{x}_i)$ at time-step $t$ \cite{jacot2018neural}, i.e. results in functional updates\vspace{-.07cm}
\begin{equation}
  \textstyle 
  \textcolor{\kernelcol}{\Delta \tilde{f}_t(\mathbf{x}) \approx - \gamma_t \sum_{i\in [n]} K^{\bm{\theta}}_t(\mathbf{x}, \mathbf{x}_i)g^\ell_{it}} \text{ where } K^{\bm{\theta}}_t(\mathbf{x}, \mathbf{x}_i) \coloneq \frac{\mathbf{1}\{i \in B_t\}}{|B_t|} \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x}_i). \vspace{-.1cm}
\end{equation}

\squish{\textit{Lazy learning} \cite{chizat2019lazy} occurs as the model gradients remain approximately constant during training, i.e. $\nabla_{\bm{\theta}} f_{\bm{\theta}_{t}}(\mathbf{x}) \approx \nabla_{\bm{\theta}} f_{\bm{\theta}_{0}}(\mathbf{x})$, $\forall t\in[T]$. For learned parameters $\bm{\theta}_T$, this implies that the approximation $\textstyle f^{lin}_{\bm{\theta}_T}(\mathbf{x})=f_{\bm{\theta}_0}(\mathbf{x}) + \nabla_{\bm{\theta}} f_{\bm{\theta}_0}(\mathbf{x})^\top(\bm{\theta}_T-\bm{\theta}_0)$ holds -- which is a \textit{linear function of the model parameters}, and thus corresponds to a linear regression in which features are given by the model gradients $\nabla_{\bm{\theta}} f_{\bm{\theta}_0}(\mathbf{x})$ instead of the inputs $\mathbf{x}$ directly --  whose training dynamics can be more easily understood theoretically. For sufficiently wide neural networks the $\nabla_{\bm{\theta}} f_{\bm{\theta}_{t}}(\mathbf{x})$, and thus the tangent kernel, have been theoretically shown to be constant throughout training in some settings \cite{jacot2018neural, lee2019wide}, but in practice they generally vary during training, as shown theoretically in \cite{liu2020linearity} and empirically in \cite{fort2020deep}. A growing theoretical literature \cite{golikov2022neural} investigates constant tangent kernel assumptions to study convergence and generalization of neural networks (e.g. \cite{jacot2018neural, lee2019wide, du2019gradient, bietti2019inductive, ghorbani2019limitations, geiger2020disentangling}). %, usually assuming continuous time gradient flow (whose dynamics can be close to the discrete time gradient descent performed in practice \cite{elkabetz2021continuous}). 
This present work relates more closely to \textit{empirical} studies making use of tangent kernels and linear approximations, such as \cite{lee2020finite, ortiz2021can} who highlight differences between lazy learning and real networks, and \cite{fort2020deep} who empirically investigate the relationship between loss landscapes and the evolution of $K^{\bm{\theta}}_t(\mathbf{x}, \mathbf{x}_i)$. }


\begin{figure}
\vspace{-1.5cm}
\centering
\tikzset{cylinder end fill/.style={path picture={
\pgftransformshift{\centerpoint}%
\pgftransformrotate{\rotate}%  
\pgfpathmoveto{\beforetop}%
\pgfpatharc{90}{-270}{\xradius and \yradius}%
\pgfpathclose
\pgfsetfillcolor{#1}%
\pgfusepath{fill}}
}}
\begin{tikzpicture}[align=left]
    % define some variables
    \def\cylheight{2.2cm}
    \def\aspect{0.5}
    \def\cylwidthinit{1cm}
    \def\cylwidthinc{0.4cm}
    % start with the test point x: testpoint
    \node (testpoint) {$\mathbf{x}$};
    % initial random predictions: randpreds
    \node[rectangle, draw=black, fill=black!5, thick, minimum height=0.8cm] (randpreds) [right=0.8cm of testpoint] {$f_{\bm{\theta}_0}(\mathbf{x})$};
    \draw[->, very thick, shorten >= 0.1cm] (testpoint.east) to (randpreds.west);
    \node[align=center] [above = 0cm of randpreds] {\footnotesize Random\\[-2pt]\footnotesize prediction};
    % first cylinder: c0
    \node [cylinder, rotate=0, minimum height=\cylheight, minimum width=\cylwidthinit, aspect=0.46, cylinder end fill=Tan!80, draw=black, left color=Tan, right color=Tan, middle color=Brown!20!yellow!10, shading angle=0] (c0) [right=0.9cm of randpreds] {$\Delta \tilde{f}_1(\mathbf{x})$};
    \path (randpreds) -- node [text width=2.5cm,midway,align=center] {$+$} (c0);
    \node (hole1) [right=-0.275cm of c0] {};
    \filldraw[inner color=Tan!90!Black!10, outer color=Tan!95!Black!5] (hole1) ellipse (0.075cm and 0.365cm);
    % second cylinder: c1
    \node [cylinder, rotate=0, minimum height=\cylheight,
    minimum width=\cylwidthinit + \cylwidthinc, aspect=0.5, cylinder end fill=Tan!80, draw=black, left color=Tan, right color=Tan, middle color=Brown!20!yellow!10, shading angle=0] (c1) [right=0.7cm of c0] {$\Delta \tilde{f}_2(\mathbf{x})$};
    \path (c0) -- node [text width=2.5cm,midway,align=center] {$+$} (c1);
    \node (hole2) [right=-0.29cm of c1] {};
    \filldraw[inner color=Tan!90!Black!10, outer color=Tan!95!Black!5] (hole2) ellipse (0.08cm and 0.56cm);
    % third cylinder: c2
    \node [cylinder, rotate=0, minimum height=\cylheight,
    minimum width=\cylwidthinit + 2*\cylwidthinc, aspect=0.6, cylinder end fill=Brown!35, draw=black, left color=Tan, right color=Tan, middle color=Brown!20!Yellow!10, shading angle=0] (c2) [right=1.3cm of c1] {$\Delta \tilde{f}_T(\mathbf{x})$};
    \path (c1) -- node [text width=2.5cm,midway,align=center] {$+\,\mydots\,+$} (c2);
    % output test prediction y: testpred
    \node (testpred) [right=0.7cm of c2] {$\tilde{f}_{\bm{\theta}_T}(\mathbf{x})$};
    \node[align=center] (outputlabel) [above=0cm of testpred] {\footnotesize Telescoping\\[-2pt]\footnotesize prediction};
    \draw[->, very thick, shorten <= 0.1cm] (c2.east) to (testpred.west);
    \node (lens) [right=-0.32cm of c2] {};
    \filldraw[inner color=cyan!10,outer color=Cyan!50!Blue!50] (lens) ellipse (0.12cm and 0.75cm);

    % add gradient annotation between randpred and c0
    \node (randpredlower) [left=1.25cm of c0.215] {};
    % \draw[->, very thick] (randpredlower)  .. controls ++(down:0.6cm) and ++(down:0.6cm) .. (c0.225) node[midway, below] (gradc0) {$\nabla_{\bm{\theta}} f_{\bm{\theta}_{1}}(\mathbf{x})$};
    \node (c0lowerleft) [below=-0.15cm of c0.215] {};
    \draw [-{Straight Barb[]}, very thick] (randpredlower) to[out=290,in=250, distance=0.5cm] node[midway, below] (gradc0) {$\nabla_{\bm{\theta}} f_{\bm{\theta}_{0}}(\mathbf{x})$} (c0lowerleft);
    \draw [-{>[width=2mm, length=1.5mm]}, gray, dashed] (testpoint) to[out=290,in=180] (gradc0.180);
    % add gradient annotation between c0 and c1
    \node (c0lower) [left=1.25cm of c1.225] {};
    \node (c1lowerleft) [below=-0.15cm of c1.225] {};
    \draw [-{Straight Barb[]}, very thick] (c0lower) to[out=290,in=250, distance=0.5cm] node[midway, below] (gradc1) {$\nabla_{\bm{\theta}} f_{\bm{\theta}_{1}}(\mathbf{x})$} (c1lowerleft);
    \draw [-{>[width=2mm, length=1.5mm]}, gray, dashed] (testpoint.250) to[out=280,in=190] (gradc1.180);

    % add parameter delta annotation between randpred and c0
    \node (randpredupper) [left=0.75cm of c0.165] {};
    \node (c0upperleft) [above=-0.15cm of c0.165] {};
    \draw [-{Straight Barb[]}, very thick] (randpredupper) to[out=60,in=120, distance=0.4cm] node[pos=0.7, above=-0.02cm] (deltac0) {$\Delta \bm{\theta}_1$} (c0upperleft);
    \node (c0upperright) [left=0.65cm of c1.160] {};
    \node (c1upperleft) [above=-0.15cm of c1.160] {};
    \draw [-{Straight Barb[]}, very thick] (c0upperright) to[out=60,in=120, distance=0.4cm] node[pos=0.5, above=-0.06cm] (deltac1) {$\Delta \bm{\theta}_2$} (c1upperleft);

    % add training data and arrows
    \node (traindata) [above =2cm of testpoint.west, anchor=west] {${\mathlarger{\mathlarger{\mathcal{D}^\text{train}}}}$};
    \draw [-{>[width=2mm, length=1.5mm]}, gray, dashed] (traindata.350) to[out=0,in=140] node[pos=0.2, below=-0.02cm] {$B_1$} (deltac0.140);
    \draw [-{>[width=2mm, length=1.5mm]}, gray, dashed] (traindata.10) to[out=0,in=140] node[pos=0.2, above=-0.02cm] {$B_2$} (deltac1.160);

    % add O(1) annotation
    \node (O1) [align=center, above left = 1.75cm and -1.55cm of c2.180] {Linear approximations \tikz{\node [cylinder, rotate=0, minimum height=\cylheight/4.5,
    minimum width=(\cylwidthinit + \cylwidthinc)/4.5, aspect=0.5, cylinder end fill=Tan!80, draw=black, left color=Tan, right color=Tan, middle color=Brown!20!yellow!10, shading angle=0] {}}\\$\Delta \tilde{f}_t(\mathbf{x}) \coloneq \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^T  \Delta \bm{\theta}_t$};
    \node (c1upperright) [above=0.05cm of c1.60] {};
    \draw [->] (O1.220) to[out=200,in=75, distance=0.4cm] (c1upperright);
    \node (c2uppercenter) [above=-0.1cm of c2.90] {};
    \draw [->] (O1.326) to[out=340,in=105, distance=0.4cm] (c2uppercenter);
    % \node [cylinder, rotate=0, minimum height=\cylheight/4.5,
    % minimum width=(\cylwidthinit + \cylwidthinc)/4.5, aspect=0.5, cylinder end fill=Tan!80, draw=black, left color=Tan, right color=Tan, middle color=Brown!20!yellow!10, shading angle=0] (c1) [right=0cm of O1] {};
    
\end{tikzpicture}
\vspace{-.7cm}
\caption{\textbf{Illustration of the telescoping model of a trained neural network.} \small Unlike the more standard framing of a neural network in terms of an iteratively learned set of parameters, the telescoping model takes a functional perspective on training a neural network in which an arbitrary test example's initially random prediction, $f_{\bm{\theta}_0}(\mathbf{x})$, is additively updated by a linearized adjustment $\Delta \tilde{f}_t(\mathbf{x})$ at each step $t$ as in \cref{eq:modeldef}.}

% Instead of looking at learning in neural networks in terms of iteratively updating its parameters, Unlike the more standard perspective of a neural network as a fixed set of parameters that are iteratively learned, the telescoping model takes a functional perspective in which an arbitrary text example's initially random prediction, $f_{\bm{\theta}_0}(\mathbf{x})$, is additively updated by a linear adjustment $\Delta \tilde{f}_t(\mathbf{x})$ at each step $t$. }
% Instead of approximating the full learning trajectory at once, we consider a model that uses linear approximations of what is learned \textit{at each of the $T$ parameter updates}. }
\vspace{-.3cm}
\label{fig:enter-label}
\end{figure}

\section{A Telescoping Model of Deep Learning}\label{sec:setup-model}
In this work, we explore whether we can exploit the approximation in \cref{eq:delta-approx} beyond the laziness assumption to gain new insight into neural network learning. Instead of applying the approximation across the entire training trajectory at once as in $\textstyle f^{lin}_{\bm{\theta}_T}(\mathbf{x})$, we consider using it \textit{incrementally} at each batch update during training to approximate \textit{what has been learned at this step}. This still provides us with a greatly simplified and transparent model of a neural network, and results in a much more reasonable approximation of the true network. Specifically, we explore whether -- instead of studying the final model $ f_{\bm{\theta}_T}(\mathbf{x})$ as a whole -- we can gain insight by \textit{telescoping out} the functional updates made throughout training, i.e. exploiting that we can always equivalently express $f_{\bm{\theta}_T}(\mathbf{x})$ as: 
% In this work, apply the same ideas as the lazy regime approximation described in \Cref{sec:background}, but \textitt{incrementally} at each batch update rather than over the entire training trajectory (thus extending the ideas of \cite{fort2020deep}). This provides us with a greatly simplified model of a neural network that results in a much more reasonable approximation of the true networks behavior. Specifically, we explore whether -- instead of studying the final model $ f_{\bm{\theta}_T}(\mathbf{x})$ as a whole -- we can gain insight by \textit{telescoping out} the functional updates made throughout training, i.e. exploiting that we can always equivalently express $ f_{\bm{\theta}_T}(\mathbf{x})$ as: 
%In this paper, we are in search of a \textit{simple} model for neural network learning in the hopes of gaining intuitive insight into some of the many counterintuitive phenomena observed in deep learning recently. To do so, we explore whether -- instead of studying the final model $ f_{\bm{\theta}_T}(\mathbf{x})$ as a whole -- we can gain insight by \textit{telescoping out} the functional updates made throughout training, i.e. exploiting that we can always equivalently express $ f_{\bm{\theta}_T}(\mathbf{x})$ as:  
\begin{equation}\label{eq:telescoping}
 \textstyle   f_{\bm{\theta}_T}(\mathbf{x}) = f_{\bm{\theta}_0}(\mathbf{x}) + \sum^T_{t=1} [f_{\bm{\theta}_t}(\mathbf{x}) - f_{\bm{\theta}_{t-1}}(\mathbf{x})] = f_{\bm{\theta}_0}(\mathbf{x}) + \sum^T_{t=1} \Delta f_t(\mathbf{x})
\end{equation} 
This representation of a trained neural network in terms of its learning trajectory rather than its final parameters is interesting because we are able to better reason about the impact of the training procedure on the intermediate updates $\Delta f_t(\mathbf{x})$ than the final function $ f_{\bm{\theta}_T}(\mathbf{x})$ itself. 
In particular, we investigate whether empirically monitoring behaviors of the sum in \cref{eq:telescoping} while making use of the approximation in \cref{eq:delta-approx} will enable us to gain practical insights into learning in neural networks, while incorporating a variety of modern design choices into the training process. That is, we explore the use of the following \textit{telescoping model} $\tilde{f}_{\bm{\theta}_T}(\mathbf{x})$ as an 
approximation of a trained neural network:
\tikzstyle{mybox} = [draw=black!70, fill=Periwinkle!4, very thick,
    rectangle, rounded corners, inner sep=10pt,
    inner ysep=3pt, % this tweaks the y-axis gap between text and box
    text depth=24pt, % increase this to lower the base of the box
    anchor=base]
\tikzstyle{fancytitle} =[rounded corners,fill=Periwinkle!20, text=black, very thick, draw=black!70]
\begin{center}
\begin{tikzpicture}
\node [mybox] (box){%
    \begin{minipage}{0.94\columnwidth}
    \begin{align}
      \tilde{f}_{\bm{\theta}_T}(\mathbf{x}) := f_{\bm{\theta}_0}(\mathbf{x}) + \sum^T_{t=1} \underbrace{\textcolor{\wacol}{\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top \Delta \bm{\theta}_t}}_{\substack{\text{(i) The \textit{weight-averaging}}\\ \text{representation}}} 
      = f_{\bm{\theta}_0}(\mathbf{x}) - \sum^T_{t=1}   \underbrace{\textcolor{\kernelcol}{\textstyle{\sum_{i \in [n]} }K^T_t(\mathbf{x}, \mathbf{x}_i) g^{\ell}_{it}}}_{\text{(ii) The \textit{kernel} representation}} \label{eq:modeldef}
    \end{align}
    \end{minipage}
};
\node[fancytitle, right=10pt] at (box.north west) {\textbf{Telescoping model of a {trained} neural network}};
% \node[fancytitle, rounded corners] at (box.east) {$\clubsuit$};
\end{tikzpicture}%
\end{center}%
{where $K^T_t(\mathbf{x}, \mathbf{x}_i)$ is determined by the neural tangent kernel as $\gamma_t K^{\bm{\theta}}_t(\mathbf{x}, \mathbf{x}_i)$ in the case of standard SGD (in which case (ii) can also be interpreted as a discrete-time approximation of \cite{domingos2020every}'s \textit{path kernel}), but can take other forms for different choices of learning algorithm as we explore in \cref{sec:otheroptimizers}.} 

\squish{\textbf{Practical considerations.} Before proceeding, it is important to emphasize that the telescoping approximation described in \cref{eq:modeldef} is intended as \textit{a tool for (empirical) analysis of learning in neural networks} and is \textit{not} being proposed as an alternative approach to training neural networks. Obtaining $\tilde{f}_{\bm{\theta}_T}(\mathbf{x})$ requires computing $\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})$ for each training \textit{and} testing example at each training step $t\in[T]$, leading to increased computation over standard training. Additionally, these computational costs are likely prohibitive for extremely large networks and datasets without further adjustments; for this purpose, further approximations such as \cite{mohamadi2023fast} could be explored. Nonetheless, computing $\tilde{f}_{\bm{\theta}_T}(\mathbf{x})$ -- or relevant parts of it -- is still feasible in many %practically 
pertinent settings as later illustrated in \cref{sec:application}.}  
 \begin{wrapfigure}[15]{r}{0.34\textwidth}
% \begin{wrapfigure}[15]{r}{0.95\textwidth}
    \centering
\includegraphics[width=0.35\textwidth]{figures/approx_main.png}
\vspace{-0.65cm}
    \caption{\small \textbf{Approximation error} of the telescoping ($\tilde{f}_{\bm{\theta}_t}(\mathbf{x})$, \textcolor{sharedred}{red}) and the linear model (${f}^{lin}_{\bm{\theta}_t}(\mathbf{x})$, \textcolor{sharedgray}{gray}).}
    \label{fig:approx}
     \vspace{-0.5cm}
\end{wrapfigure} \textbf{How good is this approximation?} In \cref{fig:approx}, we examine the quality of $\tilde{f}_{\bm{\theta}_t}(\mathbf{x})$ for a 3-layer fully-connected ReLU network of width 200, trained to discriminate 3-vs-5 from 1000 MNIST examples using the squared loss with SGD or AdamW \cite{loshchilov2017decoupled}. In \textcolor{sharedred}{red}, we plot its mean average approximation error ($\frac{1}{1000}\sum_{\mathbf{x} \in \mathcal{X}_{test}}|{f}_{\bm{\theta}_t}(\mathbf{x}) -  \tilde{f}_{\bm{\theta}_t}(\mathbf{x})|$) and observe that for small learning rates $\gamma$ the difference remains negligible. In \textcolor{sharedgray}{gray} we plot the same quantity for $\textstyle f^{lin}_{\bm{\theta}_t}(\mathbf{x})$ (i.e. the first-order expansion around $\bm{\theta}_0$) for reference  and find that iteratively telescoping out the updates instead improves the approximation \textit{by orders of magnitude} -- which is also reflected in their prediction performance (see \cref{app:res-approx}). Unsurprisingly, $\gamma$ controls approximation quality as it determines $||\Delta \bm{\theta}_t||$. Further, $\gamma$ \textit{interacts} with the optimizer choice -- e.g. Adam(W) \cite{kingma2014adam, loshchilov2017decoupled} naturally makes larger updates due to rescaling (see \cref{sec:otheroptimizers}) and therefore requires smaller $\gamma$ to ensure approximation quality than SGD. 



\section{A Closer Look at Deep Learning Phenomena Through a Telescoping Lens}\label{sec:application}
{Next, we turn to \textit{applying} the telescoping model. Below, we present three case studies revisiting existing experiments that provided evidence for a range of unexpected behaviors of neural networks. These case studies have in common that they highlight cases in which neural networks appear to generalize somewhat \textit{unpredictably}, which is also why each phenomenon has received considerable attention in recent years. For each, we then show that the telescoping model allows us to construct and extract metrics that can help predict and understand the unexpected performance of the networks. In particular, we investigate (i) surprising generalization curves (\cref{sec:complexity}), (ii) performance differences between gradient boosting and neural networks on some tabular tasks (\cref{sec:gradient-boosting}), and (iii) the success of weight averaging (\cref{sec:lmc}). We include an extended literature review in \cref{app:lit}, a detailed discussion of all experimental setups in \cref{app:exp}, and additional results in \cref{app:res}.}


\subsection{Case study 1: Exploring surprising generalization curves and benign overfitting}\label{sec:complexity}

\squish{Classical statistical wisdom provides clear intuitions about overfitting: models that can fit the training data too well -- because they have too many parameters and/or because they were trained for too long -- are expected to generalize poorly (e.g. \cite[Ch. 7]{hastie2009elements}). Modern phenomena like double descent \cite{belkin2019reconciling}, however, highlighted that pure capacity measures (capturing what \textit{could} be learned instead of what \textit{is} actually learned) would not be sufficient to understand the complexity-generalization relationship in deep learning \cite{belkin2021fit}. Raw parameter counts, for example, cannot be enough to understand the complexity of what has been learned by a neural network during training because, even when using \textit{the same architecture}, what is learned could be wildly different across various implementation choices within the optimization process -- and even at different points during the training process of the same model, as prominently exemplified by the grokking phenomenon \cite{power2022grokking}. Here, with the goal of finding clues that may help predict phenomena like double descent and grokking, we explore whether the telescoping model allows us to gain insight into the relative complexity of what is learned. }


{A complexity measure that avoids the shortcomings listed above -- because it allows to consider a \textit{specific trained} model -- was recently used by \cite{curth2023u} in their study of \textit{non-deep} double descent. As their measure $p^{0}_{\hat{\mathbf{s}}}$ builds on the literature on smoothers \cite{hastie1990GAM}, it requires to express learned predictions as a linear combination of the training labels, i.e. as $\textstyle f(\mathbf{x}) = \mathbf{\hat{s}}(\mathbf{x})\mathbf{y}=\sum_{i\in [n]}\hat{s}^i(\mathbf{x})y_i$. 
Then, \cite{curth2023u} define the \textit{effective parameters} $p^{0}_{\hat{\mathbf{s}}}$ used by the model when issuing predictions for some set of inputs $\{\mathbf{x}^0_j\}_{j \in \mathcal{I}_0}$ with indices collected in $\mathcal{I}_0$ (here, $\mathcal{I}_0$ is either  $\mathcal{I}_{train}=\{1, \ldots, n\}$ or $\mathcal{I}_{test}=\{n+1, \ldots, n+m\}$) as  $\textstyle  p^{0}_{\hat{\mathbf{s}}} \equiv  p(\mathcal{I}_{0}, \hat{\mathbf{s}}(\cdot))= \frac{n}{|\mathcal{I}_{0}|} \sum_{j \in \mathcal{I}_0} ||\hat{\mathbf{s}}(\mathbf{x}^0_j)||^2$.  Intuitively, the larger $\textstyle  p^{0}_{\hat{\mathbf{s}}}$, the less smoothing across the training labels is performed, which implies higher model complexity. 

{Due to the black-box nature of trained neural networks, however, it is not obvious how to link learned predictions to the labels observed during training. Here, we demonstrate how the telescoping model allows us to do precisely that -- enabling us to make use of $p^{0}_{\hat{\mathbf{s}}}$ as a proxy for complexity. We consider the special case of a single output ($k=1$) and training with squared loss $\ell(f(\mathbf{x}), y) = \frac{1}{2}(y - f(\mathbf{x}))^2$, and note that we can now exploit that the SGD weight update simplifies to} 
\begin{equation}
\Delta \bm{\theta}_t = \gamma_t \mathbf{T}_t (\mathbf{y} - \mathbf{f}_{\bm{\theta}_{t-1}}) \text{ where } \mathbf{y}=[y_1, \ldots, y_n]^\top \text{ and } \mathbf{f}_{\bm{\theta}_{t}}=[f_{\bm{\theta}_{t}}(\mathbf{x}_1), \ldots, f_{\bm{\theta}_{t}}(\mathbf{x}_n)]^\top.
\end{equation}
Assuming the telescoping approximation holds exactly, this implies functional updates 
\begin{equation}\label{eq:sqr-fct-update}
\Delta \tilde{f}_t(\mathbf{x}) = \gamma_t \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top \mathbf{T}_t(\mathbf{y} - \tilde{\mathbf{f}}_{\bm{\theta}_{t-1}})
\end{equation}
which use a linear combination of the training labels. Note further that after the \textit{first} SGD update
\begin{equation}
 \tilde{f}_{\bm{\theta}_1}(\mathbf{x})= {f}_{\bm{\theta}_0}(\mathbf{x}) + \Delta \tilde{f}_1(\mathbf{x}) = \underbrace{\gamma_1 \nabla_{\bm{\theta}} f_{\bm{\theta}_{0}}(\mathbf{x})^\top \mathbf{T}_1}_{\mathbf{s}_{\bm{\theta}_1}(\mathbf{x})} \mathbf{y} +  \underbrace{{f}_{\bm{\theta}_0}(\mathbf{x}) - \gamma_1 \nabla_{\bm{\theta}} f_{\bm{\theta}_{0}}(\mathbf{x})^\top \mathbf{T}_1 {\mathbf{f}}_{\bm{\theta}_{0}}}_{c^0_{\bm{\theta}_1}(\mathbf{x})}
\end{equation}
\squish{which means that the first telescoping predictions $\tilde{f}_{\bm{\theta}_1}(\mathbf{x})$ are indeed simply linear combinations of the training labels (and the predictions at initialization)! As detailed in \cref{app:theory-smooth}, this also implies that recursively substituting \cref{eq:sqr-fct-update} into \cref{eq:modeldef} further allows us to write \textit{any} prediction $\tilde{f}_{\bm{\theta}_t}(\mathbf{x})$ as a linear combination of the training labels and ${f}_{\bm{\theta}_0}(\cdot)$, i.e. $\tilde{f}_{\bm{\theta}_t}(\mathbf{x})=\mathbf{s}_{\bm{\theta}_t}(\mathbf{x})\mathbf{y} + c^0_{\bm{\theta}_t}(\mathbf{x})$ where the $1\!\times\! n$ vector $\mathbf{s}_{\bm{\theta}_t}(\mathbf{x})$ is a function of the kernels $\{K^t_{t'}(\cdot, \cdot)\}_{t'\leq t}$, and the scalar $c^0_{\bm{\theta}_t}(\mathbf{x})$ is a function of the  $\{K^t_{t'}(\cdot, \cdot)\}_{t'\leq t}$ and $f_{\bm{\theta}_0}(\cdot)$. We derive precise expressions for $\mathbf{s}_{\bm{\theta}_t}(\mathbf{x})$ and $c^0_{\bm{\theta}_t}(\mathbf{x})$ for different optimizers in \cref{app:theory-smooth} -- enabling us to use $\mathbf{s}_{\bm{\theta}_t}(\mathbf{x})$ to compute $p^{0}_{\hat{\mathbf{s}}}$ as a proxy for complexity below. } 

 \begin{wrapfigure}[14]{r}{0.3\textwidth}
 \vspace{-0.5cm}
    \centering
\includegraphics[width=0.28\textwidth]{figures/double_descent_main.png}
\vspace{-0.3cm}
    \caption{\small \textbf{Double descent} in MSE (top) and effective parameters $p^{0}_{\hat{\mathbf{s}}}$ (bottom) on CIFAR-10.}
    \label{fig:dd}
     \vspace{-0.3cm}
\end{wrapfigure}\textbf{Double descent: Model complexity vs model size.} While training error always monotonically decreases as model size (measured by parameter count) increases, \cite{belkin2019reconciling} made a surprising observation regarding test error in their seminal paper on \textit{double descent}: they found that test error initially improves with additional parameters and then worsens when the model is increasingly able to overfit to the training data (as is expected) but can \textit{improve again} as model size is increased further past the so-called interpolation threshold where perfect training performance is achieved. This would appear to contradict the classical U-shaped relationship between model complexity and test error \cite[Ch. 7]{hastie2009elements}. Here, we investigate whether tracking $p^{0}_{\hat{\mathbf{s}}}$ on train and test data separately will allow us to gain new insight into the phenomenon in neural networks.

In \cref{fig:dd}, we replicate the binary classification example of double descent in neural networks of \cite{belkin2019reconciling}, training single-hidden-layer ReLU networks of increasing width to distinguish cats and dogs on CIFAR-10 (we present additional results using MNIST in \cref{app:res-complex}). First, we indeed observe the characteristic behavior of error curves as described in \cite{belkin2019reconciling} (top panel). Measuring learned complexity using $p^{0}_{\hat{\mathbf{s}}}$, we then find that while $p^{train}_{\hat{\mathbf{s}}}$ monotonically increases as model size is increasing, the effective parameters used on the test data $p^{test}_{\hat{\mathbf{s}}}$ implied by the trained neural network \textit{decrease} as model size is increased past the interpolation threshold (bottom panel). Thus, paralleling the findings made in \cite{curth2023u} for linear regression and tree-based methods, we find that distinguishing between train- and test-time complexity of a neural network using $p^{0}_{\hat{\mathbf{s}}}$ provides new quantitative evidence that bigger networks are \textit{not} necessarily learning more complex prediction functions for unseen test examples, which resolves the ostensible tension between deep double descent and the classical U-curve. 
Importantly, note that $p^{test}_{\hat{\mathbf{s}}}$ can be computed without access to test-time labels, which means that the observed difference between  $p^{train}_{\hat{\mathbf{s}}}$ and $p^{test}_{\hat{\mathbf{s}}}$ allows to quantify whether there is \textit{benign overfitting} \cite{bartlett2020benign, yang2021taxonomizing} in a neural network.

\begin{figure}
\vspace{-1cm}
    \centering
    \includegraphics[width=.95\textwidth]{figures/complexity_main.png}\vspace{-.3cm}
    \caption{\small \textbf{Grokking} in mean squared error on a polynomial regression task (1, replicated from \cite{kumargrokking}) and in misclassification error on MNIST using a network with large initialization (2, replicated from \cite{liu2022omnigrok}) (top), against effective parameters (bottom). Column (3) shows test results on MNIST with standard initialization (with and without sigmoid activation) where time to generalization is quick and grokking does not occur.}\vspace{-.5cm}
    \label{fig:res-grok}
\end{figure}

{\textbf{Grokking: Model complexity throughout training.} The grokking phenomenon \cite{power2022grokking} then showcased that improvements in test performance \textit{during a single training run} can occur long after perfect training performance has been achieved (contradicting early stopping practice!). While \cite{liu2022omnigrok} attribute this to weight decay causing $||\bm{\theta}_t||$ to shrink late in training -- which they demonstrate on an MNIST example using unusually large $\bm{\theta}_0$ -- \cite{kumargrokking} highlight that grokking can also occur as the weight norm $||\bm{\theta}_t||$ \textit{grows} later in training -- which they demonstrate on a polynomial regression task. In \cref{fig:res-grok} we replicate\footnote{As detailed in \cref{app:exp}, we replicate \cite{kumargrokking}'s experiment exactly but adapt \cite{liu2022omnigrok}'s experiment into a binary classification task with lower learning rate $\gamma$ to enable the use of $\tilde{f}_{\bm{\theta}_T}(\mathbf{x})$. The reduction of $\gamma$ is needed here as the $\Delta \bm{\theta}_t$ are otherwise too large to obtain an accurate approximation and has a side effect that the grokking phenomenon appears visually less extreme as perfect training performance is achieved later in training.} both experiments while tracking $p^{0}_{\hat{\mathbf{s}}}$ to investigate whether this provides new insight into this apparent disagreement. Then, we observe that the continued improvement in test error, past the point of perfect training performance, is associated with divergence of $p^{train}_{\hat{\mathbf{s}}}$ and $p^{test}_{\hat{\mathbf{s}}}$ in \textit{both} experiments (analogous to the double descent experiment in \cref{fig:dd}), suggesting that grokking may reflect \textit{transition into} a measurably benign overfitting regime during training. In \cref{app:res-complex}, we additionally investigate mechanisms known to induce grokking, and show that later onset of generalization indeed coincides with later divergence of $p^{train}_{\hat{\mathbf{s}}}$ and $p^{test}_{\hat{\mathbf{s}}}$.}

\squish{\textbf{Inductive biases \& learned complexity.} We observed that the large $\bm{\theta}_0$ in \cite{liu2022omnigrok}'s MNIST example of grokking result in very large initial predictions $|f_{\bm{\theta}_0}(\mathbf{x})|\!\gg\! 1$. Because \textit{no} sigmoid is applied, the model needs to learn that all $y_i \!\in\! [0, 1]$ by reducing the magnitude of predictions substantially -- large $\bm{\theta}_0$ thus constitute a very poor inductive bias for this task.  One may expect that the better an inductive bias is, the less complex the component of the final prediction that is learned from data. To test whether this intuition is quantifiable, we repeat the MNIST experiment with standard initialization scale, with and without sigmoid activation $\sigma(\cdot)$, in column (3) of \cref{fig:res-grok} (training results shown in \cref{app:res-complex} for readability). We indeed find that both not only speed up learning significantly (a generalizing solution is found in $10^2$ instead of $10^5$ steps), but also substantially reduce effective parameters used, where the stronger inductive bias -- using  $\sigma(\cdot)$ -- indeed leads to the least learned complexity.}


{\textit{\textbf{Takeaway Case Study 1.}} The telescoping model enables us to use $p^{0}_{\hat{\mathbf{s}}}$ as a proxy for learned complexity, whose relative behavior on train and test data can quantify benign overfitting in neural networks.}


\subsection{Case study 2: Understanding differences between gradient boosting and neural networks}\label{sec:gradient-boosting}
\squish{Despite their overwhelming successes on image and language data, neural networks are -- perhaps surprisingly -- still widely considered to be outperformed by \textit{gradient boosted trees} (GBTs) on \textit{tabular data}, an important modality in many data science applications. Exploring this apparent Achilles heel of neural networks has therefore been the goal of multiple extensive benchmarking studies \cite{ grinsztajn2022tree, mcelfresh2024neural}. Here, we concentrate on a specific empirical finding of \cite{mcelfresh2024neural}: their results suggest that GBTs may particularly outperform deep learning on heterogeneous data with greater irregularity in input features, a characteristic often present in tabular data. Below, we first show that the telescoping model offers a useful lens to compare and contrast the two methods, and then use this insight to provide and test a new explanation of why GBTs can perform better in the presence of dataset irregularities. }


\squish{\textbf{Identifying (dis)similarities between learning in GBTs and neural networks.} We begin by introducing gradient boosting \cite{friedman2001greedy} closely following \cite[Ch. 10.10]{hastie2009elements}. Gradient boosting (GB) also aims to learn a predictor $\textstyle \hat{f}^{GB}: \mathcal{X} \rightarrow \mathbb{R}^k$ minimizing expected prediction loss $\ell$. While deep learning solves this problem by iteratively updating a randomly initialized set of \textit{parameters} that transform inputs to predictions, the GB formulation iteratively updates \textit{predictions} directly without requiring any iterative learning of parameters -- thus operating in function space rather than parameter space. 
Specifically, GB, with learning rate $\gamma$ and initialized at predictor $h_0(\mathbf{x})$, consists of a sequence $\textstyle \hat{f}^{GB}_T(\mathbf{x})=h_0(\mathbf{x}) + \gamma\sum^T_{t=1} \hat{h}_t(\mathbf{x}) $ where each $\hat{h}_t(\mathbf{x})$ improves upon the existing predictions $\textstyle \hat{f}^{GB}_{t-1}(\mathbf{x})$. The solution to the loss minimization problem can be achieved by executing steepest descent in function space \textit{directly}, where each update $\hat{h}_t$ simply outputs the negative training gradients of the loss function with respect to the previous model, i.e. $\textstyle \hat{h}_t(\mathbf{x}_i)  = -g_{it}^\ell$ where $g_{it}^\ell = \sfrac{\partial \ell(\hat{f}^{GB}_{{t-1}}(\mathbf{x}_i), y_i)}{\partial \hat{f}^{GB}_{t-1}(\mathbf{x}_i)}$.}
\vspace{.1cm}
\squish{However, this process is only defined at the training points $\textstyle \{\mathbf{x}_i, y_i\}_{i \in [n]}$. To obtain an estimate of the loss gradient for an arbitrary test point $\mathbf{x}$, each iterative update instead fits a weak learner $\textstyle \hat{h}_t(\cdot)$  to the current input-gradient pairs $\textstyle \{\mathbf{x}_i, -g^{\ell}_{it}\}_{i \in [n]}$ which can then also be evaluated new, unseen inputs.   While this process could in principle be implemented using any base learner, the term \textit{gradient boosting} today appears to exclusively refer to the approach outlined above implemented using shallow trees as $\hat{h}_t(\cdot)$ 
\cite{friedman2001greedy}. 
Focusing on trees which issue predictions by averaging the training outputs in each leaf, we can make use of the fact that these are sometimes interpreted as adaptive nearest neighbor estimators or kernel smoothers \cite{lin2006random, biau2010layered, curth2024random}, allowing us to express the learned predictor as:}\vspace{-.65cm}
\begin{equation}\label{eq:gbtdef}
  \hat{f}^{GB}(\mathbf{x}) = h_0(\mathbf{x}) - \gamma \sum^T_{t=1}\sum_{i\in[n]}\frac{\mathbf{1}\{{l}_{h_t}(\mathbf{x})={l}_{h_t}(\mathbf{x}_i)\}}{n_{{l}(\mathbf{x})}} g^{\ell}_{it} = h_0(\mathbf{x}) - \gamma \sum^T_{t=1} \sum_{i\in[n]}K_{\hat{h}_t}(\mathbf{x}, \mathbf{x}_i) g^{\ell}_{it} 
\end{equation}
\squish{where $\textstyle {l}_{\hat{h}_t}(\mathbf{x})$ denotes the leaf example $\mathbf{x}$ falls into, $\textstyle n_{l(\mathbf{x})}=\sum_{i\in[n]}\mathbf{1}\{{l}_{h_t}(\mathbf{x})={l}_{h_t}(\mathbf{x}_i)\}$ is the number of training examples in said leaf and $\textstyle K_{\hat{h}_t}(\mathbf{x}, \mathbf{x}_i)=\sfrac{1}{n_{leaf(\mathbf{x})}}\mathbf{1}\{{l}_{\hat{h}_t}(\mathbf{x})=l_{\hat{h}_t}(\mathbf{x}_i)\}$ is thus the kernel learned by the $\textstyle t^{th}$ tree $\textstyle \hat{h}_t(\cdot)$. Comparing \cref{eq:gbtdef} to the kernel representation of the telescoping model of neural network learning in \cref{eq:modeldef}, we make a perhaps surprising observation: the telescoping model of a neural network and GBTs have \textit{identical} structure and differ only in their used kernel! % it becomes obvious that if the telescoping model holds, the predictions learned through deep learning and GBTs have \textit{identical} structure and differ only in the used kernel! 
Below, we explore whether this new insight allows to understand some of their performance differences. }

\textbf{Why can GBTs outperform deep learning in the presence of dataset irregularities?}
Comparing \cref{eq:modeldef} and \cref{eq:gbtdef} thus suggests that at least some of the performance differences between neural networks and GBTs are likely to be rooted in the differences between the behavior of the neural network tangent kernels $K^{\bm{\theta}}_t(\mathbf{x}, \mathbf{x}_i) %= \frac{\mathbf{1}\{i \in B_t\}}{|B_t|} \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x}_i)
$ and GBT's tree kernels $K_{\hat{h}_t}(\mathbf{x}, \mathbf{x}_i)%=\frac{\mathbf{1}\{leaf_{\hat{h}_t}(\mathbf{x})=leaf_{\hat{h}_t}(\mathbf{x}_i)\}}{|leaf_{\hat{h}_t}|}
$. One difference is obvious and purely architectural: it is possible that either kernel encodes a better inductive bias to fit the underlying outcome-generating process of a dataset at hand. Another difference is more subtle and relates to the behavior of the learned model on new inputs $\mathbf{x}$: the tree kernels are likely to behave \textit{much} more predictable at test-time than the neural network tangent kernels. To see this, note that for the tree kernels we have that $\forall \mathbf{x} \in \mathcal{X}$ and $\forall i \in [n]$, $0 \leq K_{\hat{h}_t}(\mathbf{x}, \mathbf{x}_i) \leq 1$ and $\sum_{i \in [n]} K_{\hat{h}_t}(\mathbf{x}, \mathbf{x}_i)=1$; importantly, this is true regardless of whether $\mathbf{x}=\mathbf{x}_i$ for some $i$ or not. For the tangent kernels on the other hand, $K^{\bm{\theta}}_t(\mathbf{x}, \mathbf{x}_i)$ is in general unbounded and could behave \textit{very} differently for $\mathbf{x}$ not observed during training. This leads us to hypothesize that this difference may be able to explain \cite{mcelfresh2024neural}'s observation that GBTs perform better whenever features are heavy-tailed: if a test point $\mathbf{x}$ is very different from training points, the kernels implied by the neural network $\mathbf{k}^{\bm{\theta}}_t(\mathbf{x}) \coloneq [{K}^{\bm{\theta}}_t(\mathbf{x}, \mathbf{x}_1), \ldots, {K}^{\bm{\theta}}_t(\mathbf{x}, \mathbf{x}_n)]^\top$ may behave very differently than at train-time while the tree kernels $\mathbf{k}_{\hat{h}_t}(\mathbf{x}) \coloneq [K_{\hat{h}_t}(\mathbf{x}, \mathbf{x}_1), \ldots, K_{\hat{h}_t}(\mathbf{x}, \mathbf{x}_n)]^\top$ will be less affected. For instance, $\frac{1}{\sqrt{n}} \leq ||\mathbf{k}_{\hat{h}_t}(\mathbf{x})||_2 \leq 1$ for all $\mathbf{x}$ while $||\mathbf{k}^{\bm{\theta}}_{t}(\mathbf{x})||_2$ is generally unbounded. 

 \begin{wrapfigure}[20]{r}{0.4\textwidth}
 \vspace{-0.5cm}
    \centering
\includegraphics[width=0.4\textwidth]{figures/GBT_main_text.png}
\vspace{-0.7cm}
    \caption{\small \textbf{Neural Networks vs GBTs:} Relative performance (top) and behavior of kernels (bottom) with increasing test data irregularity using the \texttt{houses} dataset.}
    \label{fig:GBT}
     % \vspace{-0.5cm}
\end{wrapfigure}\squish{We empirically test this hypothesis on standard tabular benchmark datasets proposed in \cite{grinsztajn2022tree}. We wish to examine the performance of the models and the behavior of the kernels as inputs become increasingly irregular, evaluating if GBT's kernels indeed display more consistent behavior compared to the network's tangent kernels. As a simple notion for input irregularity, we apply principal component analysis to the inputs to obtain a lower dimensional representation of the data and sort the observations according to their distance from the centroid. For a fixed trained model, we then evaluate on test sets consisting of increasing proportions $p$ of the most irregular inputs (those in the top 10\% furthest from the centroid). We compare the GBTs to neural networks by examining (i) the most extreme values their kernel weights take at test-time relative to the training data (measured as $\frac{\frac{1}{T} \sum^T_{t=1} \max_{j \in \mathcal{I}^p_{test}}||\mathbf{k}_t(x_j)||_2}{\frac{1}{T} \sum^T_{t=1}\max_{i \in \mathcal{I}_{train}}||\mathbf{k}_t(\mathbf{x}_i)||_2}$) and (ii) how their relative mean squared error (measured as $\frac{MSE^{p}_{NN} - MSE^{p}_{GBT}}{MSE^{0}_{NN} -MSE^{0}_{GBT}}$) changes as the proportion $p$ of irregular examples increases. In \cref{fig:GBT} using \texttt{houses} and in \cref{app:res-gb} using additional datasets, we first observe that GBTs outperform the neural network already in the absence of irregular examples; this highlights that there may indeed be differences in the suitability of the kernels in fitting the outcome-generating processes. Consistent with our expectations, we then find that, as the test data becomes more irregular, the performance of the neural network decays faster than that of the GBTs. Importantly, this is well tracked by their kernels, where the unbounded nature of the network's tangent kernel indeed results in it changing its behavior on new, challenging examples. }%We obtain similar results we perform the same evaluation on additional datasets which we include in \hl{APPENDIX}. 

\squish{\textit{\textbf{Takeaway Case Study 2.}} \cref{eq:modeldef} provides a new lens for comparing neural networks to GBTs, and highlights that unboundedness in $\mathbf{k}^{\bm{\theta}}_t(\mathbf{x})$ can predict performance differences due to dataset irregularities.}

%We empirically test this hypothesis on standard benchmark tabular datasets proposed in \cite{grinsztajn2022tree}. We wish to examine the behavior of the kernels as inputs become increasingly irregular, evaluating if those of a GBT display more consistent behavior compared to those of a neural network. A simple notion of irregularity is to apply principal component analysis in order to obtain a lower dimensional representation of the data and sort the observations according to their distance from the centroid. For a fixed trained model, we evaluate on test sets consisting of increasing proportions of the more irregular inputs (i.e. those in the top 10\% furthest from the centroid). In \hl{FIGURE}, we compare the GBT model to the (telescoping) neural network and report their relative mean squared error and the magnitude of each methods kernels response to the test data as measured by the quantiles of the l2-norms of the test kernels (normalized by their maximum train time kernel norm). Consistent with our expectations, we find that as the test data becomes more irregular, the performance of the neural network decays faster than that of the GBT. Importantly, this behavior is well tracked by their respective kernels where the unbounded nature of the neural network kernel results in it taking more extreme values on new, challenging examples. We obtain similar results we perform the same evaluation on additional datasets which we include in \hl{APPENDIX}. 

\subsection{Case study 3: Towards understanding the success of weight averaging}\label{sec:lmc}
The final interesting phenomenon we investigate is that it is sometimes possible to simply average the weights $\bm{\theta}_1$ and $\bm{\theta}_2$ obtained from two stochastic training runs of the same model, resulting in a weight-averaged model that performs no worse than the individual models \cite{frankle2020linear, ainsworth2022git} -- which has important applications in areas such as federated learning. This phenomenon is known as linear mode connectivity (LMC) and is surprising as, a priori, it is not obvious that simply \textit{averaging the weights} of independent neural networks (instead of their predictions, as in a deep ensemble \cite{lakshminarayanan2017simple}), which are highly nonlinear functions of their parameters, would \text{not} greatly worsen performance.   While recent work has demonstrated empirically that it is sometimes possible to weight-average an even broader class of models after permuting weights \cite{singh2020model,entezari2021role, ainsworth2022git}, we focus here on understanding when LMC can be achieved for two models trained from the same initialization $\bm{\theta}_0$. 

In particular, we are interested in \cite{frankle2020linear}'s observation that LMC can emerge during training: the weights of two models $\bm{\theta}^{t'}_{jT}, j\in\{1,2\}$, which are initialized identically and follow identical optimization routine up until checkpoint $t'$ but receive different batch orderings and data augmentations after $t'$, can be averaged to give an equally performant model as long as $t'$ exceeds a so-called \textit{stability point} $t^*$, which was empirically discovered to occur early in training in \cite{frankle2020linear}. Interestingly, \cite[Sec. 5]{fort2020deep} implicitly hint at an explanation for this phenomenon in their empirical study of tangent kernels and loss landscapes, where they found an association between the disappearance of loss barriers between solutions during training and the rate of change in $K^{\bm{\theta}}_t(\cdot,\cdot)$. We further explore potential implications of this observation through the lens of the telescoping model below.


{\textbf{Why a transition into a constant-gradient regime would imply LMC.} Using the weight-averaging representation of the telescoping model, %we can extend on \cite{fort2020deep}'s empirical results to show that, 
it becomes easy to see that not only would stabilization of the tangent kernel be \textit{associated} with lower linear loss barriers, but the transition into a lazy regime during training -- i.e. reaching a point $t^*$ after which the model gradients no longer change -- can be \textit{sufficient} to imply LMC during training as observed in \cite{frankle2020linear} under a mild assumption on the performance of the two networks' \textit{ensemble}. To see this, let $\textstyle L(f) \coloneq \mathbb{E}_{X, Y \sim P}[\ell(f(X), Y)]$ denote the expected loss of $f$ and recall that if $\textstyle sup_{\alpha \in [0,1]} L(f_{\alpha\bm{\theta}^{t'}_{1T} + (1-\alpha){\bm{\theta}^{t'}_{2T}}}) - [\alpha L(f_{\bm{\theta}^{t'}_{1T}})+(1-\alpha)L(f_{\bm{\theta}^{t'}_{2T}})]\leq 0$ then LMC is said to hold. If we assume that ensembles $\textstyle \bar{f}^\alpha(\mathbf{x})\coloneq \alpha f_{\bm{\theta}^{t'}_{1T}}(\mathbf{x})+(1-\alpha)f_{\bm{\theta}^{t'}_{2T}}(\mathbf{x})$ perform no worse than the individual models (i.e. $L(\bar{f}^\alpha) \leq \alpha L(f_{\bm{\theta}^{t'}_{1T}})+(1-\alpha)L(f_{\bm{\theta}^{t'}_{2T}})$ $\forall \alpha \in [0, 1]$, as is usually the case in practice \cite{abe2023pathologies}), then one case in which LMC is guaranteed is if the predictions of weight-averaged model and ensemble are identical. In \cref{app:theory-lmc}, we show that if there exists some $t^* \in [0, T)$ after which the model gradients $\nabla_{\bm{\theta}} f_{\bm{\theta}^{t^*}_{jt}}(\cdot)$ no longer change (i.e. for all $t' \geq t^*$ the learned updates $\textstyle \bm{\theta}^{t'}_{jt}$ lie in a convex set $\textstyle \Theta^{stable}_j$ in which $\textstyle \nabla_{\bm{\theta}} f_{\bm{\theta}^{t'}_{jt}}(\cdot) \approx \nabla_{\bm{\theta}} f_{\bm{\theta}_{t^*}}(\cdot)$), then indeed}
 \begin{equation} \bar{f}^\alpha(\mathbf{x}) \approx f_{\alpha\bm{\theta}^{t'}_{1T} + (1-\alpha){\bm{\theta}^{t'}_{2T}}}(\mathbf{x}) \approx f_{\bm{\theta}_{t'}}(\mathbf{x})+ \textstyle \nabla_{\bm{\theta}} f_{\bm{\theta}_{t^*}}(\mathbf{x})^\top \sum^T_{t=t'+1}(\alpha \Delta \bm{\theta}^{t'}_{1t}+ (1-\alpha) \Delta \bm{\theta}^{t'}_{2t}). 
 \end{equation}
 That is, transitioning into a regime with constant model gradients during training can imply LMC because the ensemble and weight-averaged model become near-identical. This also has as an immediate corollary that models with the same $\bm{\theta}_0$ which train fully within this regime (e.g. those discussed in \cite{jacot2018neural, lee2019wide}) will have $t^*=0$. Note that, when using nonlinear (final) output activation $\sigma(\cdot)$ the post-activation model gradients will generally \textit{not} become constant during training (as we discuss in \cref{sec:otheroptimizers} for the sigmoid and as was shown theoretically in \cite{liu2020linearity} for general nonlinearities). If, however, the \textit{pre-activation model gradients} become constant during training and the \textit{pre-activation ensemble} -- which averages the two model's pre-activation outputs \textit{before} applying $\sigma(\cdot)$ -- performs no worse than the individual models (as is also usually the case in practice \cite{jeffares2024joint}), then the above also immediately implies LMC for such models. 

 \begin{figure}
 \vspace{-1cm}
     \centering
     \includegraphics[width=0.99\linewidth]{figures/lmc.png}
    \vspace{-.25cm}
     \caption{\small \textbf{Linear mode connectivity and gradient changes by $t'$.} (1) Decrease in accuracy when using averaged weights $\alpha\bm{\theta}^{t'}_{1T} + (1-\alpha){\bm{\theta}^{t'}_{2T}}$  for randomly initialized (\textcolor{sharedorange}{orange}) and pre-trained ResNet-20 (\textcolor{sharedgreen}{green}). \\ (2) \& (3) Changes in model gradients by layer for a randomly initialized (2) and pretrained (3) model.}
     \vspace{-.35cm}
     \label{fig:LMC}
 \end{figure}

\squish{This suggests a candidate explanation for why LMC emerged at specific points in \cite{frankle2020linear}. To test this,  we replicate their CIFAR-10 experiment using a ResNet-20 in \cref{fig:LMC}. In addition to plotting the maximal decrease in accuracy when comparing $f_{\alpha\bm{\theta}^{t'}_{1T} + (1-\alpha){\bm{\theta}^{t'}_{2T}}}(\mathbf{x})$ to the weighted average of the accuracies of the original models as \cite{frankle2020linear} to measure LMC in (1), we also plot the squared change in (pre-softmax) gradients ${(\nabla_{\bm{\theta}} f_{\bm{\theta}_{t'+390}}(\mathbf{x})-\nabla_{\bm{\theta}} f_{\bm{\theta}_{t'}}(\mathbf{x}))^2}$  over the next epoch (390 batches) after checkpoint $t'$, averaged over the test set and the parameters in each layer in (2). We find that the disappearance of the loss barrier indeed coincides with the time in training when the model gradients become \textit{more stable} across all layers. Most saliently, the appearance of LMC appears to correlate with the stabilization of the gradients of the linear output layer. However, we also continue to observe some changes in other model gradients, which indicates that these models do not train fully linearly. }

%  \begin{wrapfigure}[19]{r}{0.4\textwidth}
%  \vspace{-0.63cm}
%     \centering
% \includegraphics[width=0.4\textwidth]{figures/LMC_main.pdf}
% \vspace{-0.75cm}
%     \caption{\small \textbf{LMC.} Decrease in accuracy when using averaged weights $\alpha\bm{\theta}^{t'}_{1T} + (1-\alpha){\bm{\theta}^{t'}_{2T}}$ (top) and relative change in gradients within one epoch of $t'$, by $t'$ for randomly initialized (\textcolor{sharedblue}{blue}) and pre-trained ResNet-20 (\textcolor{sharedgreen}{green}).}
%     \label{fig:LMC}
%      % \vspace{-0.5cm}
% \end{wrapfigure}

{\textbf{Pre-training and weight averaging.} Because weight averaging methods have become increasingly popular when using \textit{pre-trained} instead of randomly initialized models \cite{neyshabur2020being, wortsman2022model, choshen2022fusing}, we are interested in testing whether pre-training may improve mode connectability through stabilizing the model gradients. To test this, we replicate the above experiment with the same architecture pre-trained on the SVHN dataset (in \textcolor{sharedgreen}{green} in \cref{fig:LMC}(1)). Mimicking findings of \cite{neyshabur2020being}, we first find the loss barrier to be substantially lower after pre-training. In \cref{fig:LMC}(3), we then observe that the gradients in the hidden and final layers indeed change less and stabilize earlier in training than in the randomly initialized model -- yet the gradients of the BatchNorm parameters change \textit{more}. Overall, the findings in this section thus highlight that while there may be a connection between gradient stabilization and LMC, it cannot fully explain it -- suggesting that further investigation into the phenomenon using this lens, particularly into the role of BatchNorm layers, may be fruitful. } 




\squish{\textit{\textbf{Takeaway Case Study 3.}} Reasoning through the learning process by telescoping out functional updates suggests that averaging model parameters trained from the same checkpoint can be effective if their models' gradients remain stable, however, this cannot fully explain LMC in the setting we consider.}

\section{The Effect of Design Choices on Linearized Functional Updates}\label{sec:otheroptimizers}

{The literature on the neural tangent kernel primarily considers plain SGD, while modern deep learning practice typically relies on a range of important modifications to the training process (see e.g. \cite[Ch. 6]{prince2023understanding}) 
-- this includes many of the experiments demonstrating surprising deep learning phenomena we examined in \cref{sec:application}. To enable us to use modern optimizers above, we derived their implied linearized functional updates through the weight-averaging representation $\Delta \tilde{f}_t(\mathbf{x}) = \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top \Delta \bm{\theta}_t$, which in turn allows us to define $K^T_t(\cdot, \cdot)$ in \cref{eq:modeldef} for these modifications using straightforward algebra. As a by-product, we found that this provides us with an interesting and pedagogical formalism to reason about the relative effect of different design choices in neural network training, and elaborate on selected learnings below.}

\textbf{\textbullet{ } Momentum} with scalar hyperparameter $\beta_1$ smoothes weight updates by employing an exponentially weighted average over the previous parameter gradients as $\textstyle \Delta \bm{\theta}_t = -\gamma_t \frac{1-\beta_1}{1-\beta^t_1}\sum^t_{k=1}\beta_1^{t-k} \mathbf{T}_k \mathbf{g}^{\ell}_k$ instead of using the current gradients alone. This implies linearized functional updates
 \begin{equation}\label{eq:momentum}
 \textstyle  \Delta \tilde{f}_t(\mathbf{x}) = -\gamma_t \frac{1-\beta_1}{1-\beta^t_1}\sum_{i \in [n]}(K^{\bm{\theta}}_t(\mathbf{x}, \mathbf{x}_i) g^{\ell}_{it} + \sum^{t-1}_{k=1}\beta_1^{t-k}{K^{\bm{\theta}}_{t,k}(\mathbf{x}, \mathbf{x}_i)} g^{\ell}_{ik} )
 \end{equation}
\squish{where $\textstyle K^{\bm{\theta}}_{t, k}(\mathbf{x}, \mathbf{x}_i) \coloneq \frac{\mathbf{1}\{i \in B_k\}}{|B_k|}\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top\nabla_{\bm{\theta}} f_{\bm{\theta}_{k-1}}(\mathbf{x}_i)$ denotes the cross-temporal tangent kernel. %the cross-temporal tangent kernels. 
Thus, the functional updates also utilize \textit{previous} loss gradients, where their weight is determined using an inner product of the model gradient features from different time steps. If $\nabla_{\bm{\theta}} f_{\bm{\theta}_t}(\mathbf{x})$ is constant throughout training % (i.e. lazy learning) 
and we use full-batch GD, then the contribution of each training example $i$ to $\Delta \tilde{f}_t(\mathbf{x})$ reduces to $-\gamma_t K^{\bm{\theta}}_0 (\mathbf{x}, \mathbf{x}_i)\frac{1-\beta_1}{1-\beta^t_1}[\sum^t_{k=1}\beta_1^{t-k}g^{\ell}_{ik}]$, an exponentially weighted moving average over its past loss gradients -- making the effect of momentum on functional updates analogous to its effect on updates in parameter space. %While momentum is generally understood as being applied to the model parameters, this representation thus reveals an interesting equivalent dual interpretation of momentum being applied to \textit{the tangent kernel-loss gradient product}. 
However, if $\nabla_{\bm{\theta}} f_{\bm{\theta}_t}(\mathbf{x})$ changes over time, it is e.g. possible that $K^{\bm{\theta}}_{k, t}(\mathbf{x}, \mathbf{x}_i)$ has opposite sign from $K^{\bm{\theta}}_{t}(\mathbf{x}, \mathbf{x}_i)$ in which case momentum reduces instead of amplifies the effect of a previous $g^{\ell}_{it}$. This is more obvious when re-writing \cref{eq:momentum} to collect all terms containing a specific $g^\ell_{it}$, leading to $K^T_t(\mathbf{x}, \mathbf{x}_i)=\sum^T_{k=t}\gamma_k \frac{1-\beta_1}{1-\beta^k_1}\beta_1^{k-t}K_{k, t}(\mathbf{x}, \mathbf{x}_i)$ for \cref{eq:modeldef}.} % i) it uses also past loss gradients $g^{\ell}_{ik}$ to update parameters at time $t>k$ and ii) it additionally makes use of cross-temporal tangent kernels.

 % intuitions 

\textbf{\textbullet{ } Weight decay} with scalar hyperparameter $\lambda$ uses $\Delta \bm{\theta}_t = -\gamma_t(\mathbf{T}_t \mathbf{g}^{\ell}_t + \lambda \bm{\theta}_{t-1})$. For constant learning rate $\gamma$ this gives $\bm{\theta}_t=\bm{\theta}_0 - \sum^t_{k=1} \gamma (\mathbf{T}_k \mathbf{g}^{\ell}_k + \lambda \bm{\theta}_{k-1})=(1-\lambda \gamma)^t \bm{\theta}_0 - \gamma \sum^t_{k=1}(1-\lambda \gamma)^{t-k}\mathbf{T}_k \mathbf{g}^\ell_k$. This then implies linearized functional updates 
\begin{equation}\label{eq:wd}
\begin{split}
    \textstyle  \Delta \tilde{f}_t(\mathbf{x}) = -\gamma \sum_{i \in [n]} (K_t(\mathbf{x}, \mathbf{x}_i) g^{\ell}_{it} - \lambda \gamma \sum^{t-1}_{k=1}(1-\lambda \gamma)^{t-1-k} K_{t, k}(\mathbf{x}, \mathbf{x}_i) g^{\ell}_{ik})\\ - \gamma \lambda (1-\lambda \gamma)^{t-1}\nabla_{\bm{\theta}}   f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top \bm{\theta}_0
\end{split}
\end{equation}
For full-batch GD and constant tangent kernels, $-\gamma K^{\bm{\theta}}_0 (\mathbf{x}, \mathbf{x}_i)[g_{it} - \lambda\gamma \sum^{t-1}_{k=1}(1-\lambda\gamma)^{t-1-k}g_{ik}$] is the contribution of each training example to the functional updates, which effectively decays the previous contributions of this example. Further, comparing the signs in \cref{eq:wd} to \cref{eq:momentum} highlights that momentum can \textit{offset} the effect of weight decay on the learned updates in function space (in which case weight decay mainly acts through the term decaying the initial weights $\bm{\theta}_0$).% while weight decay reduces the effect of previous loss gradients, momentum amplifies them.

% intuitions 

\squish{\textbf{\textbullet{ } Adaptive \& parameter-dependent learning rates} are another important modification in practice %popular strategy %to improve upon vanilla SGD 
which enable the use of different step-sizes across parameters by dividing $\Delta \bm{\theta}_t$ \textit{elementwise} by a $p\times 1$ scaling vector ${\bm{\phi}_t}$. %. This is achieved  by simply dividing intended weight updates %vanilla update $-\gamma_t\mathbf{T}_t \mathbf{g}^{\ell}_t$ \textit{element-wise} by a %$p \times 1$ scaling vector ${\bm{\phi}_t}$.  
Most prominently, this is used to adaptively normalize the magnitude of updates (e.g. Adam \cite{kingma2014adam} uses $\textstyle {\bm{\phi}_t} = \sqrt{\frac{1-\beta_2}{1-\beta^t_2}\sum^t_{k=1}\beta_2^{t-k} [\mathbf{T}_k \mathbf{g}^{\ell}_k]^2}+\epsilon$). When combined with plain SGD, this results in kernel $K^{\bm{\phi}}_t(\mathbf{x}, \mathbf{x}_i) = \frac{\mathbf{1}\{i \in B_t\}}{|B_t|}\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top \text{diag}(\tfrac{1}{{\bm{\phi}_t}})\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x}_i)$. This expression highlights that ${\bm{\phi}_t}$ admits an elegant interpretation as \textit{re-scaling the relative influence of features} on the tangent kernel, similar to structured kernels in non-parametric regression \cite[Ch. 6.4.1]{hastie2009elements}. }
% intuition

{\textbf{\textbullet{ } Architecture design choices} also impact the form of the kernel. One important practical example is whether $f_{\bm{\theta}}(\mathbf{x})$ applies a non-linear activation function to the output $g_{\bm{\theta}}(\mathbf{x})\in \mathbb{R}$ of its final layer. Consider the choice of using the sigmoid $\sigma({z})=\tfrac{1}{1+e^{-z}}$ for a binary classification problem and recall $\frac{\partial}{\partial z}\sigma({z}) =\sigma({z})(1-\sigma({z}))\in (0, \sfrac{1}{4}]$, which is largest where $\sigma({z})=\sfrac{1}{2}$ and smallest when $\sigma({z})\rightarrow 0 \lor 1$. If $K^{\bm{\theta}, g}_t(\mathbf{x}, \mathbf{x}_i)\coloneq\frac{\mathbf{1}\{i \in B_t\}}{|B_t|}\nabla_{\bm{\theta}} \,g_{\bm{\theta}_{t-1}}(\mathbf{x})^\top\nabla_{\bm{\theta}} \,g_{\bm{\theta}_{t-1}}(\mathbf{x}_i)$ denotes the tangent kernel of the model without activation, it is easy to see that the tangent kernel of the model $\sigma(g_{\bm{\theta}_t}(\mathbf{x}))$ is }
\begin{equation}\label{eq:sigmoid-tk}
    K^{\bm{\theta}, \sigma}_t(\mathbf{x}, \mathbf{x}_i) = \sigma(g_{\bm{\theta}_t}(\mathbf{x}))(1-\sigma(g_{\bm{\theta}_t}(\mathbf{x}))) \sigma(g_{\bm{\theta}_t}(\mathbf{x}_i))(1-\sigma(g_{\bm{\theta}_t}(\mathbf{x}_i)))K^{\bm{\theta}, g}_t(\mathbf{x}, \mathbf{x}_i)
\end{equation}
{indicating that $K^{\bm{\theta}, \sigma}_t(\mathbf{x}, \mathbf{x}_i)$ will give relatively higher weight in functional updates to training examples $i$ for which the model is uncertain ($\sigma(g(\mathbf{x}_i))\approx \sfrac{1}{2})$) and lower weight to examples where the model is certain ($\sigma(g_{\bm{\theta}_t}(\mathbf{x}_i))\approx 0 \lor 1$) -- 
\textit{regardless} of whether $\sigma(g_{\bm{\theta}_t}(\mathbf{x}_i))$ is the correct label. 
Conversely, \cref{eq:sigmoid-tk} also implies that when comparing the functional updates of $\sigma(g_{\bm{\theta}}(\mathbf{x}))$ to those of $g_{\bm{\theta}}(\mathbf{x})$  across inputs $\mathbf{x} \in \mathcal{X}$, updates with $\sigma(\cdot)$ will be relatively larger for $\mathbf{x}$ where the model is uncertain ($\sigma(g_{\bm{\theta}_t}(\mathbf{x}))\approx \sfrac{1}{2})$). Finally, \cref{eq:sigmoid-tk} also highlights that the (post-activation) tangent kernel of a model with sigmoid activation will generally not be constant in $t$ unless the model predictions $\sigma(g_{\bm{\theta}_t}(\mathbf{x}))$ do not change.}

\squish{\section{Conclusion}\label{sec:conclusion}This work investigated the utility of a telescoping model for neural network learning, consisting of a sequence of linear approximations, 
as a tool for understanding several recent deep learning phenomena. By revisiting existing empirical observations, we demonstrated how this perspective provides a lens through which certain surprising behaviors of deep learning can become more intelligible.  In each case study, we intentionally restricted ourselves to specific, noteworthy empirical examples which we proceeded to re-examine in greater depth. We believe that there are therefore many interesting opportunities for future research to expand on these initial findings by building upon the ideas we present to investigate such phenomena in more generality, both empirically and theoretically.} 

\newpage
\subsection*{Acknowledgements} We would like to thank James Bayliss, who first suggested to us to look into explicitly unravelling SGD updates to write trained neural networks as approximate smoothers to study deep double descent after a seminar on our paper \cite{curth2023u} on non-deep double descent. This suggestion ultimately inspired many investigations far beyond the original double descent context. We are also grateful to anonymous reviewers for helpful comments and suggestions. AC and AJ gratefully acknowledge funding from AstraZeneca and the Cystic Fybrosis Trust, respectively. This work was supported by a G-Research grant, and Azure sponsorship credits granted by Microsofts AI for Good Research Lab.

\bibliographystyle{alpha}
\bibliography{references}
\newpage
\appendix
\section*{Appendix}
This appendix is structured as follows: \cref{app:lit} presents an extended literature review, \cref{app:theory} presents additional theoretical derivations, \cref{app:exp} presents an extended discussion of experimental setups and \cref{app:res} presents additional results. The NeurIPS paper checklist is included after the appendices.
\section{Additional literature review}\label{app:lit}
In this section, we present an extended literature review related to the phenomena we consider in \cref{sec:complexity} and \cref{sec:lmc}. 

\subsection{The model complexity-performance relationship (\cref{sec:complexity})} Classical statistical textbooks convey a well-understood relationship between model complexity -- historically captured by a model's parameter count --  and prediction error: increasing model complexity is expected to modulate a transition between under- and overfitting regimes, usually represented by a U-shaped error-curve with model complexity on the x-axis in which test error first improves before it worsens as the training data can be fit too well \cite{hastie1990GAM, vapnik1999nature, hastie2009elements}. While this relationship was originally believed to hold for neural networks as well \cite{geman1992neural}, later work provided evidence that -- when using parameter counts to measure complexity -- this U-shaped relationship no longer holds \cite{neal2018modern, neal2019bias}. 

\textbf{Double descent. } Instead, the double descent \cite{belkin2019reconciling} shape has claimed its place, which postulates that the well-known U-shape holds only in the underparameterized regime where the number of model parameters $p$ is smaller than the number of training examples $n$; once we reach the interpolation threshold $p=n$ at which models have sufficient capacity to fit the training data perfectly, increasing $p$ further into the overparametrized (or: interpolation) regime leads to test error improving again. While the double descent shape itself had been previously observed in linear regression and neural networks in \cite{vallet1989linear, bos1996dynamics, advani2020high, neal2018modern, spigler2018jamming} (see also the historical note in \cite{loog2020brief}),  the seminal paper by \cite{belkin2019reconciling} both popularized it as a phenomenon and highlighted that the double descent shape can also occur tree-based methods. In addition to double descent as a function of the number of model parameters, the phenomenon has since been shown to emerge also in e.g. the number of training epochs\cite{nakkiran2021deep} and sparsity \cite{he2022sparse}. Optimal regularization has been shown to mitigate double descent \cite{nakkiran2020optimal}.

Due to its surprising and counterintuitive nature, the emergence of the double descent phenomenon sparked a rich theoretical literature attempting to understand it. One strand of this literature has focused on modeling double descent in the number of features in linear regression and has produced precise theoretical analyses for particular data-generating models  \cite{belkin2020two, advani2020high, bartlett2020benign, derezinski2020exact, hastie2022surprises, schaeffer2023double, chen2021multiple}. Another strand of work has focused on deriving exact expressions of bias and variance terms as the total number of model parameters is increased in a neural network by taking into account all sources of randomness in model training \cite{neal2018modern, adlam2020understanding, d2020double, lin2021causes}. A different perspective was presented in \cite{curth2023u}, who highlighted that in the non-deep double descent experiments of \cite{belkin2019reconciling}, a subtle change in the parameter-increasing mechanism is introduced exactly at the interpolation threshold, which is what causes the second descent. \cite{curth2023u} also demonstrated that when using a measure of the test-time \textit{effective} parameters used by the model to measure complexity on the x-axes, the double descent shapes observed for linear regression, trees, and boosting fold back into more traditional U-shaped curves. In \cref{sec:complexity}, we show that the telescoping model enables us to discover the same effect also in deep learning.

\textbf{Benign overfitting.} Closely related to the double descent phenomenon is \textit{benign overfitting} (e.g. \cite{belkin2018understand, ma2018power, bartlett2020benign, chatterji2021finite, mallinar2022benign, wyner2017explaining, haas2024mind}), i.e. the observation that, incompatible with conventional statistical wisdom about overfitting \cite{hastie2009elements}, models with perfect training performance can nonetheless generalize well to unseen test examples. In this literature, it is often argued in theoretical studies that overparameterized neural networks generalize well because they are much more well-behaved around unseen test examples than examples seen during training \cite{mallinar2022benign, haas2024mind}. In \cref{sec:complexity} we provide new \textit{empirical} evidence for this by highlighting that there is a difference between $p^{train}_{\hat{\mathbf{s}}}$  and  $p^{test}_{\hat{\mathbf{s}}}$.


\textbf{Understanding modern model complexity.} Many measures for model complexity capture some form of \textit{capacity} of a hypothesis class, which gives insight into the most complex function that \textit{could be} learned -- e.g. raw parameter counts and  VC dimensions \cite{bousquet2003introduction}. The double descent and benign overfitting phenomena prominently highlighted that complexity measures that consider only what \textit{could} be learned and not what \textit{is actually} learned for test examples, would be unlikely to help understand generalization in deep learning \cite{belkin2021fit}. Further, \cite{curth2023u} highlighted that many other measures for model complexity -- so-called measures of effective parameters (or: degrees of freedom) including measures from the literature of smoothers \cite[Ch. 3.5]{hastie1990GAM} as well as measures relying on the model's Hessian \cite{moody1991effective, mackay1991bayesian} (which have been considered for use in deep learning in \cite{maddox2020rethinking}) -- were derived in the context of in-sample prediction (where train- and test inputs would be the same) and do thus not allow to distinguish differences in the behavior of learned functions on training examples from new examples. \cite{curth2024classical} highlight that this difference in setting -- the move from in-sample prediction to measuring performance in terms of out-of-sample generalization -- is crucial for the emergence of apparently counterintuitive modern machine learning phenomena such as double descent and benign overfitting.  For this reason, \cite{curth2023u} proposed an adapted effective parameter measure for smoothers that can distinguish the two, and highlighted that differentiating between the amount of smoothing performed on train- vs test examples is crucial to understanding double descent in linear regression, trees and gradient boosting. In \cref{sec:complexity}, we show that the telescoping model makes it possible to use \cite{curth2023u}'s effective parameter measure for neural networks, allowing interesting insight into implied differences in train- and test-time complexity of neural networks. 

\textbf{Grokking.} Similar to double descent in the number of training epochs as observed in \cite{nakkiran2021deep} (where the test error first improves then gets worse and then improves again during training), the \textit{grokking} phenomenon \cite{power2022grokking} demonstrated the emergence of another type of unexpected behavior during the training run of a single model. Originally demonstrated on arithmetic tasks, the phenomenon highlights that improvements in test performance can sometimes occur long after perfect training performance has already been achieved. \cite{liu2022omnigrok} later demonstrated that this can also occur on more standard tasks such as image classification. This phenomenon has attracted much recent attention both because it appears to challenge the common practice of early stopping during training and because it showcases further gaps in our current understanding of learning dynamics.  A number of explanations for this phenomenon have been put forward recently: \cite{liu2022towards} attribute grokking to delayed learning of representations, \cite{nanda2023progress} use mechanistic explanations to examine case studies of grokking, \cite{varma2023explaining} attribute grokking to more efficient circuits being learned later in training,  \cite{liu2022omnigrok} attribute grokking to the effects of weight decay setting in later in training and \cite{thilak2022slingshot} attribute grokking to the use of adaptive optimizers. \cite{kumargrokking} highlight that the latter two explanations cannot be the sole reason for grokking by constructing an experiment where grokking occurs as the weight norm grows without the use of adaptive optimizers. Instead, \cite{kumargrokking, lyu2023dichotomy} conjecture that grokking occurs as a model transitions from the lazy regime to a feature learning regime later in training. Finally, \cite{levi2023grokking} show analytically and experimentally that grokking can also occur in simple linear estimators, and \cite{miller2023grokking} similarly study grokking outside neural networks, including Bayesian models. Our perspective presented in \cref{sec:complexity} is complementary to these lines of work: we highlight that grokking coincides with the widening of a gap in effective parameters used for training and testing examples and that there is thus a quantifiable benign overfitting effect at play. 

\subsection{Weight averaging in deep learning (\cref{sec:lmc})}
Ensembling \cite{dietterich2002ensemble}, i.e. averaging the \textit{predictions} of multiple independent models, has long established itself as a popular strategy to improve prediction performance over using single individual models. While ensembles have historically been predominantly implemented using weak base learners like trees to form random forests \cite{breiman2001random}, \textit{deep} ensembles \cite{lakshminarayanan2017simple} -- i.e. ensembles of neural networks -- have more recently emerged as a popular strategy for improving upon the performance of a single network \cite{lakshminarayanan2017simple, fort2019deep}. Interestingly, deep ensembles have been shown to perform well both when averaging the predictions of the underlying models and when averaging the pre-activations of the final network layers \cite{jeffares2024joint}. 

A much more surprising empirical observation made in recent years is that, instead of averaging model predictions as in an ensemble, it is sometimes also possible to average the learned \textit{weights} $\bm{\theta}_1$ and $\bm{\theta}_2$ of two trained neural networks and obtain a model that performs well \cite{izmailov2018averaging, frankle2020linear}. This is unexpected because neural networks are \textit{highly nonlinear} functions of their weights, so it is unclear a priori when and why averaging two sets of weights would lead to a sensible model at all. When weight averaging works, it is a much more attractive solution relative to ensembling: an ensemble consisting of $k$ models requires $k \times p$ model parameters, while a weight-averaged model requires only $p$ parameters -- making weight-averaged models both more efficient in terms of storage and at inference time. Additionally, weight averaging has interesting applications in federated learning because it could enable the merging of models trained on disjoint datasets. \cite{izmailov2018averaging} were the first to demonstrate that weight averaging can work in the context of neural networks by showing that model weights obtained by simple averaging of multiple points along the trajectory of SGD during training -- a weight-space version of the method of fast geometric ensembling \cite{garipov2018loss} -- could improve upon using the final solution directly. 

\textbf{Mode connectivity.} The literature on mode connectivity first empirically demonstrated that there are simple (but nonlinear) paths of nonincreasing loss connecting different final network weights obtained from different random initializations \cite{freeman2016topology, draxler2018essentially, garipov2018loss}. As discussed in the main text, \cite{frankle2020linear} then demonstrated empirically that two learned sets of weights can sometimes be \textit{linearly} connected by simply interpolating between the learned weights, as long as two models were trained together until some stability point $t^*$. \cite{altintacs2023disentangling} perform an empirical study investigating which networks and optimization protocols lead to mode connectivity from initialization (i.e. $t^*=0$) and which modifications ensure $t^*>0$. As highlighted in \cref{sec:lmc}, our theoretical reasoning indicates that one sufficient condition for linear mode connectivity from initialization is that models stay in a regime in which the model gradients do not change during training. In the context of \textit{task arithmetic}, where parameters from models finetuned on separate tasks are added or subtracted  (\textit{not} averaged) to add or remove a skill, \cite{ortiz2024task} find that pretrained CLIP models that are finetuned on separate tasks and allow to perform task arithmetic \textit{do not} operate in a regime in which gradients are constant.

\textbf{Methods that average weights.} Beyond \cite{izmailov2018averaging}'s stochastic weight averaging method, which averages weights from checkpoints within a single training run, weight averaging has also recently gained increased popularity in the context of averaging multiple models finetuned from the same pre-trained model \cite{neyshabur2020being, wortsman2022model, choshen2022fusing}: while \cite{neyshabur2020being} showed that multiple models finetuned from the same pretrained model lie in the same loss basin and are linearly mode connectible, the model soups method of \cite{wortsman2022model} highlighted that simply averaging the weights of multiple models fine-tuned from the same pre-trained parameters with different hyperparameters leads to performance improvements over choosing the best individual fine-tuned model. A number of methods have since been proposed that use weight-averaging of models fine-tuned from the same pretrained model for diverse purposes (e.g. \cite{rame2022diverse, ilharco2022patching}). {Our results in \cref{sec:lmc} complement the findings of \cite{neyshabur2020being} by investigating whether fine-tuning from a pre-trained model leads to better mode connectivity because the gradients of a pre-trained model remain more stable than those trained from a random initialization.}

\textbf{Weight averaging after permutation matching.} Most recently, a growing number of papers have investigated whether attempts to merge models through weight-averaging can be improved by first performing some kind of permutation matching that corrects for potential permutation symmetries in neural networks. \cite{entezari2021role} conjecture that all solutions learned by SGD are linearly mode connectible once permutation symmetries are corrected for. \cite{singh2020model, ainsworth2022git, benzing2022random} use different methods for permutation matching and find that this improves the quality of weight-averaged models. 

\section{Additional theoretical results}\label{app:theory}
\subsection{Derivation of smoother expressions using the telescoping model}\label{app:theory-smooth}
Below, we explore how we can use the telescoping model to express a function learned by a neural network as $\tilde{f}_{\bm{\theta}_t}(\mathbf{x})=\mathbf{s}_{\bm{\theta}_t}(\mathbf{x})\mathbf{y} + c^0_{\bm{\theta}_t}(\mathbf{x})$, where the $1\!\times\! n$ vector $\mathbf{s}_{\bm{\theta}_t}(\mathbf{x})$ is a function of the kernels $\{K^t_{t'}(\cdot, \cdot)\}_{t'\leq t}$, and the scalar $c^0_{\bm{\theta}_t}(\mathbf{x})$ is a function of the  $\{K^t_{t'}(\cdot, \cdot)\}_{t'\leq t}$ and the networks' initialization $f_{\bm{\theta}_0}(\cdot)$. Note that, as discussed further in the remark at the end of this section, the kernels $K^t_{t'}(\cdot, \cdot)$ for $t>1$ are \textit{data-adaptive} as they can change throughout training.


\textbf{Vanilla SGD.} Recall that letting $\mathbf{y}=[y_1, \ldots, y_n]^\top$ and $\mathbf{f}_{\bm{\theta}_{t}}=[f_{\bm{\theta}_{t}}(\mathbf{x}_1), \ldots, f_{\bm{\theta}_{t}}(\mathbf{x}_n)]$, the SGD weight update with squared loss  $\ell(f(\mathbf{x}), y) = \frac{1}{2}(y - f(\mathbf{x}))^2$, in the special case of single outputs $k=1$, simplifies to $\Delta \bm{\theta}_t = \gamma_t \mathbf{T}_t (\mathbf{y} - \mathbf{f}_{\bm{\theta}_{t-1}})$, where $\mathbf{T}_t$ is the $p\times n$ matrix $\mathbf{T}_t=[\frac{\mathbf{1}\{1 \in B_t\}}{|B_t|} \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x}_1), \ldots, \frac{\mathbf{1}\{n \in B_t\}}{|B_t|}\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x}_n)]$. If we assume that the telescoping model holds exactly, this implies functional updates $
 \Delta \tilde{f}_t(\mathbf{x}) = \gamma_t \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top \mathbf{T}_t(\mathbf{y} - \tilde{\mathbf{f}}_{\bm{\theta}_{t-1}})$. If we could write $\tilde{\mathbf{f}}_{\bm{\theta}_{t-1}}=\mathbf{S}_{\bm{\theta}_{t-1}}\mathbf{y} + \mathbf{c}_{\bm{\theta}_{t-1}}$, then we would have
 \begin{equation}\label{eq:smootherupdate}
 \begin{split}
 \Delta \tilde{\mathbf{f}}_t(\mathbf{x}) = \gamma_t \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top \mathbf{T}_t(\mathbf{y} - (\mathbf{S}_{\bm{\theta}_{t-1}}\mathbf{y} + \mathbf{c}_{\bm{\theta}_{t-1}}))\\ = \gamma_t \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top \mathbf{T}_t (\mathbf{I}_n - \mathbf{S}_{\bm{\theta}_{t-1}})\mathbf{y} - \gamma_t \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top \mathbf{T}_t \mathbf{c}_{\bm{\theta}_{t-1}}
 \end{split}
 \end{equation}
 where $\mathbf{I}_n$ is the $n \times n$ identity matrix. Noting that we must have $\mathbf{c}_{\bm{\theta}_{0}}=\mathbf{f}_{\bm{\theta}_0}$ and $\mathbf{S}_{\bm{\theta}_{0}}=\mathbf{0}^{n\times n}$ at initialization, we can recursively substitute \cref{eq:smootherupdate} into \cref{eq:modeldef} which then allows to write the vector of training predictions as
\begin{equation}\label{eq:smoothertrain}
\begin{split}
    \tilde{\mathbf{f}}_{\bm{\theta}_T} = \underbrace{\left(\sum^T_{t=1} \left(\prod^{T-t}_{k=1}(\mathbf{I}_n \!\!-\! \gamma_{t+k}\bar{\mathbf{T}}_{t+k}^\top\mathbf{T}_{t+k})\right)\gamma_t\bar{\mathbf{T}}_{t}^\top\mathbf{T}_{t}\right)\mathbf{y}}_{\mathbf{S}_{\bm{\theta}_T}\mathbf{y}}\\ + \underbrace{\left(\prod^{T-1}_{k=0}(\mathbf{I}_n \!\!-\! \gamma_{T-k}\bar{\mathbf{T}}_{T-k}^\top\mathbf{T}_{T-k})\right)\mathbf{f}_{\bm{\theta}_0}}_{\mathbf{c}_{\bm{\theta}_T}}
    \end{split}
\end{equation}
 where the $p\times n$ matrix $\bar{\mathbf{T}}_t=[\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x}_1), \ldots, \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x}_n)]$ differs from ${\mathbf{T}}_t$ only in that it includes \textit{all} training examples and is not normalized by batch size. Then note that \cref{eq:smoothertrain} is indeed
a function of the training labels $\mathbf{y}$, the predictions at initialization $\mathbf{f}_{\bm{\theta}_0}$ and the model gradients $\{\bar{\mathbf{T}}_t\}^T_{t=1}$ traversed during training (captured in the $n \times n$ matrix $\mathbf{S}_{\bm{\theta}_T}$ and the $n \times 1$ vector $\mathbf{c}_{\bm{\theta}_T}$)  \textit{alone}. Similarly, we can also write the weight updates (and, by extension, the weights $\bm{\theta}_T$) using the same quantities, i.e. $\Delta \bm{\theta}_t = \gamma_t \mathbf{T}_t (\mathbf{I}_n - \mathbf{S}_{\bm{\theta}_{t-1}})\mathbf{y} - \gamma_t \mathbf{T}_t \mathbf{c}_{\bm{\theta}_{t-1}}$. By \cref{eq:modeldef}, this also implies that we can write predictions at \textit{arbitrary} test input points as a function of the same quantities:
\begin{equation*}
    \tilde{f}_{\bm{\theta}_T}(\mathbf{x}) = \underbrace{\left(\sum^T_{t=1} \gamma_t \nabla_{\bm{\theta}}  f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top  \mathbf{T}_t (\mathbf{I}_n - \mathbf{S}_{\bm{\theta}_{t-1}})\right)\mathbf{y}}_{\mathbf{s}_{\bm{\theta}_T}(\mathbf{x})\mathbf{y}} + \underbrace{\left( f_{\bm{\theta}_0}(\mathbf{x}) - \sum^T_{t=1} \gamma_t \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top   \mathbf{T}_t \mathbf{c}_{\bm{\theta}_{t-1}}\right)}_{c_{\bm{\theta}_T}(\mathbf{x})}
\end{equation*}
where the matrix  $\mathbf{S}_{\bm{\theta}_{t-1}}$ is as defined in \cref{eq:smoothertrain}, which indeed has  $\mathbf{s}_{\bm{\theta}_{t-1}}(\mathbf{x}_i)$ as its $i$-th row (and analogously for $\mathbf{c}_{\bm{\theta}_{t-1}}$). 

\textbf{General optimization strategies.} Adapting the previous expressions to enable the use of adaptive learning rates is straightforward and requires only inserting $\text{diag}(\frac{1}{\bm{\phi}_t})\mathbf{T}_t$ into the expression for $\Delta \tilde{f}_t(\mathbf{x})$ instead of $\mathbf{T}_t$ alone; then defining the matrices similarly proceeds by recursively unraveling updates using
 $\Delta \tilde{f}_t(\mathbf{x}) = \gamma_t \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top \text{diag}(\frac{1}{\bm{\phi}_t}) \mathbf{T}_t(\mathbf{y} - \tilde{\mathbf{f}}_{\bm{\theta}_{t-1}})$. Both momentum and weight decay lead to somewhat more tedious updates and necessitate the introduction of additional notation. 
Let $\Delta \mathbf{s}_t(\mathbf{x}) = \mathbf{s}_{\bm{\theta}_{t}}(\mathbf{x}) -  \mathbf{s}_{{\bm{\theta}_{t-1}}}(\mathbf{x})$, with $\mathbf{s}_{\bm{\theta}_{0}}(\mathbf{x}) = \mathbf{0}^{1\times n}$ and $\Delta {c}_t(\mathbf{x}) = {c}_{{\bm{\theta}_{t}}}(\mathbf{x}) -  {c}_{{\bm{\theta}_{t-1}}}(\mathbf{x})$, with ${c}_{{\bm{\theta}_{0}}}(\mathbf{x})=f_{\bm{\theta}_0}(\mathbf{x})$, so that $\mathbf{s}_{\bm{\theta}_{T}}(\mathbf{x}) = \sum^T_{t=1} \Delta \mathbf{s}_t(\mathbf{x})$ and $\mathbf{c}_{\bm{\theta}_{T}}(\mathbf{x}) = f_{\bm{\theta}_0}(\mathbf{x}) + \sum^T_{t=1} \Delta \mathbf{c}_t(\mathbf{x})$. Further, we can write 
\begin{equation}\Delta \tilde{f}_t(\mathbf{x})= \Delta \mathbf{s}_t(\mathbf{x}) \mathbf{y} +  \mathbf{c}_t(\mathbf{x})=\gamma_t \nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top\mathbf{U}^{S}_t \mathbf{y} + \gamma_t\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top\mathbf{U}^{C}_t
\end{equation}
which means that to derive $\mathbf{s}_{\bm{\theta}_t}(\mathbf{x})$ for each $t$, we can use the weight update formulas to define the $p \times n$ update matrix $\mathbf{U}^{S}_t$ and the $p \times 1$ update vector $\mathbf{U}^{C}_t$ that can then be used to compute $\Delta \mathbf{s}_t(\mathbf{x})$ as $\gamma_t\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top\mathbf{U}^{S}_t$ and  $\Delta \mathbf{c}_t(\mathbf{x})$ as $\gamma_t\nabla_{\bm{\theta}} f_{\bm{\theta}_{t-1}}(\mathbf{x})^\top\mathbf{U}^{C}_t$. For vanilla SGD,
\begin{equation}
    \mathbf{U}^{S}_t=\mathbf{T}_t (\mathbf{I}_n - \mathbf{S}_{\bm{\theta}_{t-1}}) \text{ and } \mathbf{U}^{C}_t= -\mathbf{T}_t \mathbf{c}_{\bm{\theta}_{t-1}}
\end{equation}
while SGD with only adaptive learning rates uses 
\begin{equation}
    \mathbf{U}^{S}_t=\text{diag}(\frac{1}{{\bm{\phi}_t}})\mathbf{T}_t (\mathbf{I}_n - \mathbf{S}_{\bm{\theta}_{t-1}}) \text{ and } \mathbf{U}^{C}_t= -\text{diag}(\frac{1}{{\bm{\phi}_t}})\mathbf{T}_t \mathbf{c}_{\bm{\theta}_{t-1}}
\end{equation}
Momentum, without other modifications, uses $\mathbf{U}^{S}_t = \frac{1}{1-\beta_1^t} \tilde{\mathbf{U}^{S}_t}$ and $\mathbf{C}^{S}_t = \frac{1}{1-\beta_1^t} \tilde{\mathbf{U}^{C}_t}$, where
\begin{equation}\label{eq:smoother-mom}
        \tilde{\mathbf{U}}^{S}_t=(1-\beta_1)\mathbf{T}_t (\mathbf{I}_n - \mathbf{S}_{\bm{\theta}_{t-1}}) + \beta_1  \tilde{\mathbf{U}}^{S}_{t-1} \text{ and } \tilde{\mathbf{U}^{C}_t}= -((1-\beta_1)\mathbf{T}_t \mathbf{c}_{\bm{\theta}_{t-1}} + \beta_1 \tilde{\mathbf{U}^{C}_{t-1}})
\end{equation}
with $\tilde{\mathbf{U}}^{S}_0= \mathbf{0}^{p \times n}$ and $\tilde{\mathbf{U}}^{S}_0= \mathbf{0}^{p \times 1}$.

Weight decay, without other modifications, uses
\begin{equation}\label{eq:smootherwd}
    \mathbf{U}^{S}_t=\mathbf{T}_t (\mathbf{I}_n - \mathbf{S}_{\bm{\theta}_{t-1}} + \lambda \mathbf{D}^{S}_t) \text{ and } \mathbf{U}^{C}_t= -\mathbf{T}_t (\mathbf{c}_{\bm{\theta}_{t-1}} + \lambda \mathbf{D}^{C}_t)
\end{equation}
where $\mathbf{D}^{S}_t= \gamma_{t-1} \mathbf{U}^{S}_{t-1} + (1-\lambda\gamma_{t-1})  \mathbf{D}^{S}_{t-1}$ and $\mathbf{D}^{C}_t= \gamma_{t-1} \mathbf{U}^{C}_{t-1} + (1-\lambda\gamma_{t-1})  \mathbf{D}^{C}_{t-1}$ with $\mathbf{D}^{S}_0= \mathbf{0}^{p \times n}$ and $\mathbf{D}^{C}_0=\bm{\theta}_0$.

Putting all together leads to AdamW \cite{loshchilov2017decoupled} (which decouples weight decay and momentum, so that weight decay does not enter the momentum term), which uses 
\begin{equation}
    \mathbf{U}^{S}_t = \text{diag}(\frac{1}{{\bm{\phi}_t}})\frac{1}{1-\beta_1^t} \tilde{\mathbf{U}^{S}_t} + \lambda \mathbf{T}_t \mathbf{D}^{S}_t \text{ and } \mathbf{C}^{S}_t = \frac{1}{1-\beta_1^t} \text{diag}(\frac{1}{{\bm{\phi}_t}})\tilde{\mathbf{U}^{C}_t} +  \lambda \mathbf{T}_t \mathbf{D}^{C}_t
\end{equation}
where all terms are as in \cref{eq:smoother-mom} and \cref{eq:smootherwd}.

\textit{Remark:} Writing $\tilde{\mathbf{f}}_{\bm{\theta}_T}  = \mathbf{S}_{\bm{\theta}_T}\mathbf{y} + \mathbf{c}_{\bm{\theta}_T}$ is reminiscent of a \textit{smoother} as used in the statistics literature \cite{hastie1990GAM}. Prototypical smoothers issue predictions $\hat{\mathbf{y}}=\mathbf{S}\mathbf{y}$ -- which include k-Nearest Neighbor regressors, kernel smoother, and (local) linear regression as prominent members --, and are usually \textit{linear smoothers} because $\mathbf{S}$ does not depend on $\mathbf{y}$. The smoother implied by the telescoping model is \textit{not} necessarily a linear smoother because $\mathbf{S}_{\bm{\theta}_T}$ can depend on $\mathbf{y}$ through changes in gradients during training, making $\tilde{\mathbf{f}}_{\bm{\theta}_T}$ an \textit{adaptive} smoother. This adaptivity in the implied smoother is similar to trees as recently studied in \cite{curth2023u, curth2024random}. In this context, effective parameters as measured by $p^{0}_{\mathbf{s}}$ can be interpreted as measuring how non-uniform and extreme the learned smoother weights are when issuing predictions for specific inputs \cite{curth2023u}.


\subsection{Comparing predictions of ensemble and weight-averaged model after train-time transition into a constant-gradient regime}\label{app:theory-lmc}
Here, we compare the predictions of the weight-averaged model $f_{\alpha\bm{\theta}^{t'}_{1T} + (1-\alpha){\bm{\theta}^{t'}_{2T}}}(\mathbf{x})$ to the ensemble $\bar{f}^\alpha(\mathbf{x})=\alpha f_{\alpha\bm{\theta}^{t'}_{1T}}(\mathbf{x}) + (1-\alpha)f_{\alpha\bm{\theta}^{t'}_{2T}}(\mathbf{x})$ if the models transition into a lazy regime at time $t^*\leq t'$. 

We begin by noting that the assumption that the gradients no longer change after $t^*$ (i.e. $\nabla_{\bm{\theta}} f_{\bm{\theta}^{t'}_{jt}}(\cdot) \approx \nabla_{\bm{\theta}} f_{\bm{\theta}_{t^*}}(\cdot)$ for all $t \geq t^*$) implies that the rate of change of $\nabla_{\bm{\theta}}f_{\bm{\theta}_{t^*}}(\mathbf{x})$ in the direction of the weight updates must be approximately $\mathbf{0}$. That is, $\nabla^2_{\bm{\theta}} f_{\bm{\theta}_{t^*}}(\mathbf{x}) (\bm{\theta}-\bm{\theta}_{t^*}) \approx \mathbf{0}$ for all $\bm{\theta} \in {\Theta}_j^{stable}$, or equivalently all weight changes in each ${\Theta}^{stable}_j$ are in directions that are in the null-space of the Hessian (or in directions corresponding to diminishingly small eigenvalues). To avoid clutter in notation, we use splitting point $t'=t^*$ below, but note that the same arguments hold for $t'>t^*$. 

First, we now consider rewriting the predictions of the ensemble, and note that we can now write the \textit{second-order} Taylor approximation of each model $f_{\bm{\theta}^{t^*}_{jT}}(\mathbf{x})$ around $\bm{\theta}_{t^*}$ as
\begin{equation*}
\begin{split}
    f_{\bm{\theta}^{t^*}_{jT}}(\mathbf{x}) = f_{\bm{\theta}^{t^*}}(\mathbf{x}) + \nabla_{\bm{\theta}} f_{\bm{\theta}_{t^*}}(\mathbf{x})^\top \sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{jt} + \underbrace{\frac{1}{2} \left[\sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{jt}\right]^\top\nabla^2_{\bm{\theta}}f_{\bm{\theta}_{t^*}}(\mathbf{x}) \left[\sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{jt}\right]}_{\approx 0} \\+ R_2(\sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{jt})\\
   \approx f_{\bm{\theta}^{t^*}}(\mathbf{x}) + \nabla_{\bm{\theta}} f_{\bm{\theta}_{t^*}}(\mathbf{x})^\top \sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{jt}  + R_2(\sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{jt})
   %\approx f_{\bm{\theta}^{t^*}}(\mathbf{x}) + \nabla_{\bm{\theta}} f_{\bm{\theta}_{t^*}}(\mathbf{x})^\top \sum^T_{t=t^*+1}\Delta \bm{\theta}_{tj} 
    \end{split}
\end{equation*}
where $R_2(\sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{jt})$ contains remainders of order 3 and above. Then the prediction of the ensemble can be written as 
\begin{equation}
\begin{split}
    \bar{f}^\alpha(\mathbf{x}) %\approx\alpha(f_{\bm{\theta}^{t^*}}(\mathbf{x}) + f_{\bm{\theta}_{t^*}}(\mathbf{x})^\top \sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{1t}) + (1-\alpha)(f_{\bm{\theta}^{t^*}}(\mathbf{x}) + f_{\bm{\theta}_{t^*}}(\mathbf{x})^\top \sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{2t}) \\+ \alpha R_2(\sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{1t}) + (1-\alpha )R_2(\sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{2t}) \\
    \approx f_{\bm{\theta}_{t^*}}(\mathbf{x})+ f_{\bm{\theta}_{t^*}}(\mathbf{x})^\top \sum^T_{t=t^*+1}(\alpha \Delta \bm{\theta}^{t^*}_{1t}+ (1-\alpha) \Delta \bm{\theta}^{t^*}_{2t}) \\+ \alpha R_2(\sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{1t}) + (1-\alpha )R_2(\sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{2t}))
\end{split}
\end{equation}

Now consider the weight-averaged model $f_{\alpha\bm{\theta}^{t'}_{1T} + (1-\alpha){\bm{\theta}^{t'}_{2T}}}(\mathbf{x})$. Note that we can always write $\bm{\theta}^{t^*}_{jT}=\bm{\theta}_0+\sum^T_{t=1}\Delta \bm{\theta}^{t^*}_{jt} =\bm{\theta}_{t^*}+\sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{jt}$ and thus $\alpha\bm{\theta}^{t^*}_{1T} + (1-\alpha){\bm{\theta}^{t^*}_{2T}} = \bm{\theta}_{t^*}+\sum^T_{t=t^*+1}\left(\alpha \Delta \bm{\theta}^{t^*}_{1t}+ (1-\alpha) \Delta \bm{\theta}^{t^*}_{2t}\right)$. Further, because $\nabla^2_{\bm{\theta}} f_{\bm{\theta}_{t^*}}(\mathbf{x}) \sum^T_{t=t^*+1}\Delta \bm{\theta}^{t^*}_{tj}\approx \mathbf{0}$ for each $j\in\{0, 1\}$, we also have that 
\begin{equation}
    \nabla^2_{\bm{\theta}} f_{\bm{\theta}_{t^*}}(\mathbf{x}) \left(\sum^T_{t=t^*+1}\alpha\Delta \bm{\theta}_{t1}+ (1-\alpha) \Delta \bm{\theta}_{t2}\right) \approx \alpha\mathbf{0} +(1-\alpha)\mathbf{0} = \mathbf{0}
\end{equation}
Then, the second-order Taylor approximation of  $f_{\alpha\bm{\theta}^{t'}_{1T} + (1-\alpha){\bm{\theta}^{t'}_{2T}}}(\mathbf{x})$ around $\bm{\theta}_{t^*}$ gives 
\begin{equation}
\begin{split}
    f_{\alpha\bm{\theta}^{t'}_{1T} + (1-\alpha){\bm{\theta}^{t'}_{2T}}}(\mathbf{x}) \approx f_{\bm{\theta}_{t^*}}(\mathbf{x}) + \nabla_{\bm{\theta}} f_{\bm{\theta}_{t^*}}(\mathbf{x})^\top \sum^T_{t=t^*+1}\left(\alpha \Delta \bm{\theta}_{t1}+ (1-\alpha) \Delta \bm{\theta}_{t2}\right) \\+ R_2(\sum^T_{t=t^*+1}\Delta \bm{\theta}_{t1}+ (1-\alpha) \Delta \bm{\theta}_{t2})
    \end{split}
\end{equation}

Thus, $f_{\alpha\bm{\theta}^{t'}_{1T} + (1-\alpha){\bm{\theta}^{t'}_{2T}}}(\mathbf{x}) \approx   \bar{f}^\alpha(\mathbf{x})$ up to remainder terms of third order and above. 


\section{Additional Experimental details}\label{app:exp}
In this section, we provide a complete description of the experimental details throughout this work. Code is provided at \url{https://github.com/alanjeffares/telescoping-lens}. Each section also reports their respective required compute which was performed on either Azure VMs powered by 4 $\times$ NVIDIA A100 GPUs or an NVIDIA RTX A4000 GPU.


\subsection{Case study 1 (\cref{sec:complexity}) and approximation quality experiment (\cref{sec:setup-model}, \cref{fig:approx})}

\paragraph{Double descent experiments.} In \cref{fig:dd}, we replicate \cite[Sec. S.3.3]{belkin2019reconciling}'s only binary classification experiment which used fully connected ReLU networks with a single hidden layer trained using the squared loss, \textit{without} sigmoid activation, on cat and dog images from CIFAR-10 \cite{krizhevsky2009learning}. Like \cite{belkin2019reconciling}, we grayscale and downsize images to $d=8\times8$ format and use $n=1000$ training examples and use SGD with momentum $\beta_1=0.95$. We use batch size $100$ (resulting in $B=10$ batches), learning rate $\gamma=0.0025$, and test on $n_{test}=1000$ held out examples. We train for up to $e=30000$ epochs, but stop when training accuracy reaches $100\%$ or when the training squared loss does not improve by more than $10^{-4}$ for 500 consecutive epochs (the former strategy was also employed in \cite{belkin2019reconciling}, we additionally employ the latter to detect converged networks). We report results using $\{1, 2, 5, 7, 10, 15, 20, 25, 30,35,40,45,50,55,70,85,100, 200,500,1000,2000, 5000\}$ hidden units. We repeat the experiment for 4 random seeds and report mean and standard errors in all figures.

In \cref{app:res-complex}, we additionally repeat this experiment with the same hyperparameters using MNIST images \cite{lecun1998gradient}. To create a binary classification task, we similarly train the model to distinguish 3-vs-5 from $n=1000$ images downsampled to $d=8\times 8$ format and test on $1000$ examples. Likely because the task is very simple, we observe no deterioration in test error in this setting for any hidden size (see \cref{fig:dd-mnist}). Because \cite{nakkiran2021deep} found that double descent can be more apparent in the presence of label noise, we repeat this experiment while adding $20\%$ label noise to the training data, in which case the double descent shape in test error indeed emerges. As above,  we repeat both experiments for 4 random seeds and report mean and standard errors in all figures.

Further, in \cref{app:res-complex} we additionally utilize the MNIST-1D dataset \cite{greydanus2020scaling} which was proposed recently as a sandbox for investigating empirical deep learning phenomena. We replicate a binary classification version of their MLP double descent experiment with added 15\% label noise from \cite{greydanus2020scaling} (which was itself adapted from the textbook \cite{prince2023understanding}). We select only examples with label 0 and 1, and train fully connected neural networks with a single hidden layer with batch size 100, learning rate $\gamma=0.01$ for 500 epochs, considering models with $[1, 2, 3, 5, 10, 20, 30, 40, 50, 70, 100, 200, 300, 400]$ hidden units.

Compute: We train $\texttt{num\_settings} \times \texttt{num\_hidden\_sizes} \times \texttt{num\_seeds}$ $(\approx 4 \times 22 \times 4 = 352)$ models for up to $T=B \times e = 300000$ gradient steps. Training times, which included all gradient computations to create the telescoping approximation, depended on the dataset and hidden sizes, but completing a single seed for all hidden sizes for one setting took an average of 36 hours. 
 

\paragraph{Grokking experiments.} In panel (1) of \cref{fig:res-grok}, we replicate the polynomial regression experiment from \cite[Sec. 5]{kumargrokking} exactly. \cite{kumargrokking} use a neural network with a single hidden layer, using custom nonlinearities, of width $n_h=500$ in which the weights of the final layer are fixed, that is they use
\begin{equation}
    f_{\bm{\theta}}(\mathbf{x})=\frac{1}{n_h} \sum^{n_h}_{j=1} \phi(\bm{\theta}_j^\top \mathbf{x}) \text{ where } \phi(h)=h+\frac{\epsilon}{2}h^2
\end{equation}
Inputs $x\in R^d$ are sampled from an isotropic Gaussian with variance $\frac{1}{d}$ and targets $y$ are generated as $y(\mathbf{x})=\frac{1}{2}(\bm{\beta}^\top \mathbf{x})^2$. In this setup, $\epsilon$ used in the activation function of the network controls how easy it is to fit the outcome function (the larger $\epsilon$, the better aligned it is for the task at hand), which in turn controls whether grokking appears. In the main text, we present results using $\epsilon=.2$; in \cref{app:res-complex} we additionally present results using $\epsilon=.05$ and $\epsilon=0.5$. Like \cite{kumargrokking}, we use $d=100$, $n_{train}=550$, $n_{test}=500$, initialize all weights using standard normals, and train using full-batch gradient descent with $\gamma=B=500$ on the squared loss. We repeat the experiment for 5 random seeds and report mean and standard errors in all figures.

In panel (2) of \cref{fig:res-grok}, we report an adapted version of \cite{liu2022omnigrok}'s experiment reporting grokking on MNIST data. To enable the use of our model, we once more consider the binary classification task 3-vs-5 from $n=1000$ images downsampled to $d=8 \times 8$ features and test on 1000 held-out examples. Like \cite{liu2022omnigrok}, we use a 3-layer fully connected ReLU network trained with squared loss (\textit{without} sigmoid activation) and larger than usual initialization by using $\alpha\bm{\theta}_0$ instead of the default initialization $\bm{\theta}_0$. We report $\alpha=6$ in the main text and include results with $\alpha=5$ and $\alpha=7$ in \cref{app:res-complex}. Like \cite{liu2022omnigrok} we use the AdamW optimizer \cite{loshchilov2017decoupled} with batches of size $200$, $\beta_1=.9$ and $\beta_2=.99$, and use weight decay $\lambda=.1$. While \cite{liu2022omnigrok} use learning rate $10^{-3}$, we need to reduce this by factor 10 to $\gamma=10^{-4}$ and additionally use linear learning rate warmup over the first 100 batches to ensure that weight updates are small enough to ensure the quality of the telescoping approximation; this is particularly critical because of the large initialization which otherwise results in instability in the approximation early in training. Panel (C) of \cref{fig:res-grok} uses an identical setup but lets $\alpha=1$ (i.e. standard initialization) and additionally applies a sigmoid to the output of the network. We repeat these experiments for 4 random seeds and report mean and standard errors in all figures.

Compute: Replicating \cite{kumargrokking}'s experiments required training  $\texttt{num\_settings} \times  \texttt{num\_seeds}$ $(3 \times 5 = 15)$ models for $T=100,000$ gradient steps. Each training run including all gradient computations took less than 1 hour to complete. Replicating \cite{liu2022omnigrok}'s experiments required training  $\texttt{num\_settings} \times  \texttt{num\_seeds}$ $(3 \times 4 = 12)$ for $T=100,000$ gradient steps. Each training run including all gradient computations took around 5 hours to complete. The MNIST experiments with standard initialization required training  $\texttt{num\_settings} \times  \texttt{num\_seeds}$ $(2 \times 4 = 8)$ for $T=1000$ gradient steps, these took no more than 2 hours to complete in total.

\paragraph{Approximation quality experiment (\cref{fig:approx})} The approximation quality experiment uses the identical MNIST setup, training process and architecture as in the grokking experiments (differing only in that we use standard initialization $\alpha$ and no learning rate warmup). In addition to the vanilla SGD and AdamW experiments presented in the main text, we present additional settings --  using momentum alone, weight decay alone and using sigmoid activation -- in \cref{app:res-approx}. In particular, we use the following hyperparameter settings for the different panels:
\begin{itemize}
    \item \textit{``SGD'':} $\lambda=0$, $\beta_1=0$, no sigmoid.
     \item \textit{``AdamW'':} $\lambda=0.1$, $\beta_1=0.9$, $\beta_2=.99$, no sigmoid.
         \item \textit{``SGD + Momentum'':} $\lambda=0$, $\beta_1=0.9$, no sigmoid.
    \item \textit{``SGD + Weight decay'':} $\lambda=0.1$, $\beta_1=0$, no sigmoid.
\item \textit{``SGD + $\sigma(\cdot)$'':} $\lambda=0$, $\beta_1=0$, with sigmoid activation.
\end{itemize}
We repeat the experiment for 4 random seeds and report mean and standard errors in all figures.

Compute: Creating \cref{fig:approx-app} required training $\texttt{num\_settings} \times  \texttt{num\_seeds}$ $(5 \times 4 = 20)$ for $T=5,000$ gradient steps. Each training run including all gradient computations took approximately 15 minutes to complete.

\subsection{Case study 2 (\cref{sec:gradient-boosting})} In \cref{fig:GBT,fig:GBT-app} we provide results on tabular benchmark datasets from \cite{grinsztajn2022tree}. We select four datasets with > 20,000 examples (\texttt{houses}, \texttt{superconduct}, \texttt{california}, \texttt{house\_sales}) to ensure there is sufficient hold-out data for evaluation across irregularity proportions. We apply standard preprocessing including log transformations of skewed features and target rescaling. As discussed in the main text, irregular examples are defined by first projecting each (normalized) dataset's input features onto its first principal component and then calculating each example's absolute distance to the empirical median in this space. We note that several recent works have discussed metrics of an examples irregularity or ``hardness'' (e.g. \cite{kwok2024dataset, seedat2023dissecting}) finding the choice of metric to be highly context-dependent. Therefore we select a principal component prototypicality approach based on its simplicity and transparency. The top $K$ irregular examples are removed from the data (these form the ``irregular examples at test-time'') and the remainder (the ``regular examples'') is split into training and testing. We then construct test datasets containing 4000 examples, constructed from a mixture of standard test examples and irregular examples according to each proportion $p$. 

We train both a standard neural network (while computing its telescoping approximation as described in \cref{eq:modeldef}) and a gradient boosted tree model (using \cite{scikit-learn}) on the training data. We select hyperparameters by further splitting the training data to obtain a validation set of size 2000 and applying a random search consisting of 25 runs. We use the search spaces suggested in \cite{grinsztajn2022tree}. Specifically, for GBTs we consider \texttt{learning\_rate} $\in \text{LogNormal}[\log(0.01), \log(10)]$, \texttt{num\_estimators} $\in \text{LogUniformInt}[10.5, 1000.5]$, and \texttt{max\_depth} $\in [\text{None}, 2, 3, 4, 5]$ with respective probabilities $[0.1, 0.1, 0.6, 0.1, 0.1]$. For the neural network, we consider \texttt{learning\_rate} 
$\in \text{LogUniform}[1e-5, 1e-2]$ and set \texttt{batch\_size} $= 128$, \texttt{num\_layers} $= 3$, and \texttt{hidden\_dim} $= 64$ with ReLU activations throughout. Each model is then trained on the full training set with its optimal parameters and is evaluated on each of test sets corresponding to the various proportions of irregular examples. All models are trained and evaluated for 4 random seeds and we report the mean and a standard error in our results. 

As discussed in the main text, we report how the relative relative mean squared error of neural network and GBT (measured as $\frac{MSE^{p}_{NN} - MSE^{p}_{GBT}}{MSE^{0}_{NN} -MSE^{0}_{GBT}}$) changes as the proportion $p$ of irregular examples increases and relate this to changes in $\frac{\frac{1}{T} \sum^T_{t=1} \max_{j \in \mathcal{I}^p_{test}}||\mathbf{k}_t(x_j)||}{\frac{1}{T} \sum^T_{t=1}\max_{i \in \mathcal{I}_{train}}||\mathbf{k}_t(\mathbf{x}_i)||}$, which measures how the kernels behave at their extreme during testing relative to the maximum of the equivalent values measured for the training examples such that the test values can be interpreted relative to the kernel at train time (i.e. values > 1 can be interpreted as being larger than the largest value observed across the entire training set). %In our figures we report the maximum, 95th quantile and median of these values \hl{TODO: update this based on what we end up actually including}.

Compute: The hyperparameter search results in $\texttt{num\_searches} \times \texttt{num\_datasets} \times \texttt{num\_models}$ ($25 \times 4 \times 2 = 200$) training runs and evaluations. Then the main experiment requires $\texttt{num\_seeds} \times \texttt{num\_datasets} \times \texttt{num\_models}$ ($4 \times 4 \times 2 = 32$) training runs and $\texttt{num\_seeds} \times \texttt{num\_datasets} \times \texttt{num\_models} \times \texttt{num\_proportions}$ ($4 \times 4 \times 2 \times 5 = 160$) evaluations. This results in a total of 232 training runs and 360 evaluations. Individual training and evaluation times depend on the model and dataset but generally require < 1 hour.
 
\subsection{Case study 3 (\cref{sec:lmc})} In \cref{fig:LMC} we follow the experimental setup described in \cite{frankle2020linear}. Specifically, for each model we train for a total of 63,000 iterations over batches of size 128 with stochastic gradient descent. At a predetermined set of checkpoints ($t' \in [0, 4, 25, 50, 100, 224, 500, 1000, 2000, 4472, 10000, 25100]$) we create two copies of the current state of the network and train until completion with different batch orderings, where linear mode connectivity measurements are calculated. This process sometimes also referred to as \textit{spawning} \cite{fort2020deep} and is repeated for 3 seeds at each $t'$. The entire process is repeated for 3 seeds resulting in a total of $3 \times 3  = 9$ total values over which we report the mean and a standard error. Momentum is set to 0.9 and a stepwise learning rate is applied beginning at 0.1 and decreasing by a factor of 10 at iterations 32,000 and 48,000. For the ResNet-20 architecture \cite{he2016deep}, we use an implementation from \cite{Idelbayev18a}. Experiments are conducted on CIFAR-10 \cite{krizhevsky2009learning} where the inputs are normalized with random crops and random horizontal flips used as data augmentations. 

Pretraining of the finetuned model model is performed on the SVHN dataset \cite{netzer2011reading} which is also an image classification task with identically shaped input and output dimensions as CIFAR-10. We use a training setup similar to that of the CIFAR-10 model but set the number of training iterations to 30,000 and perform the stepwise decrease in learning rate at iterations 15,000 and 25,000 decaying by a factor of 5. Three models are trained following this protocol which achieve validation accuracy of 95.5\%, 95.5\%, and 95.4\% on SVHN. We then repeat the CIFAR-10 training protocol for finetuning but parameterize the three initialization with the respective pretrained weights rather than random initialization. We also find that a shorter finetuning period is sufficient and therefore finetune for 12,800 steps with the learning rate decaying by a factor of 5 at steps 6,400 and 9,600. 

Also following the protocol of \cite{frankle2020linear}, for each pair of trained spawned networks ($f_{\bm{\theta}_{1}} \& f_{\bm{\theta}_{2}}$) we consider interpolating their losses (i.e. $\ell^\text{avg}_\alpha \coloneq \alpha \cdot \ell(f_{\bm{\theta}_{1}}(\mathbf{x}),y)  + (1 - \alpha) \cdot \ell(f_{\bm{\theta}_{2}}(\mathbf{x}),y)$) and parameters (i.e. $\ell^\text{lmc}_\alpha \coloneq \ell(f_{\bm{\theta}^\text{lmc}}(\mathbf{x}),y)$ where $\bm{\theta}^\text{lmc} = \alpha \bm{\theta}_{1} + (1 - \alpha) \bm{\theta}_{2}$) for 30 equally spaced values of $\alpha \in [0,1]$. In the upper panel of \cref{fig:LMC} we plot the accuracy gap at each checkpoint $t^\prime$ (i.e. the point from which two identical copies of the model are made and independently trained to completion) which is simply defined as the average final validation accuracy of the two individual child models minus the final validation accuracy of the weight averaged version of these two child models. Beyond the original experiment, we also wish to evaluate how the gradients $\nabla f_{\bm{\theta}_t}(\cdot)$ evolve throughout training. Therefore, in panels (2) and (3) \cref{fig:LMC}, at each checkpoint $t'$ we also measure the mean squared change in (pre-softmax) gradients ${(\nabla_{\bm{\theta}} f_{\bm{\theta}_{t'+390}}(\mathbf{x})-\nabla_{\bm{\theta}} f_{\bm{\theta}_{t'}}(\mathbf{x}))^2}$  between the current iteration $t'$ and those at the next epoch $t'+390$, averaged over a set of $n = 256$ test examples and the parameters in each layer.

Compute: We train $\texttt{num\_outer\_seeds} \times \texttt{num\_inner\_seeds} \times \texttt{num\_child\_models} \times \texttt{num\_checkpoints}$ ($3 \times 3 \times 2 \times 12 = 216$) networks for the randomly initialized model. For the finetuned model this results in $3 \times 3 \times 2 \times 10 = 180$ training runs. Additionally, we require the pertaining of the 3 base models on SVHN. Combined this results in a total of $216 + 180 + 3 = 399$ training runs. Training each ResNet-20 on CIFAR-10 required <1 hour including additional gradient computations. 
% This results in an upper bound estimate of $216 + 180 + 3 = 399$ computational hours.

\subsection{Data licenses}\label{sec:app-licenses}
All image experiments are performed on CIFAR-10 \cite{krizhevsky2009learning}, MNIST \cite{lecun1998gradient}, MNIST1D \cite{greydanus2020scaling}, or SVHN \cite{netzer2011reading}. Tabular experiments are run on \texttt{houses}, \texttt{superconduct}, \texttt{california}, and \texttt{house\_sales} from OpenML \cite{OpenML2013} as described in \cite{grinsztajn2022tree}. CIFAR-10 is released with an MIT license. MNIST is released with a Creative Commons Attribution-Share Alike 3.0 license. MNIST1D is released with an Apache-2.0 license. SVHN is released with a CC0:Public Domain license. OpenML datasets are released with a 3-Clause BSD License. All the datasets used in this work are publicly available. 



\section{Additional results}\label{app:res}
\subsection{Additional results on approximation quality (supplementing \cref{fig:approx})}\label{app:res-approx}
\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{figures/app/approx_app.png}
    \caption{\textbf{Approximation error} of the telescoping ($\tilde{f}_{\bm{\theta}_t}(\mathbf{x})$, \textcolor{sharedred}{red}) and the model linearized around the initialization (${f}^{lin}_{\bm{\theta}_t}(\mathbf{x})$, \textcolor{sharedgray}{gray}) by optimization step for different optimization strategies and other design choices. Iteratively telescoping out the updates using $\tilde{f}_{\bm{\theta}_t}(\mathbf{x})$ improves upon the lazy approximation around the initialization by orders of magnitude.  }
    \label{fig:approx-app}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{figures/app/approx_acc_app.png}
    \caption{\textbf{Test accuracy} of the telescoping ($\tilde{f}_{\bm{\theta}_t}(\mathbf{x})$, \textcolor{sharedred}{red}, top row) and the model linearized around the initialization (${f}^{lin}_{\bm{\theta}_t}(\mathbf{x})$, \textcolor{sharedblue}{blue}, bottom row) against accuracy of the actual neural network (\textcolor{sharedgray}{gray}) by optimization step for different optimization strategies and other design choices.  While the telescoping model visibly matches the accuracy of the actual neural network, the linear approximation around the initialization leads to substantial differences in accuracy later in training. }
    \label{fig:approx-acc}
\end{figure}

In \cref{fig:approx-app}, we present results investigating the evolution of approximation errors of the telescoping and linear approximation around the initialization during training using additional configurations compared to the results presented in \cref{fig:approx} in the main text (replicated in the first two columns of \cref{fig:approx-app}). We observe the same trends as in the main text, where the telescoping approximation matches the predictions by the neural network by orders of magnitudes better than the linear approximation around the initialization. Importantly, we highlight in \cref{fig:approx-acc} that this is also reflected in how well each approximation matches the accuracy of the predictions of the real neural network: while the small errors of the telescoping model lead to no visible differences in accuracy compared to the real neural network, using the Taylor expansion around the initialization leads to significantly different accuracy later in training. 

\subsection{Additional results for case study 1: Exploring surprising generalization curves and benign overfitting}\label{app:res-complex}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/app/double_descent_app.png}
    \caption{\textbf{Double descent} experiments using MNIST, distinguishing 3-vs-5, with 20\% added label noise during training (left) and no added label noise (right). Without label noise, there is no double descent in error on this task; when label noise is added we observe the prototypical double descent shape in test error. }
    \label{fig:dd-mnist}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{figures/app/rebuttal_double_descent_app.png}
    \caption{\textbf{Double descent} experiment using MNIST-1D, distinguishing class 0 and 1, with 15\% added label noise during training. Mean squared error (top) and effective parameters (bottom) for train and test examples by number of hidden neurons.}
    \label{fig:dd-mnist-1d}
\end{figure}

\paragraph{Double descent on MNIST.} In \cref{fig:dd-mnist}, we replicate the CIFAR-10 experiment from the main text while training models to distinguish 3-vs-5 on MNIST. We find that in the absence of label noise, no problematic overfitting occurs for any hidden size; both train and test error monotonically improve with increased width. Only when we add label noise to the training data, do we observe the characteristic double descent behavior in error -- this is in line with \cite{nakkiran2021deep}'s observation that double descent can be more pronounced when there is noise in the data. Importantly, we observe that as in the main text, the improvement of test error past the interpolation threshold is associated with the divergence of effective parameters used on train and test data. In \cref{fig:dd-mnist-1d} we additionally repeat the experiment using the MNIST-1D dataset with 15\% labelnoise as in \cite{greydanus2020scaling}, and find that the decrease in test error after the interpolation threshold is again accompanied by a \textit{decrease in effective parameters} as the number of raw model parameters is further \textit{increased} in the interpolation regime.

\paragraph{Additional grokking results.} In \cref{fig:complex-poly}, we replicate the polynomial grokking results of \cite{kumargrokking} with additional values of $\epsilon$. Like \cite{kumargrokking}, we observe that larger values of $\epsilon=0.5$ lead to less delayed generalization. This is reflected in a gap between effective parameters on test and train emerging earlier. With very small $\epsilon=.05$, conversely, we even observe a double descent-like phenomenon where test error first worsens before it improves later in training. This is reflected also in the effective parameters, where $p^{test}_{\mathbf{\hat{s}}}$ first exceeds $p^{train}_{\mathbf{\hat{s}}}$ before dropping below it as benign overfitting sets in later in training. In \cref{fig:complex-mnist}, we replicate the MNIST results with additional values of $\alpha$; like \cite{liu2022omnigrok} we observe that grokking behavior is more extreme for larger $\alpha$. This is indeed also reflected in the gap between $p^{test}_{\mathbf{\hat{s}}}$ and $p^{train}_{\mathbf{\hat{s}}}$ emerging later in training.

\paragraph{Additional training results on MNIST with standard initialization.} In \cref{fig:complexity-sigmoid}, we present train and test results on MNIST with standard initialization to supplement the test results presented in the main text. Both with and without sigmoid, train and test behavior is almost identical, and learning is orders of magnitude faster than with the larger initialization. The stronger inductive biases of small initialization, and additionally using sigmoid activation, lead to much lower learned complexity on both train and test data as measured by effective parameters.  

\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{figures/app/complexity_poly_app.png}
    \caption{\textbf{Grokking} in mean squared error (top) on a polynomial regression task (replicated from \cite{kumargrokking}) against effective parameters (bottom) with different task alignment parameters $\epsilon$.}
    \label{fig:complex-poly}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{figures/app/complexity_mnist_app.png}
    \caption{\textbf{Grokking} in misclassification error on MNIST using a network with large initialization ( replicated from \cite{liu2022omnigrok}) (top), against effective parameters (bottom) with different initialization scales $\alpha$.}
    \label{fig:complex-mnist}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/app/complexity_sigmoid_app.png}
    \caption{\textbf{No grokking} in misclassification error on MNIST (top), against effective parameters (bottom) using a network with standard initialization ($\alpha=1$) with and without sigmoid activation.}
    \label{fig:complexity-sigmoid}
\end{figure}




\subsection{Additional results for Case study 2: Understanding differences between gradient boosting and neural networks}\label{app:res-gb}
\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{figures/app/GBT_app.png}
    \caption{\textbf{Neural Networks vs GBTs:} Relative performance (top) and behavior of kernels (bottom) with increasing test data irregularity for three additional datasets.}
    \label{fig:GBT-app}
\end{figure}

In \cref{fig:GBT-app}, we replicate the experiment from \cref{sec:gradient-boosting} on three further datasets from \cite{grinsztajn2022tree}'s tabular benchmark. We find that the results match the trends present in \cref{fig:GBT} in the main text: the neural network is outperformed by the GBTs already at baseline, and the performance gap grows as the test dataset becomes increasingly more irregular. The growth in the gap is tracked by the behavior of the normalized maximum kernel weight norm of the neural network's kernel. Only on the \texttt{california} dataset do we observe a slightly different behavior of the neural network's kernel: unlike the other three datasets, $\frac{\frac{1}{T} \sum^T_{t=1} \max_{j \in \mathcal{I}^p_{test}}||\mathbf{k}_t(x_j)||_2}{\frac{1}{T} \sum^T_{t=1}\max_{i \in \mathcal{I}_{train}}||\mathbf{k}_t(\mathbf{x}_i)||_2}$ stays substantially below 1 at all $p$; this indicates that there may have been examples in the training set that are irregular in ways not captured by our experimental protocol. Nonetheless, we observe the same trend that $\frac{\frac{1}{T} \sum^T_{t=1} \max_{j \in \mathcal{I}^p_{test}}||\mathbf{k}_t(x_j)||_2}{\frac{1}{T} \sum^T_{t=1}\max_{i \in \mathcal{I}_{train}}||\mathbf{k}_t(\mathbf{x}_i)||_2}$ increases in relative terms as $p$ increases.


\end{document}