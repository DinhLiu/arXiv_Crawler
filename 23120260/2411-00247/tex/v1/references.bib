@book{prince2023understanding,
  title={Understanding Deep Learning},
  author={Prince, Simon JD},
  year={2023},
  publisher={MIT press}
}

@article{guo2023stochastic,
  title={Stochastic weight averaging revisited},
  author={Guo, Hao and Jin, Jiyong and Liu, Bin},
  journal={Applied Sciences},
  volume={13},
  number={5},
  pages={2935},
  year={2023},
  publisher={MDPI}
}

@article{jain2018parallelizing,
  title={Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  journal={Journal of machine learning research},
  volume={18},
  number={223},
  pages={1--42},
  year={2018}
}

@article{yang2021taxonomizing,
  title={Taxonomizing local versus global structure in neural network loss landscapes},
  author={Yang, Yaoqing and Hodgkinson, Liam and Theisen, Ryan and Zou, Joe and Gonzalez, Joseph E and Ramchandran, Kannan and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18722--18733},
  year={2021}
}

@inproceedings{seleznova2022analyzing,
  title={Analyzing Finite Neural Networks: Can We Trust Neural Tangent Kernel Theory?},
  author={Seleznova, Mariia and Kutyniok, Gitta},
  booktitle={Mathematical and Scientific Machine Learning},
  pages={868--895},
  year={2022},
  organization={PMLR}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@article{entezari2021role,
  title={The role of permutation invariance in linear mode connectivity of neural networks},
  author={Entezari, Rahim and Sedghi, Hanie and Saukh, Olga and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2110.06296},
  year={2021}
}

@article{ainsworth2022git,
  title={Git re-basin: Merging models modulo permutation symmetries},
  author={Ainsworth, Samuel K and Hayase, Jonathan and Srinivasa, Siddhartha},
  journal={arXiv preprint arXiv:2209.04836},
  year={2022}
}

@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International Conference on Machine Learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}

@article{singh2020model,
  title={Model fusion via optimal transport},
  author={Singh, Sidak Pal and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22045--22055},
  year={2020}
}

@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{altintas2023disentangling,
  title={Disentangling Linear Mode-Connectivity},
  author={Altintas, Gul Sena and Bachmann, Gregor and Noci, Lorenzo and Hofmann, Thomas},
  journal={arXiv preprint arXiv:2312.09832},
  year={2023}
}

@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H},
  volume={2},
  year={2009},
  publisher={Springer}
}

@inproceedings{liu2022omnigrok,
  title={Omnigrok: Grokking beyond algorithmic data},
  author={Liu, Ziming and Michaud, Eric J and Tegmark, Max},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{liu2022towards,
  title={Towards understanding grokking: An effective theory of representation learning},
  author={Liu, Ziming and Kitouni, Ouail and Nolte, Niklas S and Michaud, Eric and Tegmark, Max and Williams, Mike},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34651--34663},
  year={2022}
}

@article{liu2019variance,
  title={On the variance of the adaptive learning rate and beyond},
  author={Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  journal={arXiv preprint arXiv:1908.03265},
  year={2019}
}

@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}

@article{curth2023u,
  title={A u-turn on double descent: Rethinking parameter counting in statistical learning},
  author={Curth, Alicia and Jeffares, Alan and van der Schaar, Mihaela},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{curth2024random,
  title={Why do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers},
  author={Curth, Alicia and Jeffares, Alan and van der Schaar, Mihaela},
  journal={arXiv preprint arXiv:2402.01502},
  year={2024}
}

# check out 
@article{golikov2022neural,
  title={Neural tangent kernel: A survey},
  author={Golikov, Eugene and Pokonechnyy, Eduard and Korviakov, Vladimir},
  journal={arXiv preprint arXiv:2208.13614},
  year={2022}
}

@article{bai2020taylorized,
  title={Taylorized training: Towards better approximation of neural network training at finite width},
  author={Bai, Yu and Krause, Ben and Wang, Huan and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:2002.04010},
  year={2020}
}

@article{vyas2022limitations,
  title={Limitations of the ntk for understanding generalization in deep learning},
  author={Vyas, Nikhil and Bansal, Yamini and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2206.10012},
  year={2022}
}

@article{lucas2021analyzing,
  title={Analyzing monotonic linear interpolation in neural network loss landscapes},
  author={Lucas, James and Bae, Juhan and Zhang, Michael R and Fort, Stanislav and Zemel, Richard and Grosse, Roger},
  journal={arXiv preprint arXiv:2104.11044},
  year={2021}
}

@article{qadeer2023efficient,
  title={Efficient kernel surrogates for neural network-based regression},
  author={Qadeer, Saad and Engel, Andrew and Tsou, Adam and Vargas, Max and Stinis, Panos and Chiang, Tony},
  journal={arXiv preprint arXiv:2310.18612},
  year={2023}
}

@article{long2021properties,
  title={Properties of the after kernel},
  author={Long, Philip M},
  journal={arXiv preprint arXiv:2105.10585},
  year={2021}
}

@article{fort2020deep,
  title={Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel},
  author={Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5850--5861},
  year={2020}
}

@article{beaglehole2024gradient,
  title={Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks},
  author={Beaglehole, Daniel and Mitliagkas, Ioannis and Agarwala, Atish},
  journal={arXiv preprint arXiv:2402.05271},
  year={2024}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{mcelfresh2024neural,
  title={When do neural nets outperform boosted trees on tabular data?},
  author={McElfresh, Duncan and Khandagale, Sujay and Valverde, Jonathan and Prasad C, Vishak and Ramakrishnan, Ganesh and Goldblum, Micah and White, Colin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{grinsztajn2022tree,
  title={Why do tree-based models still outperform deep learning on typical tabular data?},
  author={Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={507--520},
  year={2022}
}

@inproceedings{qin2020neural,
  title={Are neural rankers still outperformed by gradient boosted decision trees?},
  author={Qin, Zhen and Yan, Le and Zhuang, Honglei and Tay, Yi and Pasumarthi, Rama Kumar and Wang, Xuanhui and Bendersky, Michael and Najork, Marc},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{lin2006random,
  title={Random forests and adaptive nearest neighbors},
  author={Lin, Yi and Jeon, Yongho},
  journal={Journal of the American Statistical Association},
  volume={101},
  number={474},
  pages={578--590},
  year={2006},
  publisher={Taylor \& Francis}
}

@article{biau2010layered,
  title={On the layered nearest neighbour estimate, the bagged nearest neighbour estimate and the random forest method in regression and classification},
  author={Biau, G{\'e}rard and Devroye, Luc},
  journal={Journal of Multivariate Analysis},
  volume={101},
  number={10},
  pages={2499--2518},
  year={2010},
  publisher={Elsevier}
}

@article{hastie1990GAM,
  title={Generalized additive models},
  author={Hastie, Trevor and Tibshirani, Robert},
  journal={Monographs on statistics and applied probability. Chapman \& Hall},
  volume={43},
  pages={335},
  year={1990}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{yunis2022convexity,
  title={On convexity and linear mode connectivity in neural networks},
  author={Yunis, David and Patel, Kumar Kshitij and Savarese, Pedro Henrique Pamplona and Vardi, Gal and Frankle, Jonathan and Walter, Matthew and Livescu, Karen and Maire, Michael},
  booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
  year={2022}
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}
@inproceedings{kumargrokking,
  title={Grokking as the transition from lazy to rich training dynamics},
  author={Kumar, Tanishq and Bordelon, Blake and Gershman, Samuel J and Pehlevan, Cengiz},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{belkin2021fit,
  title={Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation},
  author={Belkin, Mikhail},
  journal={Acta Numerica},
  volume={30},
  pages={203--248},
  year={2021},
  publisher={Cambridge University Press}
}

@inproceedings{mohamadi2023fast,
  title={A fast, well-founded approximation to the empirical neural tangent kernel},
  author={Mohamadi, Mohamad Amin and Bae, Wonho and Sutherland, Danica J},
  booktitle={International Conference on Machine Learning},
  pages={25061--25081},
  year={2023},
  organization={PMLR}
}

@inproceedings{seedat2023dissecting,
  title={Dissecting sample hardness: Fine-grained analysis of Hardness Characterization Methods},
  author={Seedat, Nabeel and Imrie, Fergus and van der Schaar, Mihaela},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{kwok2024dataset,
  title={Dataset Difficulty and the Role of Inductive Bias},
  author={Kwok, Devin and Anand, Nikhil and Frankle, Jonathan and Dziugaite, Gintare Karolina and Rolnick, David},
  journal={arXiv preprint arXiv:2401.01867},
  year={2024}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@article{elkabetz2021continuous,
  title={Continuous vs. discrete optimization of deep neural networks},
  author={Elkabetz, Omer and Cohen, Nadav},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4947--4960},
  year={2021}
}

@article{liu2020linearity,
  title={On the linearity of large non-linear models: when and why the tangent kernel is constant},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Misha},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15954--15964},
  year={2020}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@misc{Idelbayev18a,
  author       = "Yerlan Idelbayev",
  title        = "Proper {ResNet} Implementation for {CIFAR10/CIFAR100} in {PyTorch}",
  howpublished = "\url{https://github.com/akamaster/pytorch_resnet_cifar10}",
  note         = "Accessed: 2024-05-15"
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@book{roberts2022principles,
  title={The principles of deep learning theory},
  author={Roberts, Daniel A and Yaida, Sho and Hanin, Boris},
  volume={46},
  year={2022},
  publisher={Cambridge University Press Cambridge, MA, USA}
}

@article{de2021survey,
  title={A survey on deep matrix factorizations},
  author={De Handschutter, Pierre and Gillis, Nicolas and Siebert, Xavier},
  journal={Computer Science Review},
  volume={42},
  pages={100423},
  year={2021},
  publisher={Elsevier}
}

@article{kawaguchi2022generalization,
  title={Generalization in Deep Learning},
  author={Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  journal={In Mathematical Aspects of Deep Learning. Cambridge University Press},
  year={2022}
}

@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{zhang2021survey,
  title={A survey on neural network interpretability},
  author={Zhang, Yu and Ti{\v{n}}o, Peter and Leonardis, Ale{\v{s}} and Tang, Ke},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume={5},
  number={5},
  pages={726--742},
  year={2021},
  publisher={IEEE}
}

@article{bereska2024mechanistic,
  title={Mechanistic Interpretability for AI Safety--A Review},
  author={Bereska, Leonard and Gavves, Efstratios},
  journal={arXiv preprint arXiv:2404.14082},
  year={2024}
}

@article{ortiz2021can,
  title={What can linearized neural networks actually say about generalization?},
  author={Ortiz-Jim{\'e}nez, Guillermo and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8998--9010},
  year={2021}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@article{lee2020finite,
  title={Finite versus infinite neural networks: an empirical study},
  author={Lee, Jaehoon and Schoenholz, Samuel and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15156--15172},
  year={2020}
}

@article{bietti2019inductive,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{rahimi2008weighted,
  title={Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning},
  author={Rahimi, Ali and Recht, Benjamin},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@inproceedings{altintacs2023disentangling,
  title={Disentangling Linear Mode Connectivity},
  author={Alt{\i}nta{\c{s}}, G{\"u}l Sena and Bachmann, Gregor and Noci, Lorenzo and Hofmann, Thomas},
  booktitle={UniReps: the First Workshop on Unifying Representations in Neural Models},
  year={2023}
}

@article{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@book{vapnik1999nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={1995},
  publisher={Springer science \& business media}
}

@article{geman1992neural,
  title={Neural networks and the bias/variance dilemma},
  author={Geman, Stuart and Bienenstock, Elie and Doursat, Ren{\'e}},
  journal={Neural computation},
  volume={4},
  number={1},
  pages={1--58},
  year={1992},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{neal2019bias,
  title={On the bias-variance tradeoff: Textbooks need an update},
  author={Neal, Brady},
  journal={arXiv preprint arXiv:1912.08286},
  year={2019}
}

@article{neal2018modern,
  title={A modern take on the bias-variance tradeoff in neural networks},
  author={Neal, Brady and Mittal, Sarthak and Baratin, Aristide and Tantia, Vinayak and Scicluna, Matthew and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:1810.08591},
  year={2018}
}

@article{loog2020brief,
  title={A brief prehistory of double descent},
  author={Loog, Marco and Viering, Tom and Mey, Alexander and Krijthe, Jesse H and Tax, David MJ},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={20},
  pages={10625--10626},
  year={2020},
  publisher={National Acad Sciences}
}

@article{vallet1989linear,
  title={Linear and nonlinear extension of the pseudo-inverse solution for learning Boolean functions},
  author={Vallet, F and Cailton, J-G and Refregier, Ph},
  journal={Europhysics Letters},
  volume={9},
  number={4},
  pages={315},
  year={1989},
  publisher={IOP Publishing}
}

@article{bos1996dynamics,
  title={Dynamics of training},
  author={B{\"o}s, Siegfried and Opper, Manfred},
  journal={Advances in Neural Information Processing Systems},
  volume={9},
  year={1996}
}

@article{spigler2018jamming,
  title={A jamming transition from under-to over-parametrization affects loss landscape and generalization},
  author={Spigler, Stefano and Geiger, Mario and d'Ascoli, St{\'e}phane and Sagun, Levent and Biroli, Giulio and Wyart, Matthieu},
  journal={arXiv preprint arXiv:1810.09665},
  year={2018}
}

@article{advani2020high,
  title={High-dimensional dynamics of generalization error in neural networks},
  author={Advani, Madhu S and Saxe, Andrew M and Sompolinsky, Haim},
  journal={Neural Networks},
  volume={132},
  pages={428--446},
  year={2020},
  publisher={Elsevier}
}

@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={12},
  pages={124003},
  year={2021},
  publisher={IOP Publishing}
}

@inproceedings{he2022sparse,
  title={Sparse Double Descent: Where Network Pruning Aggravates Overfitting},
  author={He, Zheng and Xie, Zeke and Zhu, Quanzhi and Qin, Zengchang},
  booktitle={International Conference on Machine Learning},
  pages={8635--8659},
  year={2022},
  organization={PMLR}
}

@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30063--30070},
  year={2020},
  publisher={National Acad Sciences}
}

@article{nakkiran2020optimal,
  title={Optimal regularization can mitigate double descent},
  author={Nakkiran, Preetum and Venkat, Prayaag and Kakade, Sham and Ma, Tengyu},
  journal={arXiv preprint arXiv:2003.01897},
  year={2020}
}

@article{schaeffer2023double,
  title={Double descent demystified: Identifying, interpreting \& ablating the sources of a deep learning puzzle},
  author={Schaeffer, Rylan and Khona, Mikail and Robertson, Zachary and Boopathy, Akhilan and Pistunova, Kateryna and Rocks, Jason W and Fiete, Ila Rani and Koyejo, Oluwasanmi},
  journal={arXiv preprint arXiv:2303.14151},
  year={2023}
}

@article{chen2021multiple,
  title={Multiple descent: Design your own generalization curve},
  author={Chen, Lin and Min, Yifei and Belkin, Mikhail and Karbasi, Amin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8898--8912},
  year={2021}
}


@article{adlam2020understanding,
  title={Understanding double descent requires a fine-grained bias-variance decomposition},
  author={Adlam, Ben and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11022--11032},
  year={2020}
}

@inproceedings{d2020double,
  title={Double trouble in double descent: Bias and variance (s) in the lazy regime},
  author={d’Ascoli, St{\'e}phane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
  booktitle={International Conference on Machine Learning},
  pages={2280--2290},
  year={2020},
  organization={PMLR}
}

@article{belkin2020two,
  title={Two models of double descent for weak features},
  author={Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={4},
  pages={1167--1180},
  year={2020},
  publisher={SIAM}
}

@article{hastie2022surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={The Annals of Statistics},
  volume={50},
  number={2},
  pages={949--986},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}

@article{lin2021causes,
  title={What causes the test error? going beyond bias-variance via anova},
  author={Lin, Licong and Dobriban, Edgar},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={6925--7006},
  year={2021},
  publisher={JMLRORG}
}

@article{derezinski2020exact,
  title={Exact expressions for double descent and implicit regularization via surrogate random design},
  author={Derezinski, Michal and Liang, Feynman T and Mahoney, Michael W},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={5152--5164},
  year={2020}
}

@article{moody1991effective,
  title={The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems},
  author={Moody, John},
  journal={Advances in neural information processing systems},
  volume={4},
  year={1991}
}

@article{mackay1991bayesian,
  title={Bayesian model comparison and backprop nets},
  author={MacKay, David},
  journal={Advances in neural information processing systems},
  volume={4},
  year={1991}
}


@article{maddox2020rethinking,
  title={Rethinking parameter counting in deep models: Effective dimensionality revisited},
  author={Maddox, Wesley J and Benton, Gregory and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2003.02139},
  year={2020}
}

@incollection{bousquet2003introduction,
  title={Introduction to statistical learning theory},
  author={Bousquet, Olivier and Boucheron, St{\'e}phane and Lugosi, G{\'a}bor},
  booktitle={Summer school on machine learning},
  pages={169--207},
  year={2003},
  publisher={Springer}
}

@article{mallinar2022benign,
  title={Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting},
  author={Mallinar, Neil and Simon, James and Abedsoltan, Amirhesam and Pandit, Parthe and Belkin, Misha and Nakkiran, Preetum},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1182--1195},
  year={2022}
}

@article{wyner2017explaining,
  title={Explaining the success of adaboost and random forests as interpolating classifiers},
  author={Wyner, Abraham J and Olson, Matthew and Bleich, Justin and Mease, David},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={48},
  pages={1--33},
  year={2017}
}

@article{haas2024mind,
  title={Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension},
  author={Haas, Moritz and Holzm{\"u}ller, David and Luxburg, Ulrike and Steinwart, Ingo},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{ma2018power,
  title={The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning},
  author={Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  booktitle={International Conference on Machine Learning},
  pages={3325--3334},
  year={2018},
  organization={PMLR}
}

@article{chatterji2021finite,
  title={Finite-sample analysis of interpolating linear classifiers in the overparameterized regime},
  author={Chatterji, Niladri S and Long, Philip M},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={5721--5750},
  year={2021},
  publisher={JMLRORG}
}

@inproceedings{belkin2018understand,
  title={To understand deep learning we need to understand kernel learning},
  author={Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  booktitle={International Conference on Machine Learning},
  pages={541--549},
  year={2018},
  organization={PMLR}
}

@article{thilak2022slingshot,
  title={The slingshot mechanism: An empirical study of adaptive optimizers and the grokking phenomenon},
  author={Thilak, Vimal and Littwin, Etai and Zhai, Shuangfei and Saremi, Omid and Paiss, Roni and Susskind, Joshua},
  journal={arXiv preprint arXiv:2206.04817},
  year={2022}
}

@article{varma2023explaining,
  title={Explaining grokking through circuit efficiency},
  author={Varma, Vikrant and Shah, Rohin and Kenton, Zachary and Kram{\'a}r, J{\'a}nos and Kumar, Ramana},
  journal={arXiv preprint arXiv:2309.02390},
  year={2023}
}

@inproceedings{lyu2023dichotomy,
  title={Dichotomy of early and late phase implicit biases can provably induce grokking},
  author={Lyu, Kaifeng and Jin, Jikai and Li, Zhiyuan and Du, Simon Shaolei and Lee, Jason D and Hu, Wei},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@article{ghorbani2019limitations,
  title={Limitations of lazy training of two-layers neural network},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{geiger2020disentangling,
  title={Disentangling feature and lazy training in deep neural networks},
  author={Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2020},
  number={11},
  pages={113301},
  year={2020},
  publisher={IOP Publishing}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{dietterich2002ensemble,
  title={Ensemble learning},
  author={Dietterich, Thomas G},
  journal={The handbook of brain theory and neural networks},
  volume={2},
  number={1},
  pages={110--125},
  year={2002}
}

@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}

@article{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{freeman2016topology,
  title={Topology and geometry of half-rectified network optimization},
  author={Freeman, C Daniel and Bruna, Joan},
  journal={arXiv preprint arXiv:1611.01540},
  year={2016}
}

@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}

@article{choshen2022fusing,
  title={Fusing finetuned models for better pretraining},
  author={Choshen, Leshem and Venezian, Elad and Slonim, Noam and Katz, Yoav},
  journal={arXiv preprint arXiv:2204.03044},
  year={2022}
}

@article{neyshabur2020being,
  title={What is being transferred in transfer learning?},
  author={Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={512--523},
  year={2020}
}

@article{rame2022diverse,
  title={Diverse weight averaging for out-of-distribution generalization},
  author={Rame, Alexandre and Kirchmeyer, Matthieu and Rahier, Thibaud and Rakotomamonjy, Alain and Gallinari, Patrick and Cord, Matthieu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={10821--10836},
  year={2022}
}

@article{ilharco2022patching,
  title={Patching open-vocabulary models by interpolating weights},
  author={Ilharco, Gabriel and Wortsman, Mitchell and Gadre, Samir Yitzhak and Song, Shuran and Hajishirzi, Hannaneh and Kornblith, Simon and Farhadi, Ali and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29262--29277},
  year={2022}
}

@article{benzing2022random,
  title={Random initialisations performing above chance and how to find them},
  author={Benzing, Frederik and Schug, Simon and Meier, Robert and Von Oswald, Johannes and Akram, Yassir and Zucchet, Nicolas and Aitchison, Laurence and Steger, Angelika},
  journal={arXiv preprint arXiv:2209.07509},
  year={2022}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{jeffares2024joint,
  title={Joint training of deep ensembles fails due to learner collusion},
  author={Jeffares, Alan and Liu, Tennison and Crabb{\'e}, Jonathan and van der Schaar, Mihaela},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{abe2023pathologies,
  title={Pathologies of Predictive Diversity in Deep Ensembles},
  author={Abe, Taiga and Buchanan, E Kelly and Pleiss, Geoff and Cunningham, John Patrick},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@inproceedings{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Baolin and Ng, Andrew Y and others},
  booktitle={NIPS workshop on deep learning and unsupervised feature learning},
  volume={2011},
  number={5},
  pages={7},
  year={2011},
  organization={Granada, Spain}
}


@article{OpenML2013,
  author = {Joaquin Vanschoren and Jan N. van Rijn and Bernd Bischl and Luis Torgo},
  title = {OpenML: networked science in machine learning},
  journal = {SIGKDD Explorations},
  volume = {15},
  number = {2},
  year = {2013},
  pages = {49-60},
  url = {http://doi.acm.org/10.1145/2641190.264119},
  doi = {10.1145/2641190.2641198},
  publisher = {ACM}
}

@article{fort2019deep,
  title={Deep ensembles: A loss landscape perspective},
  author={Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1912.02757},
  year={2019}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{brownlanguage,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
journal={Advances in neural information processing systems},
year={2020}
}

@article{domingos2020every,
  title={Every model learned by gradient descent is approximately a kernel machine},
  author={Domingos, Pedro},
  journal={arXiv preprint arXiv:2012.00152},
  year={2020}
}

@inproceedings{greydanus2020scaling,
  title={Scaling Down Deep Learning with MNIST-1D},
  author={Greydanus, Samuel James and Kobak, Dmitry},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{ortiz2024task,
  title={Task arithmetic in the tangent space: Improved editing of pre-trained models},
  author={Ortiz-Jimenez, Guillermo and Favero, Alessandro and Frossard, Pascal},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{levi2023grokking,
  title={Grokking in Linear Estimators--A Solvable Model that Groks without Understanding},
  author={Levi, Noam and Beck, Alon and Bar-Sinai, Yohai},
  journal={International Conference on Learning Representations},
  year={2024}
}

@article{miller2023grokking,
  title={Grokking Beyond Neural Networks: An Empirical Exploration with Model Complexity},
  author={Miller, Jack and O'Neill, Charles and Bui, Thang},
  journal={Transactions on Machine Learning Research (TMLR)},
  year={2024}
}

@article{curth2024classical,
  title={Classical Statistical (In-Sample) Intuitions Don't Generalize Well: A Note on Bias-Variance Tradeoffs, Overfitting and Moving from Fixed to Random Designs},
  author={Curth, Alicia},
  journal={arXiv preprint arXiv:2409.18842},
  year={2024}
}