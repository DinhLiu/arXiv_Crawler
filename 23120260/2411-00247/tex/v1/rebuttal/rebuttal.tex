\documentclass[a4paper,10pt]{extarticle}

% \usepackage[nonatbib]{neurips_2024}




\usepackage{amsmath}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{wrapfig}
\usepackage{thm-restate}
\usepackage{mathtools}
\usepackage[shortlabels]{enumitem}
\usepackage{subfigure}

\definecolor{aurometalsaurus}{rgb}{0.43, 0.5, 0.5}
\definecolor{ashgrey}{rgb}{0.7, 0.75, 0.71}
% \definecolor{sharedred}{rgb}{1.0, 0.15, 0.07}
\definecolor{sharedred}{rgb}{0.93, 0.11, 0.14}
% \definecolor{sharedblue}{rgb}{0.01, 0.35, 0.93}
\definecolor{cbgreen}{rgb}{0.30196078431372547, 0.6862745098039216, 0.2901960784313726}
\definecolor{cborange}{rgb}{1.0, 0.4980392156862745, 0.0}
\definecolor{lineblue}{rgb}{0.23192618223760095, 0.5456516724336793, 0.7626143790849673}

\newtheorem{prop}{Proposition}
\newtheorem{defi}{Definition}

%\counterwithin{figure}{subsection}


\newcommand{\squish}[1]{{#1\parfillskip=0pt\par}}
\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage{geometry}
\geometry{legalpaper, portrait, margin=0.45in}

% \usepackage{hyperref}       % hyperlinks
\usepackage{sepfootnotes}
\usepackage[colorlinks=true]{hyperref}
% \usepackage{footnotebackref}
\usepackage[capitalise]{cleveref}
\crefname{section}{Sec.}{Sections}

\begin{document}

\begin{center}
    \textbf{Numerically verifying analytical findings from Sec. 3.1 in actual neural networks.}
\end{center}
\vspace{-0.15cm}
\begin{figure}[h]
\centering
\subfigure[MLP on MNIST]{
    \includegraphics[width=.45\textwidth]{rebuttal/figures/mnist-momentum.png}}
    \label{fig:mlp}
\subfigure[ResNet20 on CIFAR10]{
   \includegraphics[width=.45\textwidth]{rebuttal/figures/cifar-momentum.png}}
    \label{fig:resnet}\vspace{-.2cm}
\caption{\textbf{Empirically verifying the opposing effects of momentum and (centered\protect\footnotemark[1]) weight decay  $\lambda (\bm{\theta}_t - \bm{\theta}_0)$ on predictions in real networks (3-layer ReLU-MLP on MNIST and a ResNet20 on CIFAR-10, trained using standard SGD with learning rate $\gamma=.01$ implemented in PyTorch).} Because there are many complex forces at play, we zoom in on the first updates where the effects of the different optimization choices can be observed most clearly. We evaluate how the predictions on the \textit{first batch} move relative to their labels when the \textit{second batch weight update} takes place with momentum ($\beta_1=.9, \lambda=0$), weight decay ($\beta_1=0, \lambda=\frac{.9}{\gamma}$), neither ($\beta_1=\lambda=0$) and both ($\beta_1=.9$, $\lambda=\frac{\beta_1}{\gamma}$).  We show how the different optimization choices affect how the \textit{predictions} move at the next weight update. First, by manually setting all loss gradients to zero in the second batch before taking a gradient step in weight space, we verify that if there was no signal in the next batch then all movement in the predictions on the first batch comes from the momentum and weight decay terms, where momentum moves predictions closer \textit{towards} their label and weight decay moves them in the opposite direction. Applying both at the same time indeed perfectly cancels out the update. Second, we show that when taking a standard gradient step using information from the second batch, predictions on the first batch already improve without either term due to the added signal. Momentum then moves the predictions closer towards their label while weight decay acts in the opposite direction, and both again perfectly offset each other. This thus verifies the statements in the text empirically in (non-approximate!) standard networks. Regarding the practical implications of this observation, note that the weight decay needed to offset momentum ($\lambda=\frac{\beta_1}{\gamma}=90$) is \textit{much} larger than $\lambda$ typically used in practice. Therefore, as indicated in the main text, this means that in practice, when used together with momentum, weight decay conceptually predominantly acts to shrink the initialization $\bm{\theta_0}$ towards zero.}
    \label{fig:res-sim}
\end{figure}
\footnotetext[1]{Note that as highlighted in the text, weight decay effectively acts by decaying both the initialization $\bm{\theta}_0$ \textit{and} the previous weight updates. Because we are interested here in isolating how the latter operates relative to momentum, we use a \textit{centered} weight decay term $\lambda (\bm{\theta}_t - \bm{\theta}_0)$ here (instead of $\lambda \bm{\theta}_t$) for illustrative purposes, which removes the effect of decaying $\bm{\theta_0}$.}
\vspace{-0.6cm}
\begin{center}
\linethickness{0.8pt}
\line(1,0){550}
\end{center}
\vspace{-0.05cm}
\begin{center}
    \textbf{Measuring effective parameters on the parity prediction problem.}
\end{center}
\vspace{-0.2cm}
\begin{figure}[h]
  \begin{minipage}[c]{0.47\textwidth}
    \includegraphics[width=\textwidth]{rebuttal/figures/parity_grok.pdf}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.5\textwidth}
    \caption{\textbf{Grokking in accuracy (top) against effective parameters (bottom) for parity prediction setups with increasing number of spurious features.}
        Supplementing the grokking experiments on the MNIST and polynomial regression tasks from the original submission, we replicate the parity prediction experiments from Merrill et al. (2023) [``A tale of two circuits: Grokking as competition of sparse and dense subnetworks''] and  Miller et al. (TMLR2024) [``Grokking Beyond Neural Networks: An Empirical Exploration with Model Complexity''] where $k=3$ relevant features are hidden among $n$ total features and grokking becomes more extreme as $n$ grows. As in Appendix D.2, we observe also here that later generalization (i.e. more grokking) is predicted by an effective parameter gap emerging later in training as the growth in test-time effective parameters flattens out later in training for more extreme observations of grokking. As in Merrill et al. (2023), we use $1000$ training examples and single hidden layer ReLU networks trained with weight decay. 
    }
  \end{minipage}
\end{figure}
\vspace{-0.6cm}
\begin{center}
\linethickness{0.8pt}
\line(1,0){550}
\end{center}
\vspace{-0.05cm}
\begin{center}
    \textbf{Quantifying double descent on recently proposed MNIST-1D benchmark dataset.}
\end{center}
\vspace{-0.2cm}
\begin{figure}[h!]
  \begin{minipage}[c]{0.65\textwidth}
    \caption{\textbf{Deep double descent in mean squared error (top) against effective parameters (bottom) on the MNIST-1D benchmark dataset.}
       Supplementing the deep double descent experiments on CIFAR-10 and MNIST presented in the original submission, we utilize the MNIST-1D dataset (Greydanus  \& Kobak, (ICML2024). ``Scaling Down Deep Learning with MNIST-1D.'') which was proposed recently as a sandbox for investigating empirical deep learning phenomena. We replicate a binary classification version of the MLP double descent experiment with label noise from the MNIST-1D paper (which was itself adapted from the textbook [Pri23]), while additionally computing effective parameters. We find the same trends as in the main paper: the decrease in test error after the interpolation threshold is again accompanied by a \textit{decrease in effective parameters} as the number of raw model parameters is further \textit{increased} in the interpolation regime.
    }
  \end{minipage}\hfill
  \begin{minipage}[c]{0.3\textwidth}
    \includegraphics[width=\textwidth]{rebuttal/figures/mnist_1d_mean.pdf}
  \end{minipage}\hfill
\end{figure}

\end{document}